{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3201,"status":"ok","timestamp":1679824465751,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"MbN-qdA5z1G-","outputId":"5d57aa74-86d1-4986-d2de-b773e15b372a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8511,"status":"ok","timestamp":1679824474257,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"EcXBnbTmz2nW","outputId":"0ecd1dee-4ce8-4151-8887-223436939446"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sastrawi in /usr/local/lib/python3.9/dist-packages (1.0.1)\n"]}],"source":["pip install sastrawi"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5805,"status":"ok","timestamp":1679824480056,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"1AVL6mTkz2x9","outputId":"888ebfed-502f-4a55-8818-fb1316952189"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.4.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from nltk) (1.16.0)\n"]}],"source":["pip install nltk"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2101,"status":"ok","timestamp":1679824482154,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"DFkbHffjz299","outputId":"07e16c76-6b1a-4dca-da01-045967f61c87"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1679824482155,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"wRQaDdXaz6fl","outputId":"538e2576-5f87-495c-e56f-1991825c71ef"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":643,"status":"ok","timestamp":1679824482795,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"1d61a3ba","outputId":"343bcd90-2c2c-4a0f-c8a1-9b5ff4106851"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Indonesian Query Answering Dataset for Online Essay Test System.zip', 'dict.json', 'Preprocessing', 'Analysis Data', 'Text Preprocessing', 'Data', 'dataset.py', '__pycache__']\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sn\n","import os\n","import re\n","import nltk\n","import random\n","from nltk.corpus import stopwords\n","from gensim.models import Word2Vec\n","from nltk.tokenize import sent_tokenize, LineTokenizer, RegexpTokenizer\n","from nltk.tokenize import word_tokenize\n","from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n","\n","print(os.listdir(\"/content/drive/MyDrive/Paper_TA_ASAG/DATASET_TA/\"))\n","factory = StemmerFactory()\n","stemmer = factory.create_stemmer()\n","import string\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","stopwords_indonesia = stopwords.words('indonesian')\n","stop_words = set(stopwords.words('indonesian'))\n","import sys\n","import os"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1679824482795,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"UbTgH1-D-kLA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6118,"status":"ok","timestamp":1679824488909,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"icJEdL3SOegH","outputId":"9dafc42d-82f5-4988-d709-3b271bbcb0ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nlp_id in /usr/local/lib/python3.9/dist-packages (0.1.13.0)\n","Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.9/dist-packages (from nlp_id) (3.4.5)\n","Requirement already satisfied: scikit-learn==1.1.0 in /usr/local/lib/python3.9/dist-packages (from nlp_id) (1.1.0)\n","Requirement already satisfied: wget==3.2 in /usr/local/lib/python3.9/dist-packages (from nlp_id) (3.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from nltk==3.4.5-\u003enlp_id) (1.16.0)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.1.0-\u003enlp_id) (3.1.0)\n","Requirement already satisfied: joblib\u003e=1.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.1.0-\u003enlp_id) (1.1.1)\n","Requirement already satisfied: scipy\u003e=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.1.0-\u003enlp_id) (1.10.1)\n","Requirement already satisfied: numpy\u003e=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.1.0-\u003enlp_id) (1.22.4)\n"]}],"source":["pip install nlp_id"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9623,"status":"ok","timestamp":1679824498526,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"LMdPOnmpOkt_","outputId":"472c8387-5282-4999-a4da-d2f870335728"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.3)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: tokenizers!=0.11.3,\u003c0.14,\u003e=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.11.0-\u003etransformers) (4.5.0)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers) (1.26.15)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers) (2.0.12)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers) (2022.12.7)\n"]}],"source":["pip install transformers"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1679824784881,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"t4N6fkUEOk65"},"outputs":[],"source":["import re\n","import random\n","import pandas as pd\n","import torch\n","import tensorflow as tf\n","import numpy as np\n","\n","from nlp_id.lemmatizer import Lemmatizer\n","from nltk.corpus import stopwords\n","from tqdm import tqdm\n","from sklearn.preprocessing import KBinsDiscretizer\n","from sklearn.metrics import f1_score, cohen_kappa_score\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.initializers import TruncatedNormal\n","from tensorflow.keras.losses import CategoricalCrossentropy\n","from tensorflow.keras.metrics import CategoricalAccuracy\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Input, Dense\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from nltk.corpus import stopwords"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1679824820331,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"e8cYZB7IXAky","outputId":"7d34f9ab-d2fb-40a5-b0e7-5cfc215f6a91"},"outputs":[{"name":"stdout","output_type":"stream","text":["cp: missing destination file operand after '/content/drive/MyDrive/Paper_TA_ASAG/DATASET_TA/dataset.py'\n","Try 'cp --help' for more information.\n","cp: missing destination file operand after '/content/drive/MyDrive/Paper_TA_ASAG/DATASET_TA/dict.json'\n","Try 'cp --help' for more information.\n"]}],"source":["!cp /content/drive/MyDrive/Paper_TA_ASAG/DATASET_TA/dataset.py\n","!cp /content/drive/MyDrive/Paper_TA_ASAG/DATASET_TA/dict.json\n","import json\n","import random\n","import itertools\n","from google.colab import files\n","\n","py_file_location = \"/content/drive/MyDrive/Paper_TA_ASAG/DATASET_TA/dict.json\"\n","\n","with open(py_file_location, 'r') as file:\n","  data_json = json.load(file)"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1679825043615,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"sVsXL-WcK5iA"},"outputs":[],"source":["# fungsi untuk menghapus kata negasi\n","def hapus_kata_negasi(kalimat):\n","    # membuat daftar kata negasi\n","    negasi = ['tidak', 'bukan', 'belum', 'tak', 'jangan', 'tidaklah', 'bukannya', 'tiada', 'gak', 'ngga', 'nggak', 'enggak']\n","    # melakukan split kalimat menjadi token\n","    result = []\n","    negate = False\n","    for word in kalimat:\n","        if any(neg == word for neg in negasi):\n","            negate = True\n","        elif negate and word not in string.punctuation:\n","            word = 'tidak_' + word\n","            negate = False\n","        result.append(word)\n","    return result\n","\n","def remove_stopword(token):\n","    return ' '. join(token)\n","\n","def augment_data(text, dictionary):\n","    new_sentences = []\n","    word_synonyms = []\n","    base_word = []\n","    words = text.split()\n","    for i in range(min(5, len(words))):\n","        synonym = []\n","        if words[i] in dictionary:\n","            synonyms = dictionary[words[i]]['sinonim'][0]\n","            synonym.append(synonyms)\n","            synonym.append(words[i]) \n","            word_synonyms.append(synonym)\n","        else:\n","          synonym.append(words[i])\n","          word_synonyms.append(synonym)\n","    \n","    for j in range(5, len(words)):\n","      a = []\n","      a.append(words[j])\n","      word_synonyms.append(a)\n","\n","    new_sentences = list(itertools.product(*word_synonyms))\n","    new_sentences = [' '.join(pasangan) for pasangan in new_sentences]\n","    return new_sentences"]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":596,"status":"ok","timestamp":1679829904728,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"dxklXiDDWOkc"},"outputs":[],"source":["seed_val = 17\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","punctuation = r'[^\\w\\s]'\n","\n","def f1_score_func(preds, labels):\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return f1_score(labels_flat, preds_flat, average='weighted')\n","\n","def qwk_score_func(preds, labels):\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return cohen_kappa_score(labels_flat, preds_flat)\n","\n","def accuracy_per_class(preds, labels):\n","    label_dict_inverse = {v: k for k, v in label_dict.items()}\n","\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    for label in np.unique(labels_flat):\n","        y_preds = preds_flat[labels_flat==label]\n","        y_true = labels_flat[labels_flat==label]\n","        print(f'Class: {label_dict_inverse[label]}')\n","        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n","\n","def evaluate(dataloader_val, device, model):\n","\n","    model.eval()\n","\n","    loss_val_total = 0\n","    predictions, true_vals = [], []\n","\n","    for batch in dataloader_val:\n","\n","        batch = tuple(b.to(device) for b in batch)\n","\n","        inputs = {'input_ids':      batch[0],\n","                  'attention_mask': batch[1],\n","                  'labels':         batch[2],\n","                 }\n","\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","\n","        loss = outputs[0]\n","        logits = outputs[1]\n","        loss_val_total += loss.item()\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = inputs['labels'].cpu().numpy()\n","        predictions.append(logits)\n","        true_vals.append(label_ids)\n","\n","    loss_val_avg = loss_val_total/len(dataloader_val)\n","\n","    predictions = np.concatenate(predictions, axis=0)\n","    true_vals = np.concatenate(true_vals, axis=0)\n","\n","    return loss_val_avg, predictions, true_vals\n","\n","def train_eval(df_final, pretrainedmodel):\n","    df_final_new = []\n","\n","    for i, row in df_final.iterrows():\n","        text = row['jawaban']\n","        augmented_data = augment_data(text, data_json)\n","        for j in range(0, len(augmented_data) - 1):\n","            row['jawaban'] = augmented_data[j]\n","            new_row = row\n","            df_final = df_final.append(new_row)\n","  \n","    df_final.reset_index(drop=True, inplace=True)\n","    df_final['nilai'] = pd.qcut(df_final['nilai'], 5, labels=False)\n","\n","    # make sure that the training set and test set ratio is 80:20\n","    add = len(df_final[df_final['tipe'] == 'test']) - (round(0.2*(len(df_final[df_final['tipe'] == 'train'])+len(df_final[df_final['tipe'] == 'test']))))\n","    for i in df_final[df_final['tipe'] == 'test'].sample(n = add).itertuples():\n","        df_final.at[i.Index, 'tipe'] = 'train'\n","\n","    # load model and tokenizer\n","    tokenizer = BertTokenizer.from_pretrained(pretrainedmodel, ignore_mismatched_sizes=True)\n","\n","    encoded_data_train = tokenizer.batch_encode_plus(\n","        df_final[df_final.tipe=='train']['jawaban'].values,\n","        add_special_tokens=True,\n","        return_attention_mask=True,\n","        pad_to_max_length=True,\n","        truncation=True,\n","        max_length=256,\n","        padding='max_length',\n","        return_tensors='pt'\n","    )\n","\n","    encoded_data_val = tokenizer.batch_encode_plus(\n","        df_final[df_final.tipe=='test']['jawaban'].values,\n","        add_special_tokens=True,\n","        return_attention_mask=True,\n","        pad_to_max_length=True,\n","        truncation=True,\n","        max_length=256,\n","        padding='max_length',\n","        return_tensors='pt'\n","    )\n","\n","    input_ids_train = encoded_data_train['input_ids']\n","    attention_masks_train = encoded_data_train['attention_mask']\n","    labels_train = torch.tensor(df_final[df_final.tipe=='train'].nilai.values)\n","\n","    input_ids_val = encoded_data_val['input_ids']\n","    attention_masks_val = encoded_data_val['attention_mask']\n","    labels_val = torch.tensor(df_final[df_final.tipe=='test'].nilai.values)\n","\n","    dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n","    dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n","\n","    model = BertForSequenceClassification.from_pretrained(pretrainedmodel,\n","                                                          num_labels=5,\n","                                                          output_attentions=False,\n","                                                          output_hidden_states=False, ignore_mismatched_sizes=True)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","\n","    batch_size = 4\n","\n","    dataloader_train = DataLoader(dataset_train,\n","                                  sampler=RandomSampler(dataset_train),\n","                                  batch_size=batch_size)\n","\n","    dataloader_validation = DataLoader(dataset_val,\n","                                       sampler=SequentialSampler(dataset_val),\n","                                       batch_size=batch_size)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(),\n","                      lr=2e-5,\n","                      eps=1e-8)\n","\n","    epochs = 4\n","\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps=0,\n","                                                num_training_steps=len(dataloader_train)*epochs)\n","\n","    for epoch in tqdm(range(1, epochs+1)):\n","\n","        model.train()\n","\n","        loss_train_total = 0\n","\n","        progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n","        for batch in progress_bar:\n","\n","            model.zero_grad()\n","\n","            batch = tuple(b.to(device) for b in batch)\n","\n","            inputs = {'input_ids':      batch[0],\n","                      'attention_mask': batch[1],\n","                      'labels':         batch[2],\n","                     }\n","\n","            outputs = model(**inputs)\n","\n","            loss = outputs[0]\n","            loss_train_total += loss.item()\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            optimizer.step()\n","            scheduler.step()\n","\n","            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n","\n","\n","        torch.save(model.state_dict(), f'/content/drive/MyDrive/Paper_TA_ASAG/DATASET_TA/Data/Model_Save/finetuned_BERT_{df_final[\"path\"]}_epoch_{epoch}.model')\n","\n","        tqdm.write(f'\\nEpoch {epoch}')\n","\n","        loss_train_avg = loss_train_total/len(dataloader_train)\n","        tqdm.write(f'Training loss: {loss_train_avg}')\n","\n","        val_loss, predictions, true_vals = evaluate(dataloader_validation, device, model)\n","        val_f1 = f1_score_func(predictions, true_vals)\n","        val_qwk = qwk_score_func(predictions, true_vals)\n","        tqdm.write(f'Validation loss: {val_loss}')\n","        tqdm.write(f'F1 Score (Weighted): {val_f1}')\n","        tqdm.write(f'QWK Score: {val_qwk}')"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":1120,"status":"ok","timestamp":1679831729055,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"AN2_FoKXWQkj"},"outputs":[],"source":["def asag_systems(path_dir):\n","    list_pre_trained_model = ['indobenchmark/indobert-lite-base-p2']\n","    list_dir = os.listdir(path_dir)\n","    for m in list_pre_trained_model:\n","        print(m)\n","        for idx, ele in enumerate(list_dir):\n","            df_raw = pd.read_excel(open(path_dir+'/'+ele, 'rb'),\n","                                  sheet_name='Soal',\n","                                  header=1,\n","                                  index_col=0,\n","                                  usecols='B:D')\n","\n","            list_final = []\n","\n","            for i in df_raw.itertuples():\n","                list_final.append(\n","                    {\n","                        'jawaban': i[1],\n","                        'nilai': 100,\n","                        'tipe': 'train',\n","                        'path': f' augmentasi data',\n","                    }\n","                )\n","                df_tmp = pd.read_excel(open(path_dir+'/'+ele, 'rb'),\n","                                        sheet_name='No.'+str(i.Index),\n","                                        header=1,\n","                                        index_col=0,\n","                                        usecols='B:N')\n","                \n","                df_tmp = df_tmp.dropna()\n","                for j in df_tmp.itertuples():\n","                    list_final.append(\n","                        {\n","                            'jawaban': j[1],\n","                            'nilai': j[12],\n","                            'tipe': 'test',\n","                            'path': f'augmentasi data',\n","                        }\n","                    )\n","            if idx == 0:\n","                df_final = pd.DataFrame(list_final)\n","            else:\n","                df_final.append(pd.DataFrame(list_final), ignore_index=True)\n","\n","            print(' '.join(ele.rstrip('.xslx').split('_')))\n","    \n","            train_eval(df_final, m)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"iOJ-oZCtWRab"},"outputs":[{"name":"stdout","output_type":"stream","text":["indobenchmark/indobert-lite-base-p2\n","Analisis Essay Grading Lifestyle\n"]},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","\u003cipython-input-51-8d76b8aa0705\u003e:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_final = df_final.append(new_row)\n","The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'AlbertTokenizerFast'. \n","The class this function is called from is 'BertTokenizer'.\n","You are using a model of type albert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n","Some weights of the model checkpoint at indobenchmark/indobert-lite-base-p2 were not used when initializing BertForSequenceClassification: ['pooler.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight', 'pooler.weight', 'encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight', 'encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight', 'encoder.albert_layer_groups.0.albert_layers.0.ffn.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias', 'encoder.embedding_hidden_mapping_in.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight', 'encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight', 'encoder.embedding_hidden_mapping_in.weight', 'encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight', 'encoder.albert_layer_groups.0.albert_layers.0.ffn.weight', 'encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight', 'encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p2 and are newly initialized: ['encoder.layer.10.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.2.attention.self.query.weight', 'classifier.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'classifier.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'pooler.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'pooler.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p2 and are newly initialized because the shapes did not match:\n","- embeddings.word_embeddings.weight: found shape torch.Size([30000, 128]) in the checkpoint and torch.Size([30000, 768]) in the model instantiated\n","- embeddings.position_embeddings.weight: found shape torch.Size([512, 128]) in the checkpoint and torch.Size([512, 768]) in the model instantiated\n","- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 128]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n","- embeddings.LayerNorm.weight: found shape torch.Size([128]) in the checkpoint and torch.Size([768]) in the model instantiated\n","- embeddings.LayerNorm.bias: found shape torch.Size([128]) in the checkpoint and torch.Size([768]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  0%|          | 0/4 [00:00\u003c?, ?it/s]\n","Epoch 1:   0%|          | 0/131 [00:00\u003c?, ?it/s]\u001b[A\n","Epoch 1:   0%|          | 0/131 [00:12\u003c?, ?it/s, training_loss=0.531]\u001b[A\n","Epoch 1:   1%|          | 1/131 [00:12\u003c27:47, 12.83s/it, training_loss=0.531]\u001b[A\n","Epoch 1:   1%|          | 1/131 [00:23\u003c27:47, 12.83s/it, training_loss=0.764]\u001b[A\n","Epoch 1:   2%|▏         | 2/131 [00:23\u003c24:22, 11.34s/it, training_loss=0.764]\u001b[A\n","Epoch 1:   2%|▏         | 2/131 [00:35\u003c24:22, 11.34s/it, training_loss=0.517]\u001b[A\n","Epoch 1:   2%|▏         | 3/131 [00:35\u003c25:23, 11.91s/it, training_loss=0.517]\u001b[A\n","Epoch 1:   2%|▏         | 3/131 [00:48\u003c25:23, 11.91s/it, training_loss=1.030]\u001b[A\n","Epoch 1:   3%|▎         | 4/131 [00:48\u003c26:02, 12.31s/it, training_loss=1.030]\u001b[A\n","Epoch 1:   3%|▎         | 4/131 [01:00\u003c26:02, 12.31s/it, training_loss=0.651]\u001b[A\n","Epoch 1:   4%|▍         | 5/131 [01:00\u003c25:17, 12.05s/it, training_loss=0.651]\u001b[A\n","Epoch 1:   4%|▍         | 5/131 [01:11\u003c25:17, 12.05s/it, training_loss=0.814]\u001b[A\n","Epoch 1:   5%|▍         | 6/131 [01:11\u003c24:25, 11.73s/it, training_loss=0.814]\u001b[A\n","Epoch 1:   5%|▍         | 6/131 [01:24\u003c24:25, 11.73s/it, training_loss=0.629]\u001b[A\n","Epoch 1:   5%|▌         | 7/131 [01:24\u003c25:03, 12.12s/it, training_loss=0.629]\u001b[A\n","Epoch 1:   5%|▌         | 7/131 [01:36\u003c25:03, 12.12s/it, training_loss=0.503]\u001b[A\n","Epoch 1:   6%|▌         | 8/131 [01:36\u003c25:14, 12.31s/it, training_loss=0.503]\u001b[A\n","Epoch 1:   6%|▌         | 8/131 [01:47\u003c25:14, 12.31s/it, training_loss=0.524]\u001b[A\n","Epoch 1:   7%|▋         | 9/131 [01:47\u003c23:39, 11.64s/it, training_loss=0.524]\u001b[A\n","Epoch 1:   7%|▋         | 9/131 [01:59\u003c23:39, 11.64s/it, training_loss=0.614]\u001b[A\n","Epoch 1:   8%|▊         | 10/131 [01:59\u003c24:09, 11.98s/it, training_loss=0.614]\u001b[A\n","Epoch 1:   8%|▊         | 10/131 [02:12\u003c24:09, 11.98s/it, training_loss=0.575]\u001b[A\n","Epoch 1:   8%|▊         | 11/131 [02:12\u003c24:31, 12.26s/it, training_loss=0.575]\u001b[A\n","Epoch 1:   8%|▊         | 11/131 [02:23\u003c24:31, 12.26s/it, training_loss=0.334]\u001b[A\n","Epoch 1:   9%|▉         | 12/131 [02:23\u003c23:34, 11.89s/it, training_loss=0.334]\u001b[A\n","Epoch 1:   9%|▉         | 12/131 [02:35\u003c23:34, 11.89s/it, training_loss=0.688]\u001b[A\n","Epoch 1:  10%|▉         | 13/131 [02:35\u003c23:12, 11.80s/it, training_loss=0.688]\u001b[A\n","Epoch 1:  10%|▉         | 13/131 [02:48\u003c23:12, 11.80s/it, training_loss=0.648]\u001b[A\n","Epoch 1:  11%|█         | 14/131 [02:48\u003c23:37, 12.12s/it, training_loss=0.648]\u001b[A\n","Epoch 1:  11%|█         | 14/131 [03:00\u003c23:37, 12.12s/it, training_loss=0.477]\u001b[A\n","Epoch 1:  11%|█▏        | 15/131 [03:00\u003c23:30, 12.16s/it, training_loss=0.477]\u001b[A\n","Epoch 1:  11%|█▏        | 15/131 [03:11\u003c23:30, 12.16s/it, training_loss=0.687]\u001b[A\n","Epoch 1:  12%|█▏        | 16/131 [03:11\u003c22:24, 11.69s/it, training_loss=0.687]\u001b[A\n","Epoch 1:  12%|█▏        | 16/131 [03:23\u003c22:24, 11.69s/it, training_loss=0.560]\u001b[A\n","Epoch 1:  13%|█▎        | 17/131 [03:23\u003c22:50, 12.02s/it, training_loss=0.560]\u001b[A\n","Epoch 1:  13%|█▎        | 17/131 [03:36\u003c22:50, 12.02s/it, training_loss=0.559]\u001b[A\n","Epoch 1:  14%|█▎        | 18/131 [03:36\u003c23:06, 12.27s/it, training_loss=0.559]\u001b[A\n","Epoch 1:  14%|█▎        | 18/131 [03:47\u003c23:06, 12.27s/it, training_loss=0.531]\u001b[A\n","Epoch 1:  15%|█▍        | 19/131 [03:47\u003c21:52, 11.72s/it, training_loss=0.531]\u001b[A\n","Epoch 1:  15%|█▍        | 19/131 [03:59\u003c21:52, 11.72s/it, training_loss=0.482]\u001b[A\n","Epoch 1:  15%|█▌        | 20/131 [03:59\u003c21:57, 11.87s/it, training_loss=0.482]\u001b[A\n","Epoch 1:  15%|█▌        | 20/131 [04:12\u003c21:57, 11.87s/it, training_loss=0.665]\u001b[A\n","Epoch 1:  16%|█▌        | 21/131 [04:12\u003c22:18, 12.17s/it, training_loss=0.665]\u001b[A\n","Epoch 1:  16%|█▌        | 21/131 [04:23\u003c22:18, 12.17s/it, training_loss=0.551]\u001b[A\n","Epoch 1:  17%|█▋        | 22/131 [04:23\u003c21:51, 12.03s/it, training_loss=0.551]\u001b[A\n","Epoch 1:  17%|█▋        | 22/131 [04:34\u003c21:51, 12.03s/it, training_loss=0.684]\u001b[A\n","Epoch 1:  18%|█▊        | 23/131 [04:34\u003c21:02, 11.69s/it, training_loss=0.684]\u001b[A\n","Epoch 1:  18%|█▊        | 23/131 [04:47\u003c21:02, 11.69s/it, training_loss=0.623]\u001b[A\n","Epoch 1:  18%|█▊        | 24/131 [04:47\u003c21:27, 12.03s/it, training_loss=0.623]\u001b[A\n","Epoch 1:  18%|█▊        | 24/131 [05:00\u003c21:27, 12.03s/it, training_loss=0.565]\u001b[A\n","Epoch 1:  19%|█▉        | 25/131 [05:00\u003c21:36, 12.23s/it, training_loss=0.565]\u001b[A\n","Epoch 1:  19%|█▉        | 25/131 [05:10\u003c21:36, 12.23s/it, training_loss=0.749]\u001b[A\n","Epoch 1:  20%|█▉        | 26/131 [05:10\u003c20:20, 11.62s/it, training_loss=0.749]\u001b[A\n","Epoch 1:  20%|█▉        | 26/131 [05:23\u003c20:20, 11.62s/it, training_loss=0.545]\u001b[A\n","Epoch 1:  21%|██        | 27/131 [05:23\u003c20:44, 11.96s/it, training_loss=0.545]\u001b[A\n","Epoch 1:  21%|██        | 27/131 [05:36\u003c20:44, 11.96s/it, training_loss=0.417]\u001b[A\n","Epoch 1:  21%|██▏       | 28/131 [05:36\u003c20:59, 12.23s/it, training_loss=0.417]\u001b[A\n","Epoch 1:  21%|██▏       | 28/131 [05:47\u003c20:59, 12.23s/it, training_loss=0.584]\u001b[A\n","Epoch 1:  22%|██▏       | 29/131 [05:47\u003c20:09, 11.86s/it, training_loss=0.584]\u001b[A\n","Epoch 1:  22%|██▏       | 29/131 [05:58\u003c20:09, 11.86s/it, training_loss=0.467]\u001b[A\n","Epoch 1:  23%|██▎       | 30/131 [05:58\u003c19:53, 11.82s/it, training_loss=0.467]\u001b[A\n","Epoch 1:  23%|██▎       | 30/131 [06:11\u003c19:53, 11.82s/it, training_loss=0.532]\u001b[A\n","Epoch 1:  24%|██▎       | 31/131 [06:11\u003c20:13, 12.14s/it, training_loss=0.532]\u001b[A\n","Epoch 1:  24%|██▎       | 31/131 [06:24\u003c20:13, 12.14s/it, training_loss=0.507]\u001b[A\n","Epoch 1:  24%|██▍       | 32/131 [06:24\u003c20:05, 12.18s/it, training_loss=0.507]\u001b[A\n","Epoch 1:  24%|██▍       | 32/131 [06:34\u003c20:05, 12.18s/it, training_loss=0.547]\u001b[A\n","Epoch 1:  25%|██▌       | 33/131 [06:34\u003c19:04, 11.68s/it, training_loss=0.547]\u001b[A\n","Epoch 1:  25%|██▌       | 33/131 [06:47\u003c19:04, 11.68s/it, training_loss=0.499]\u001b[A\n","Epoch 1:  26%|██▌       | 34/131 [06:47\u003c19:26, 12.03s/it, training_loss=0.499]\u001b[A\n","Epoch 1:  26%|██▌       | 34/131 [07:00\u003c19:26, 12.03s/it, training_loss=0.480]\u001b[A\n","Epoch 1:  27%|██▋       | 35/131 [07:00\u003c19:39, 12.29s/it, training_loss=0.480]\u001b[A\n","Epoch 1:  27%|██▋       | 35/131 [07:10\u003c19:39, 12.29s/it, training_loss=0.417]\u001b[A\n","Epoch 1:  27%|██▋       | 36/131 [07:10\u003c18:38, 11.78s/it, training_loss=0.417]\u001b[A\n","Epoch 1:  27%|██▋       | 36/131 [07:23\u003c18:38, 11.78s/it, training_loss=0.591]\u001b[A\n","Epoch 1:  28%|██▊       | 37/131 [07:23\u003c18:37, 11.89s/it, training_loss=0.591]\u001b[A\n","Epoch 1:  28%|██▊       | 37/131 [07:35\u003c18:37, 11.89s/it, training_loss=0.582]\u001b[A\n","Epoch 1:  29%|██▉       | 38/131 [07:35\u003c18:51, 12.17s/it, training_loss=0.582]\u001b[A\n","Epoch 1:  29%|██▉       | 38/131 [07:49\u003c18:51, 12.17s/it, training_loss=0.342]\u001b[A\n","Epoch 1:  30%|██▉       | 39/131 [07:49\u003c19:05, 12.45s/it, training_loss=0.342]\u001b[A\n","Epoch 1:  30%|██▉       | 39/131 [07:59\u003c19:05, 12.45s/it, training_loss=0.544]\u001b[A\n","Epoch 1:  31%|███       | 40/131 [07:59\u003c18:02, 11.90s/it, training_loss=0.544]\u001b[A\n","Epoch 1:  31%|███       | 40/131 [08:12\u003c18:02, 11.90s/it, training_loss=0.517]\u001b[A\n","Epoch 1:  31%|███▏      | 41/131 [08:12\u003c18:16, 12.18s/it, training_loss=0.517]\u001b[A\n","Epoch 1:  31%|███▏      | 41/131 [08:25\u003c18:16, 12.18s/it, training_loss=0.587]\u001b[A\n","Epoch 1:  32%|███▏      | 42/131 [08:25\u003c18:22, 12.39s/it, training_loss=0.587]\u001b[A\n","Epoch 1:  32%|███▏      | 42/131 [08:35\u003c18:22, 12.39s/it, training_loss=0.487]\u001b[A\n","Epoch 1:  33%|███▎      | 43/131 [08:35\u003c17:19, 11.82s/it, training_loss=0.487]\u001b[A\n","Epoch 1:  33%|███▎      | 43/131 [08:47\u003c17:19, 11.82s/it, training_loss=0.666]\u001b[A\n","Epoch 1:  34%|███▎      | 44/131 [08:47\u003c17:15, 11.90s/it, training_loss=0.666]\u001b[A\n","Epoch 1:  34%|███▎      | 44/131 [09:00\u003c17:15, 11.90s/it, training_loss=0.656]\u001b[A\n","Epoch 1:  34%|███▍      | 45/131 [09:00\u003c17:29, 12.20s/it, training_loss=0.656]\u001b[A\n","Epoch 1:  34%|███▍      | 45/131 [09:12\u003c17:29, 12.20s/it, training_loss=0.761]\u001b[A\n","Epoch 1:  35%|███▌      | 46/131 [09:12\u003c17:06, 12.07s/it, training_loss=0.761]\u001b[A\n","Epoch 1:  35%|███▌      | 46/131 [09:23\u003c17:06, 12.07s/it, training_loss=0.306]\u001b[A\n","Epoch 1:  36%|███▌      | 47/131 [09:23\u003c16:26, 11.74s/it, training_loss=0.306]\u001b[A\n","Epoch 1:  36%|███▌      | 47/131 [09:36\u003c16:26, 11.74s/it, training_loss=0.442]\u001b[A\n","Epoch 1:  37%|███▋      | 48/131 [09:36\u003c16:41, 12.07s/it, training_loss=0.442]\u001b[A\n","Epoch 1:  37%|███▋      | 48/131 [09:49\u003c16:41, 12.07s/it, training_loss=0.399]\u001b[A\n","Epoch 1:  37%|███▋      | 49/131 [09:49\u003c16:46, 12.27s/it, training_loss=0.399]\u001b[A\n","Epoch 1:  37%|███▋      | 49/131 [09:59\u003c16:46, 12.27s/it, training_loss=0.626]\u001b[A\n","Epoch 1:  38%|███▊      | 50/131 [09:59\u003c15:44, 11.66s/it, training_loss=0.626]\u001b[A\n","Epoch 1:  38%|███▊      | 50/131 [10:11\u003c15:44, 11.66s/it, training_loss=0.610]\u001b[A\n","Epoch 1:  39%|███▉      | 51/131 [10:11\u003c15:54, 11.94s/it, training_loss=0.610]\u001b[A\n","Epoch 1:  39%|███▉      | 51/131 [10:24\u003c15:54, 11.94s/it, training_loss=0.749]\u001b[A\n","Epoch 1:  40%|███▉      | 52/131 [10:24\u003c16:05, 12.22s/it, training_loss=0.749]\u001b[A\n","Epoch 1:  40%|███▉      | 52/131 [10:36\u003c16:05, 12.22s/it, training_loss=0.409]\u001b[A\n","Epoch 1:  40%|████      | 53/131 [10:36\u003c15:28, 11.91s/it, training_loss=0.409]\u001b[A\n","Epoch 1:  40%|████      | 53/131 [10:47\u003c15:28, 11.91s/it, training_loss=0.573]\u001b[A\n","Epoch 1:  41%|████      | 54/131 [10:47\u003c15:06, 11.77s/it, training_loss=0.573]\u001b[A\n","Epoch 1:  41%|████      | 54/131 [11:00\u003c15:06, 11.77s/it, training_loss=0.276]\u001b[A\n","Epoch 1:  42%|████▏     | 55/131 [11:00\u003c15:20, 12.11s/it, training_loss=0.276]\u001b[A\n","Epoch 1:  42%|████▏     | 55/131 [11:12\u003c15:20, 12.11s/it, training_loss=0.541]\u001b[A\n","Epoch 1:  43%|████▎     | 56/131 [11:12\u003c15:16, 12.22s/it, training_loss=0.541]\u001b[A\n","Epoch 1:  43%|████▎     | 56/131 [11:23\u003c15:16, 12.22s/it, training_loss=0.365]\u001b[A\n","Epoch 1:  44%|████▎     | 57/131 [11:23\u003c14:23, 11.67s/it, training_loss=0.365]\u001b[A\n","Epoch 1:  44%|████▎     | 57/131 [11:36\u003c14:23, 11.67s/it, training_loss=0.330]\u001b[A\n","Epoch 1:  44%|████▍     | 58/131 [11:36\u003c14:38, 12.04s/it, training_loss=0.330]\u001b[A\n","Epoch 1:  44%|████▍     | 58/131 [11:49\u003c14:38, 12.04s/it, training_loss=0.557]\u001b[A\n","Epoch 1:  45%|████▌     | 59/131 [11:49\u003c14:45, 12.29s/it, training_loss=0.557]\u001b[A\n","Epoch 1:  45%|████▌     | 59/131 [11:59\u003c14:45, 12.29s/it, training_loss=0.370]\u001b[A\n","Epoch 1:  46%|████▌     | 60/131 [11:59\u003c14:02, 11.87s/it, training_loss=0.370]\u001b[A\n","Epoch 1:  46%|████▌     | 60/131 [12:11\u003c14:02, 11.87s/it, training_loss=0.391]\u001b[A\n","Epoch 1:  47%|████▋     | 61/131 [12:11\u003c13:50, 11.86s/it, training_loss=0.391]\u001b[A\n","Epoch 1:  47%|████▋     | 61/131 [12:24\u003c13:50, 11.86s/it, training_loss=0.421]\u001b[A\n","Epoch 1:  47%|████▋     | 62/131 [12:24\u003c13:59, 12.17s/it, training_loss=0.421]\u001b[A\n","Epoch 1:  47%|████▋     | 62/131 [12:36\u003c13:59, 12.17s/it, training_loss=0.503]\u001b[A\n","Epoch 1:  48%|████▊     | 63/131 [12:36\u003c13:44, 12.13s/it, training_loss=0.503]\u001b[A\n","Epoch 1:  48%|████▊     | 63/131 [12:47\u003c13:44, 12.13s/it, training_loss=0.297]\u001b[A\n","Epoch 1:  49%|████▉     | 64/131 [12:47\u003c13:04, 11.72s/it, training_loss=0.297]\u001b[A\n","Epoch 1:  49%|████▉     | 64/131 [13:00\u003c13:04, 11.72s/it, training_loss=0.381]\u001b[A\n","Epoch 1:  50%|████▉     | 65/131 [13:00\u003c13:15, 12.05s/it, training_loss=0.381]\u001b[A\n","Epoch 1:  50%|████▉     | 65/131 [13:13\u003c13:15, 12.05s/it, training_loss=0.625]\u001b[A\n","Epoch 1:  50%|█████     | 66/131 [13:13\u003c13:19, 12.29s/it, training_loss=0.625]\u001b[A\n","Epoch 1:  50%|█████     | 66/131 [13:23\u003c13:19, 12.29s/it, training_loss=0.314]\u001b[A\n","Epoch 1:  51%|█████     | 67/131 [13:23\u003c12:30, 11.72s/it, training_loss=0.314]\u001b[A\n","Epoch 1:  51%|█████     | 67/131 [13:35\u003c12:30, 11.72s/it, training_loss=0.383]\u001b[A\n","Epoch 1:  52%|█████▏    | 68/131 [13:35\u003c12:29, 11.90s/it, training_loss=0.383]\u001b[A\n","Epoch 1:  52%|█████▏    | 68/131 [13:48\u003c12:29, 11.90s/it, training_loss=0.269]\u001b[A\n","Epoch 1:  53%|█████▎    | 69/131 [13:48\u003c12:36, 12.20s/it, training_loss=0.269]\u001b[A\n","Epoch 1:  53%|█████▎    | 69/131 [14:00\u003c12:36, 12.20s/it, training_loss=0.559]\u001b[A\n","Epoch 1:  53%|█████▎    | 70/131 [14:00\u003c12:13, 12.03s/it, training_loss=0.559]\u001b[A\n","Epoch 1:  53%|█████▎    | 70/131 [14:11\u003c12:13, 12.03s/it, training_loss=0.461]\u001b[A\n","Epoch 1:  54%|█████▍    | 71/131 [14:11\u003c11:45, 11.76s/it, training_loss=0.461]\u001b[A\n","Epoch 1:  54%|█████▍    | 71/131 [14:24\u003c11:45, 11.76s/it, training_loss=0.554]\u001b[A\n","Epoch 1:  55%|█████▍    | 72/131 [14:24\u003c11:54, 12.12s/it, training_loss=0.554]\u001b[A\n","Epoch 1:  55%|█████▍    | 72/131 [14:37\u003c11:54, 12.12s/it, training_loss=0.666]\u001b[A\n","Epoch 1:  56%|█████▌    | 73/131 [14:37\u003c11:51, 12.26s/it, training_loss=0.666]\u001b[A\n","Epoch 1:  56%|█████▌    | 73/131 [14:47\u003c11:51, 12.26s/it, training_loss=0.671]\u001b[A\n","Epoch 1:  56%|█████▋    | 74/131 [14:47\u003c11:04, 11.67s/it, training_loss=0.671]\u001b[A\n","Epoch 1:  56%|█████▋    | 74/131 [15:00\u003c11:04, 11.67s/it, training_loss=0.622]\u001b[A\n","Epoch 1:  57%|█████▋    | 75/131 [15:00\u003c11:13, 12.03s/it, training_loss=0.622]\u001b[A\n","Epoch 1:  57%|█████▋    | 75/131 [15:13\u003c11:13, 12.03s/it, training_loss=0.707]\u001b[A\n","Epoch 1:  58%|█████▊    | 76/131 [15:13\u003c11:15, 12.29s/it, training_loss=0.707]\u001b[A\n","Epoch 1:  58%|█████▊    | 76/131 [15:23\u003c11:15, 12.29s/it, training_loss=0.477]\u001b[A\n","Epoch 1:  59%|█████▉    | 77/131 [15:24\u003c10:41, 11.88s/it, training_loss=0.477]\u001b[A\n","Epoch 1:  59%|█████▉    | 77/131 [15:35\u003c10:41, 11.88s/it, training_loss=0.342]\u001b[A\n","Epoch 1:  60%|█████▉    | 78/131 [15:35\u003c10:27, 11.85s/it, training_loss=0.342]\u001b[A\n","Epoch 1:  60%|█████▉    | 78/131 [15:49\u003c10:27, 11.85s/it, training_loss=0.337]\u001b[A\n","Epoch 1:  60%|██████    | 79/131 [15:49\u003c10:48, 12.46s/it, training_loss=0.337]\u001b[A\n","Epoch 1:  60%|██████    | 79/131 [16:01\u003c10:48, 12.46s/it, training_loss=0.481]\u001b[A\n","Epoch 1:  61%|██████    | 80/131 [16:01\u003c10:31, 12.39s/it, training_loss=0.481]\u001b[A\n","Epoch 1:  61%|██████    | 80/131 [16:12\u003c10:31, 12.39s/it, training_loss=0.367]\u001b[A\n","Epoch 1:  62%|██████▏   | 81/131 [16:12\u003c09:53, 11.87s/it, training_loss=0.367]\u001b[A\n","Epoch 1:  62%|██████▏   | 81/131 [16:25\u003c09:53, 11.87s/it, training_loss=0.238]\u001b[A\n","Epoch 1:  63%|██████▎   | 82/131 [16:25\u003c09:55, 12.16s/it, training_loss=0.238]\u001b[A\n","Epoch 1:  63%|██████▎   | 82/131 [16:38\u003c09:55, 12.16s/it, training_loss=0.472]\u001b[A\n","Epoch 1:  63%|██████▎   | 83/131 [16:38\u003c09:53, 12.36s/it, training_loss=0.472]\u001b[A\n","Epoch 1:  63%|██████▎   | 83/131 [16:48\u003c09:53, 12.36s/it, training_loss=0.642]\u001b[A\n","Epoch 1:  64%|██████▍   | 84/131 [16:48\u003c09:13, 11.79s/it, training_loss=0.642]\u001b[A\n","Epoch 1:  64%|██████▍   | 84/131 [17:00\u003c09:13, 11.79s/it, training_loss=0.522]\u001b[A\n","Epoch 1:  65%|██████▍   | 85/131 [17:00\u003c09:08, 11.92s/it, training_loss=0.522]\u001b[A\n","Epoch 1:  65%|██████▍   | 85/131 [17:13\u003c09:08, 11.92s/it, training_loss=0.446]\u001b[A\n","Epoch 1:  66%|██████▌   | 86/131 [17:13\u003c09:09, 12.22s/it, training_loss=0.446]\u001b[A\n","Epoch 1:  66%|██████▌   | 86/131 [17:25\u003c09:09, 12.22s/it, training_loss=0.642]\u001b[A\n","Epoch 1:  66%|██████▋   | 87/131 [17:25\u003c08:50, 12.06s/it, training_loss=0.642]\u001b[A\n","Epoch 1:  66%|██████▋   | 87/131 [17:36\u003c08:50, 12.06s/it, training_loss=0.394]\u001b[A\n","Epoch 1:  67%|██████▋   | 88/131 [17:36\u003c08:25, 11.77s/it, training_loss=0.394]\u001b[A\n","Epoch 1:  67%|██████▋   | 88/131 [17:49\u003c08:25, 11.77s/it, training_loss=0.593]\u001b[A\n","Epoch 1:  68%|██████▊   | 89/131 [17:49\u003c08:27, 12.09s/it, training_loss=0.593]\u001b[A\n","Epoch 1:  68%|██████▊   | 89/131 [18:02\u003c08:27, 12.09s/it, training_loss=0.482]\u001b[A\n","Epoch 1:  69%|██████▊   | 90/131 [18:02\u003c08:23, 12.29s/it, training_loss=0.482]\u001b[A\n","Epoch 1:  69%|██████▊   | 90/131 [18:12\u003c08:23, 12.29s/it, training_loss=0.643]\u001b[A\n","Epoch 1:  69%|██████▉   | 91/131 [18:12\u003c07:45, 11.63s/it, training_loss=0.643]\u001b[A\n","Epoch 1:  69%|██████▉   | 91/131 [18:25\u003c07:45, 11.63s/it, training_loss=0.247]\u001b[A\n","Epoch 1:  70%|███████   | 92/131 [18:25\u003c07:47, 12.00s/it, training_loss=0.247]\u001b[A\n","Epoch 1:  70%|███████   | 92/131 [18:37\u003c07:47, 12.00s/it, training_loss=0.500]\u001b[A\n","Epoch 1:  71%|███████   | 93/131 [18:37\u003c07:45, 12.26s/it, training_loss=0.500]\u001b[A\n","Epoch 1:  71%|███████   | 93/131 [18:49\u003c07:45, 12.26s/it, training_loss=0.356]\u001b[A\n","Epoch 1:  72%|███████▏  | 94/131 [18:49\u003c07:20, 11.92s/it, training_loss=0.356]\u001b[A\n","Epoch 1:  72%|███████▏  | 94/131 [19:00\u003c07:20, 11.92s/it, training_loss=0.495]\u001b[A\n","Epoch 1:  73%|███████▎  | 95/131 [19:00\u003c07:05, 11.82s/it, training_loss=0.495]\u001b[A\n","Epoch 1:  73%|███████▎  | 95/131 [19:13\u003c07:05, 11.82s/it, training_loss=0.365]\u001b[A\n","Epoch 1:  73%|███████▎  | 96/131 [19:13\u003c07:05, 12.15s/it, training_loss=0.365]\u001b[A\n","Epoch 1:  73%|███████▎  | 96/131 [19:25\u003c07:05, 12.15s/it, training_loss=0.507]\u001b[A\n","Epoch 1:  74%|███████▍  | 97/131 [19:25\u003c06:54, 12.20s/it, training_loss=0.507]\u001b[A\n","Epoch 1:  74%|███████▍  | 97/131 [19:36\u003c06:54, 12.20s/it, training_loss=0.363]\u001b[A\n","Epoch 1:  75%|███████▍  | 98/131 [19:36\u003c06:26, 11.70s/it, training_loss=0.363]\u001b[A\n","Epoch 1:  75%|███████▍  | 98/131 [19:49\u003c06:26, 11.70s/it, training_loss=0.521]\u001b[A\n","Epoch 1:  76%|███████▌  | 99/131 [19:49\u003c06:25, 12.06s/it, training_loss=0.521]\u001b[A\n","Epoch 1:  76%|███████▌  | 99/131 [20:02\u003c06:25, 12.06s/it, training_loss=0.353]\u001b[A\n","Epoch 1:  76%|███████▋  | 100/131 [20:02\u003c06:21, 12.32s/it, training_loss=0.353]\u001b[A\n","Epoch 1:  76%|███████▋  | 100/131 [20:12\u003c06:21, 12.32s/it, training_loss=0.537]\u001b[A\n","Epoch 1:  77%|███████▋  | 101/131 [20:12\u003c05:53, 11.78s/it, training_loss=0.537]\u001b[A\n","Epoch 1:  77%|███████▋  | 101/131 [20:24\u003c05:53, 11.78s/it, training_loss=0.508]\u001b[A\n","Epoch 1:  78%|███████▊  | 102/131 [20:24\u003c05:44, 11.88s/it, training_loss=0.508]\u001b[A\n","Epoch 1:  78%|███████▊  | 102/131 [20:37\u003c05:44, 11.88s/it, training_loss=0.419]\u001b[A\n","Epoch 1:  79%|███████▊  | 103/131 [20:37\u003c05:40, 12.17s/it, training_loss=0.419]\u001b[A\n","Epoch 1:  79%|███████▊  | 103/131 [20:49\u003c05:40, 12.17s/it, training_loss=0.167]\u001b[A\n","Epoch 1:  79%|███████▉  | 104/131 [20:49\u003c05:24, 12.03s/it, training_loss=0.167]\u001b[A\n","Epoch 1:  79%|███████▉  | 104/131 [21:00\u003c05:24, 12.03s/it, training_loss=0.516]\u001b[A\n","Epoch 1:  80%|████████  | 105/131 [21:00\u003c05:05, 11.73s/it, training_loss=0.516]\u001b[A\n","Epoch 1:  80%|████████  | 105/131 [21:13\u003c05:05, 11.73s/it, training_loss=0.354]\u001b[A\n","Epoch 1:  81%|████████  | 106/131 [21:13\u003c05:01, 12.07s/it, training_loss=0.354]\u001b[A\n","Epoch 1:  81%|████████  | 106/131 [21:26\u003c05:01, 12.07s/it, training_loss=0.435]\u001b[A\n","Epoch 1:  82%|████████▏ | 107/131 [21:26\u003c04:54, 12.28s/it, training_loss=0.435]\u001b[A\n","Epoch 1:  82%|████████▏ | 107/131 [21:36\u003c04:54, 12.28s/it, training_loss=0.246]\u001b[A\n","Epoch 1:  82%|████████▏ | 108/131 [21:36\u003c04:27, 11.64s/it, training_loss=0.246]\u001b[A\n","Epoch 1:  82%|████████▏ | 108/131 [21:49\u003c04:27, 11.64s/it, training_loss=0.602]\u001b[A\n","Epoch 1:  83%|████████▎ | 109/131 [21:49\u003c04:24, 12.00s/it, training_loss=0.602]\u001b[A\n","Epoch 1:  83%|████████▎ | 109/131 [22:02\u003c04:24, 12.00s/it, training_loss=0.366]\u001b[A\n","Epoch 1:  84%|████████▍ | 110/131 [22:02\u003c04:17, 12.27s/it, training_loss=0.366]\u001b[A\n","Epoch 1:  84%|████████▍ | 110/131 [22:13\u003c04:17, 12.27s/it, training_loss=0.609]\u001b[A\n","Epoch 1:  85%|████████▍ | 111/131 [22:13\u003c03:58, 11.91s/it, training_loss=0.609]\u001b[A\n","Epoch 1:  85%|████████▍ | 111/131 [22:24\u003c03:58, 11.91s/it, training_loss=0.428]\u001b[A\n","Epoch 1:  85%|████████▌ | 112/131 [22:24\u003c03:44, 11.82s/it, training_loss=0.428]\u001b[A\n","Epoch 1:  85%|████████▌ | 112/131 [22:37\u003c03:44, 11.82s/it, training_loss=0.468]\u001b[A\n","Epoch 1:  86%|████████▋ | 113/131 [22:37\u003c03:38, 12.13s/it, training_loss=0.468]\u001b[A\n","Epoch 1:  86%|████████▋ | 113/131 [22:49\u003c03:38, 12.13s/it, training_loss=0.367]\u001b[A\n","Epoch 1:  87%|████████▋ | 114/131 [22:49\u003c03:26, 12.16s/it, training_loss=0.367]\u001b[A\n","Epoch 1:  87%|████████▋ | 114/131 [23:00\u003c03:26, 12.16s/it, training_loss=0.566]\u001b[A\n","Epoch 1:  88%|████████▊ | 115/131 [23:00\u003c03:08, 11.78s/it, training_loss=0.566]\u001b[A\n","Epoch 1:  88%|████████▊ | 115/131 [23:13\u003c03:08, 11.78s/it, training_loss=0.651]\u001b[A\n","Epoch 1:  89%|████████▊ | 116/131 [23:13\u003c03:01, 12.09s/it, training_loss=0.651]\u001b[A\n","Epoch 1:  89%|████████▊ | 116/131 [23:26\u003c03:01, 12.09s/it, training_loss=0.283]\u001b[A\n","Epoch 1:  89%|████████▉ | 117/131 [23:26\u003c02:52, 12.33s/it, training_loss=0.283]\u001b[A\n","Epoch 1:  89%|████████▉ | 117/131 [23:36\u003c02:52, 12.33s/it, training_loss=0.469]\u001b[A\n","Epoch 1:  90%|█████████ | 118/131 [23:36\u003c02:33, 11.80s/it, training_loss=0.469]\u001b[A\n","Epoch 1:  90%|█████████ | 118/131 [23:49\u003c02:33, 11.80s/it, training_loss=0.602]\u001b[A\n","Epoch 1:  91%|█████████ | 119/131 [23:49\u003c02:22, 11.88s/it, training_loss=0.602]\u001b[A\n","Epoch 1:  91%|█████████ | 119/131 [24:01\u003c02:22, 11.88s/it, training_loss=0.156]\u001b[A\n","Epoch 1:  92%|█████████▏| 120/131 [24:01\u003c02:13, 12.18s/it, training_loss=0.156]\u001b[A\n","Epoch 1:  92%|█████████▏| 120/131 [24:13\u003c02:13, 12.18s/it, training_loss=0.227]\u001b[A\n","Epoch 1:  92%|█████████▏| 121/131 [24:13\u003c02:00, 12.02s/it, training_loss=0.227]\u001b[A\n","Epoch 1:  92%|█████████▏| 121/131 [24:24\u003c02:00, 12.02s/it, training_loss=0.366]\u001b[A\n","Epoch 1:  93%|█████████▎| 122/131 [24:24\u003c01:45, 11.72s/it, training_loss=0.366]\u001b[A\n","Epoch 1:  93%|█████████▎| 122/131 [24:37\u003c01:45, 11.72s/it, training_loss=0.355]\u001b[A\n","Epoch 1:  94%|█████████▍| 123/131 [24:37\u003c01:36, 12.02s/it, training_loss=0.355]\u001b[A\n","Epoch 1:  94%|█████████▍| 123/131 [24:49\u003c01:36, 12.02s/it, training_loss=0.367]\u001b[A\n","Epoch 1:  95%|█████████▍| 124/131 [24:49\u003c01:25, 12.19s/it, training_loss=0.367]\u001b[A\n","Epoch 1:  95%|█████████▍| 124/131 [24:59\u003c01:25, 12.19s/it, training_loss=0.491]\u001b[A\n","Epoch 1:  95%|█████████▌| 125/131 [24:59\u003c01:09, 11.57s/it, training_loss=0.491]\u001b[A\n","Epoch 1:  95%|█████████▌| 125/131 [25:12\u003c01:09, 11.57s/it, training_loss=0.475]\u001b[A\n","Epoch 1:  96%|█████████▌| 126/131 [25:12\u003c00:59, 11.94s/it, training_loss=0.475]\u001b[A\n","Epoch 1:  96%|█████████▌| 126/131 [25:25\u003c00:59, 11.94s/it, training_loss=0.238]\u001b[A\n","Epoch 1:  97%|█████████▋| 127/131 [25:25\u003c00:48, 12.21s/it, training_loss=0.238]\u001b[A\n","Epoch 1:  97%|█████████▋| 127/131 [25:36\u003c00:48, 12.21s/it, training_loss=0.585]\u001b[A\n","Epoch 1:  98%|█████████▊| 128/131 [25:36\u003c00:35, 11.81s/it, training_loss=0.585]\u001b[A\n","Epoch 1:  98%|█████████▊| 128/131 [25:47\u003c00:35, 11.81s/it, training_loss=0.367]\u001b[A\n","Epoch 1:  98%|█████████▊| 129/131 [25:47\u003c00:23, 11.62s/it, training_loss=0.367]\u001b[A\n","Epoch 1:  98%|█████████▊| 129/131 [26:00\u003c00:23, 11.62s/it, training_loss=0.393]\u001b[A\n","Epoch 1:  99%|█████████▉| 130/131 [26:00\u003c00:11, 11.86s/it, training_loss=0.393]\u001b[A\n","Epoch 1:  99%|█████████▉| 130/131 [26:03\u003c00:11, 11.86s/it, training_loss=0.570]\u001b[A\n","Epoch 1: 100%|██████████| 131/131 [26:03\u003c00:00,  9.31s/it, training_loss=0.570]\u001b[A\n","  0%|          | 0/4 [26:03\u003c?, ?it/s]\n"]},{"ename":"RuntimeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-55-b277799f1436\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0masag_systems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Paper_TA_ASAG/DATASET_TA/Data/Data Lagi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m\u003cipython-input-54-33da14a21b9b\u003e\u001b[0m in \u001b[0;36masag_systems\u001b[0;34m(path_dir)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.xslx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 47\u001b[0;31m             \u001b[0mtrain_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m\u003cipython-input-51-8d76b8aa0705\u003e\u001b[0m in \u001b[0;36mtrain_eval\u001b[0;34m(df_final, pretrainedmodel)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 180\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'/content/drive/MyDrive/Paper_TA_ASAG/DATASET_TA/Data/Model_Save/finetuned_BERT_{df_final[\"path\"]}_epoch_{epoch}.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nEpoch {epoch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 422\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 309\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_writer_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 287\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_zipfile_writer_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: File /content/drive/MyDrive/Paper_TA_ASAG/DATASET_TA/Data/Model_Save/finetuned_BERT_0       augmentasi data\n1       augmentasi data\n2       augmentasi data\n3       augmentasi data\n4       augmentasi data\n             ...       \n646     augmentasi data\n647     augmentasi data\n648     augmentasi data\n649     augmentasi data\n650     augmentasi data\nName: path, Length: 651, dtype: object_epoch_1.model cannot be opened."]}],"source":["asag_systems('/content/drive/MyDrive/Paper_TA_ASAG/DATASET_TA/Data/Data Lagi')"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1679824512168,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"32c4042a"},"outputs":[],"source":["seed_val = 17\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","punctuation = r'[^\\w\\s]'\n","\n","def f1_score_func(preds, labels):\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return f1_score(labels_flat, preds_flat, average='weighted')\n","\n","def qwk_score_func(preds, labels):\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return cohen_kappa_score(labels_flat, preds_flat)\n","\n","def accuracy_per_class(preds, labels):\n","    label_dict_inverse = {v: k for k, v in label_dict.items()}\n","\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    for label in np.unique(labels_flat):\n","        y_preds = preds_flat[labels_flat==label]\n","        y_true = labels_flat[labels_flat==label]\n","        print(f'Class: {label_dict_inverse[label]}')\n","        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n","\n","def evaluate(dataloader_val, device, model):\n","\n","    model.eval()\n","\n","    loss_val_total = 0\n","    predictions, true_vals = [], []\n","\n","    for batch in dataloader_val:\n","\n","        batch = tuple(b.to(device) for b in batch)\n","\n","        inputs = {'input_ids':      batch[0],\n","                  'attention_mask': batch[1],\n","                  'labels':         batch[2],\n","                 }\n","\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","\n","        loss = outputs[0]\n","        logits = outputs[1]\n","        loss_val_total += loss.item()\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = inputs['labels'].cpu().numpy()\n","        predictions.append(logits)\n","        true_vals.append(label_ids)\n","\n","    loss_val_avg = loss_val_total/len(dataloader_val)\n","\n","    predictions = np.concatenate(predictions, axis=0)\n","    true_vals = np.concatenate(true_vals, axis=0)\n","\n","    return loss_val_avg, predictions, true_vals\n","\n","def train_eval(df_final, pretrainedmodel):\n","    # bin nilai (continuous variable) into intervals\n","    df_final['nilai'] = pd.qcut(df_final['nilai'], 5, labels=False)\n","\n","    df_final['jawaban'] = df_final['jawaban'].apply(lambda x: x.lower())\n","    df_final['jawaban'] = df_final['jawaban'].apply(lambda x: re.sub(punctuation, '', x))\n","    df_final['jawaban'] = df_final['jawaban'].apply(nltk.word_tokenize)\n","    df_final['jawaban'] = df_final['jawaban'].apply(lambda x: [word for word in x if word not in stop_words])\n","    df_final['jawaban'] = df_final['jawaban'].apply(lambda x: remove_stopword(x))\n","    df_final['jawaban'] = df_final['jawaban'].apply(lambda x: hapus_kata_negasi(x))\n","    df_final['jawaban'] = df_final['jawaban'].apply(lambda x: remove_stopword(x))\n","\n","    # make sure that the training set and test set ratio is 80:20\n","    add = len(df_final[df_final['tipe'] == 'test']) - (round(0.2*(len(df_final[df_final['tipe'] == 'train'])+len(df_final[df_final['tipe'] == 'test']))))\n","    for i in df_final[df_final['tipe'] == 'test'].sample(n = add).itertuples():\n","        df_final.at[i.Index, 'tipe'] = 'train'\n","\n","    # load model and tokenizer\n","    tokenizer = BertTokenizer.from_pretrained(pretrainedmodel, ignore_mismatched_sizes=True)\n","\n","    encoded_data_train = tokenizer.batch_encode_plus(\n","        df_final[df_final.tipe=='train']['jawaban'].values,\n","        add_special_tokens=True,\n","        return_attention_mask=True,\n","        pad_to_max_length=True,\n","        truncation=True,\n","        max_length=256,\n","        padding='max_length',\n","        return_tensors='pt'\n","    )\n","\n","    encoded_data_val = tokenizer.batch_encode_plus(\n","        df_final[df_final.tipe=='test']['jawaban'].values,\n","        add_special_tokens=True,\n","        return_attention_mask=True,\n","        pad_to_max_length=True,\n","        truncation=True,\n","        max_length=256,\n","        padding='max_length',\n","        return_tensors='pt'\n","    )\n","\n","    input_ids_train = encoded_data_train['input_ids']\n","    attention_masks_train = encoded_data_train['attention_mask']\n","    labels_train = torch.tensor(df_final[df_final.tipe=='train'].nilai.values)\n","\n","    input_ids_val = encoded_data_val['input_ids']\n","    attention_masks_val = encoded_data_val['attention_mask']\n","    labels_val = torch.tensor(df_final[df_final.tipe=='test'].nilai.values)\n","\n","    dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n","    dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n","\n","    model = BertForSequenceClassification.from_pretrained(pretrainedmodel,\n","                                                          num_labels=5,\n","                                                          output_attentions=False,\n","                                                          output_hidden_states=False, ignore_mismatched_sizes=True)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","\n","    batch_size = 15\n","\n","    dataloader_train = DataLoader(dataset_train,\n","                                  sampler=RandomSampler(dataset_train),\n","                                  batch_size=batch_size)\n","\n","    dataloader_validation = DataLoader(dataset_val,\n","                                       sampler=SequentialSampler(dataset_val),\n","                                       batch_size=batch_size)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(),\n","                      lr=1e-5,\n","                      eps=1e-8)\n","\n","    epochs = 10\n","\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps=0,\n","                                                num_training_steps=len(dataloader_train)*epochs)\n","\n","    for epoch in tqdm(range(1, epochs+1)):\n","\n","        model.train()\n","\n","        loss_train_total = 0\n","\n","        progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n","        for batch in progress_bar:\n","\n","            model.zero_grad()\n","\n","            batch = tuple(b.to(device) for b in batch)\n","\n","            inputs = {'input_ids':      batch[0],\n","                      'attention_mask': batch[1],\n","                      'labels':         batch[2],\n","                     }\n","\n","            outputs = model(**inputs)\n","\n","            loss = outputs[0]\n","            loss_train_total += loss.item()\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            optimizer.step()\n","            scheduler.step()\n","\n","            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n","\n","\n","        torch.save(model.state_dict(), f'/content/drive/MyDrive/Paper_TA_ASAG/DATASET_TA/Data/Model_Save/finetuned_BERT_{df_final[\"path\"]}_epoch_{epoch}.model')\n","\n","        tqdm.write(f'\\nEpoch {epoch}')\n","\n","        loss_train_avg = loss_train_total/len(dataloader_train)\n","        tqdm.write(f'Training loss: {loss_train_avg}')\n","\n","        val_loss, predictions, true_vals = evaluate(dataloader_validation, device, model)\n","        val_f1 = f1_score_func(predictions, true_vals)\n","        val_qwk = qwk_score_func(predictions, true_vals)\n","        tqdm.write(f'Validation loss: {val_loss}')\n","        tqdm.write(f'F1 Score (Weighted): {val_f1}')\n","        tqdm.write(f'QWK Score: {val_qwk}')"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1679824512168,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"Ju3GxLhL-4I_"},"outputs":[],"source":["def asag_systems(path_dir):\n","    list_pre_trained_model = ['indobenchmark/indobert-lite-base-p2']\n","    list_dir = os.listdir(path_dir)\n","    for m in list_pre_trained_model:\n","        print(m)\n","        for idx, ele in enumerate(list_dir):\n","            df_raw = pd.read_excel(open(path_dir+'/'+ele, 'rb'),\n","                                  sheet_name='Soal',\n","                                  header=1,\n","                                  index_col=0,\n","                                  usecols='B:D')\n","\n","            list_final = []\n","\n","            for i in df_raw.itertuples():\n","                list_final.append(\n","                    {\n","                        'jawaban': i[1],\n","                        'nilai': 100,\n","                        'tipe': 'train',\n","                        'path': f'{ele}, Negation',\n","                    }\n","                )\n","                df_tmp = pd.read_excel(open(path_dir+'/'+ele, 'rb'),\n","                                        sheet_name='No.'+str(i.Index),\n","                                        header=1,\n","                                        index_col=0,\n","                                        usecols='B:N')\n","                \n","                df_tmp = df_tmp.dropna()\n","                for j in df_tmp.itertuples():\n","                    list_final.append(\n","                        {\n","                            'jawaban': j[1],\n","                            'nilai': j[12],\n","                            'tipe': 'test',\n","                            'path': f'{ele}, Negation',\n","                        }\n","                    )\n","            if idx == 0:\n","                df_final = pd.DataFrame(list_final)\n","            else:\n","                df_final.append(pd.DataFrame(list_final), ignore_index=True)\n","\n","            print(' '.join(ele.rstrip('.xslx').split('_')))\n","    \n","            train_eval(df_final, m)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":15166,"status":"error","timestamp":1679824527329,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"8fFU1LQzO0hn","outputId":"bdc2992c-96f3-44fe-bbac-978ff2ae58fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["indobenchmark/indobert-lite-base-p2\n","Analisis Essay Grading Lifestyle\n"]},{"name":"stderr","output_type":"stream","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'AlbertTokenizerFast'. \n","The class this function is called from is 'BertTokenizer'.\n","You are using a model of type albert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n","Some weights of the model checkpoint at indobenchmark/indobert-lite-base-p2 were not used when initializing BertForSequenceClassification: ['pooler.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight', 'pooler.weight', 'encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight', 'encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight', 'encoder.albert_layer_groups.0.albert_layers.0.ffn.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias', 'encoder.embedding_hidden_mapping_in.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight', 'encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight', 'encoder.embedding_hidden_mapping_in.weight', 'encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight', 'encoder.albert_layer_groups.0.albert_layers.0.ffn.weight', 'encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight', 'encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p2 and are newly initialized: ['encoder.layer.10.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.2.attention.self.query.weight', 'classifier.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'classifier.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'pooler.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'pooler.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p2 and are newly initialized because the shapes did not match:\n","- embeddings.word_embeddings.weight: found shape torch.Size([30000, 128]) in the checkpoint and torch.Size([30000, 768]) in the model instantiated\n","- embeddings.position_embeddings.weight: found shape torch.Size([512, 128]) in the checkpoint and torch.Size([512, 768]) in the model instantiated\n","- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 128]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n","- embeddings.LayerNorm.weight: found shape torch.Size([128]) in the checkpoint and torch.Size([768]) in the model instantiated\n","- embeddings.LayerNorm.bias: found shape torch.Size([128]) in the checkpoint and torch.Size([768]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  0%|          | 0/10 [00:00\u003c?, ?it/s]\n","Epoch 1:   0%|          | 0/31 [00:00\u003c?, ?it/s]\u001b[A\n","  0%|          | 0/10 [00:06\u003c?, ?it/s]\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"name":"stdout","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"\u003cipython-input-13-780573c4e306\u003e\", line 1, in \u003cmodule\u003e\n","    asag_systems('/content/drive/MyDrive/Paper_TA_ASAG/DATASET_TA/Data')\n","  File \"\u003cipython-input-12-7a6323408cd4\u003e\", line 47, in asag_systems\n","    train_eval(df_final, m)\n","  File \"\u003cipython-input-11-0337c7d0e96b\u003e\", line 164, in train_eval\n","    outputs = model(**inputs)\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py\", line 1562, in forward\n","    outputs = self.bert(\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py\", line 1020, in forward\n","    encoder_outputs = self.encoder(\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py\", line 610, in forward\n","    layer_outputs = layer_module(\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py\", line 495, in forward\n","    self_attention_outputs = self.attention(\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py\", line 425, in forward\n","    self_outputs = self.self(\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py\", line 363, in forward\n","    context_layer = torch.matmul(attention_probs, value_layer)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.9/inspect.py\", line 1543, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.9/inspect.py\", line 1501, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.9/inspect.py\", line 709, in getsourcefile\n","    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n","  File \"/usr/lib/python3.9/inspect.py\", line 755, in getmodule\n","    os.path.realpath(f)] = module.__name__\n","  File \"/usr/lib/python3.9/posixpath.py\", line 393, in realpath\n","    return abspath(path)\n","  File \"/usr/lib/python3.9/posixpath.py\", line 382, in abspath\n","    return normpath(path)\n","  File \"/usr/lib/python3.9/posixpath.py\", line 354, in normpath\n","    path.startswith(sep*2) and not path.startswith(sep*3)):\n","KeyboardInterrupt\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"]}],"source":["asag_systems('/content/drive/MyDrive/Paper_TA_ASAG/DATASET_TA/Data')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1679824527330,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"7404f2d1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1679824527331,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"3XjjBXBJK6Sn"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"766caac7eb69e8a0a4a596af183e8606e532f32ec205da93d6afbc58c03966c0"}}},"nbformat":4,"nbformat_minor":5}