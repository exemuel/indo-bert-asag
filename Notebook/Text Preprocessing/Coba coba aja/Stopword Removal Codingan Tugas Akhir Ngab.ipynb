{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5508,"status":"ok","timestamp":1679820094260,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"MbN-qdA5z1G-","outputId":"fe93ad00-8fdf-4804-8f2e-b20e94257ffd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6530,"status":"ok","timestamp":1679820100785,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"EcXBnbTmz2nW","outputId":"047e6ee8-d5bc-4eb5-80e1-9fc7fe0f96f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sastrawi in /usr/local/lib/python3.9/dist-packages (1.0.1)\n"]}],"source":["pip install sastrawi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6797,"status":"ok","timestamp":1679820107577,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"1AVL6mTkz2x9","outputId":"af6de7d5-0117-45ea-fca8-bc4a0b7b1222"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.4.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from nltk) (1.16.0)\n"]}],"source":["pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1673,"status":"ok","timestamp":1679820109243,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"DFkbHffjz299","outputId":"390389b1-ae6f-4443-cdc7-0993af88ff05"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1679820109244,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"wRQaDdXaz6fl","outputId":"5bf57f43-5b78-4702-fadd-6b5bb9a897e8"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":752,"status":"ok","timestamp":1679820109993,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"1d61a3ba","outputId":"120e738c-ef87-4df7-8c85-1c648035fa97"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Indonesian Query Answering Dataset for Online Essay Test System.zip', 'dict.json', 'Preprocessing', 'Analysis Data', 'Text Preprocessing', 'Data', 'dataset.py', '__pycache__']\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sn\n","import os\n","import re\n","import nltk\n","import random\n","from nltk.corpus import stopwords\n","from gensim.models import Word2Vec\n","from nltk.tokenize import sent_tokenize, LineTokenizer, RegexpTokenizer\n","from nltk.tokenize import word_tokenize\n","from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n","\n","print(os.listdir(\"/content/drive/MyDrive/Paper_TA_ASAG/DATASET_TA/\"))\n","factory = StemmerFactory()\n","stemmer = factory.create_stemmer()\n","import string\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","stopwords_indonesia = stopwords.words('indonesian')\n","stop_words = set(stopwords.words('indonesian'))\n","import sys\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UbTgH1-D-kLA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6210,"status":"ok","timestamp":1679820116201,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"icJEdL3SOegH","outputId":"970428d0-6ad9-450d-ef4a-4fd4540cf646"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nlp_id in /usr/local/lib/python3.9/dist-packages (0.1.13.0)\n","Requirement already satisfied: wget==3.2 in /usr/local/lib/python3.9/dist-packages (from nlp_id) (3.2)\n","Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.9/dist-packages (from nlp_id) (3.4.5)\n","Requirement already satisfied: scikit-learn==1.1.0 in /usr/local/lib/python3.9/dist-packages (from nlp_id) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from nltk==3.4.5-\u003enlp_id) (1.16.0)\n","Requirement already satisfied: scipy\u003e=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.1.0-\u003enlp_id) (1.10.1)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.1.0-\u003enlp_id) (3.1.0)\n","Requirement already satisfied: joblib\u003e=1.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.1.0-\u003enlp_id) (1.1.1)\n","Requirement already satisfied: numpy\u003e=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.1.0-\u003enlp_id) (1.22.4)\n"]}],"source":["pip install nlp_id"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5458,"status":"ok","timestamp":1679820121654,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"LMdPOnmpOkt_","outputId":"e6d8d1cb-0a02-4c54-9828-e6a93158a9eb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.3)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tokenizers!=0.11.3,\u003c0.14,\u003e=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.11.0-\u003etransformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers) (2.0.12)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers) (2022.12.7)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers) (3.4)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers) (1.26.15)\n"]}],"source":["pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZ1HV-O7D9iM"},"outputs":[],"source":["# fungsi untuk menghapus kata negasi\n","def hapus_kata_negasi(kalimat):\n","    # membuat daftar kata negasi\n","    negasi = ['tidak', 'bukan', 'belum', 'tak', 'jangan', 'tidaklah', 'bukannya', 'tiada', 'gak', 'ngga', 'nggak', 'enggak']\n","    # melakukan split kalimat menjadi token\n","    result = []\n","    negate = False\n","    for word in kalimat:\n","        if any(neg == word for neg in negasi):\n","            negate = True\n","        elif negate and word not in string.punctuation:\n","            word = 'tidak_' + word\n","            negate = False\n","        result.append(word)\n","    return result\n","\n","def remove_stopword(token):\n","    return ' '. join(token)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t4N6fkUEOk65"},"outputs":[],"source":["import re\n","import random\n","import pandas as pd\n","import torch\n","import tensorflow as tf\n","import numpy as np\n","\n","from nlp_id.lemmatizer import Lemmatizer\n","from nltk.corpus import stopwords\n","from tqdm import tqdm\n","from sklearn.preprocessing import KBinsDiscretizer\n","from sklearn.metrics import f1_score, cohen_kappa_score\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.initializers import TruncatedNormal\n","from tensorflow.keras.losses import CategoricalCrossentropy\n","from tensorflow.keras.metrics import CategoricalAccuracy\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Input, Dense\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from nltk.corpus import stopwords"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":72,"status":"ok","timestamp":1679825652748,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"32c4042a"},"outputs":[],"source":["seed_val = 17\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","punctuation = r'[^\\w\\s]'\n","\n","def f1_score_func(preds, labels):\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return f1_score(labels_flat, preds_flat, average='weighted')\n","\n","def qwk_score_func(preds, labels):\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return cohen_kappa_score(labels_flat, preds_flat)\n","\n","def accuracy_per_class(preds, labels):\n","    label_dict_inverse = {v: k for k, v in label_dict.items()}\n","\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    for label in np.unique(labels_flat):\n","        y_preds = preds_flat[labels_flat==label]\n","        y_true = labels_flat[labels_flat==label]\n","        print(f'Class: {label_dict_inverse[label]}')\n","        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n","\n","def evaluate(dataloader_val, device, model):\n","\n","    model.eval()\n","\n","    loss_val_total = 0\n","    predictions, true_vals = [], []\n","\n","    for batch in dataloader_val:\n","\n","        batch = tuple(b.to(device) for b in batch)\n","\n","        inputs = {'input_ids':      batch[0],\n","                  'attention_mask': batch[1],\n","                  'labels':         batch[2],\n","                 }\n","\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","\n","        loss = outputs[0]\n","        logits = outputs[1]\n","        loss_val_total += loss.item()\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = inputs['labels'].cpu().numpy()\n","        predictions.append(logits)\n","        true_vals.append(label_ids)\n","\n","    loss_val_avg = loss_val_total/len(dataloader_val)\n","\n","    predictions = np.concatenate(predictions, axis=0)\n","    true_vals = np.concatenate(true_vals, axis=0)\n","\n","    return loss_val_avg, predictions, true_vals\n","\n","def train_eval(df_final, pretrainedmodel):\n","    # bin nilai (continuous variable) into intervals\n","    df_final['nilai'] = pd.qcut(df_final['nilai'], 5, labels=False)\n","\n","    df_final['jawaban'] = df_final['jawaban'].apply(lambda x: x.lower())\n","    df_final['jawaban'] = df_final['jawaban'].apply(lambda x: re.sub(punctuation, '', x))\n","    df_final['jawaban'] = df_final['jawaban'].apply(nltk.word_tokenize)\n","    df_final['jawaban'] = df_final['jawaban'].apply(lambda x: [word for word in x if word not in stop_words])\n","    df_final['jawaban'] = df_final['jawaban'].apply(lambda x: remove_stopword(x))\n","\n","    # make sure that the training set and test set ratio is 80:20\n","    add = len(df_final[df_final['tipe'] == 'test']) - (round(0.2*(len(df_final[df_final['tipe'] == 'train'])+len(df_final[df_final['tipe'] == 'test']))))\n","    for i in df_final[df_final['tipe'] == 'test'].sample(n = add).itertuples():\n","        df_final.at[i.Index, 'tipe'] = 'train'\n","\n","    # load model and tokenizer\n","    tokenizer = BertTokenizer.from_pretrained(pretrainedmodel, ignore_mismatched_sizes=True)\n","\n","    encoded_data_train = tokenizer.batch_encode_plus(\n","        df_final[df_final.tipe=='train']['jawaban'].values,\n","        add_special_tokens=True,\n","        return_attention_mask=True,\n","        pad_to_max_length=True,\n","        truncation=True,\n","        max_length=256,\n","        padding='max_length',\n","        return_tensors='pt'\n","    )\n","\n","    encoded_data_val = tokenizer.batch_encode_plus(\n","        df_final[df_final.tipe=='test']['jawaban'].values,\n","        add_special_tokens=True,\n","        return_attention_mask=True,\n","        pad_to_max_length=True,\n","        truncation=True,\n","        max_length=256,\n","        padding='max_length',\n","        return_tensors='pt'\n","    )\n","\n","    input_ids_train = encoded_data_train['input_ids']\n","    attention_masks_train = encoded_data_train['attention_mask']\n","    labels_train = torch.tensor(df_final[df_final.tipe=='train'].nilai.values)\n","\n","    input_ids_val = encoded_data_val['input_ids']\n","    attention_masks_val = encoded_data_val['attention_mask']\n","    labels_val = torch.tensor(df_final[df_final.tipe=='test'].nilai.values)\n","\n","    dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n","    dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n","\n","    model = BertForSequenceClassification.from_pretrained(pretrainedmodel,\n","                                                          num_labels=5,\n","                                                          output_attentions=False,\n","                                                          output_hidden_states=False, ignore_mismatched_sizes=True)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","\n","    batch_size = 4\n","\n","    dataloader_train = DataLoader(dataset_train,\n","                                  sampler=RandomSampler(dataset_train),\n","                                  batch_size=batch_size)\n","\n","    dataloader_validation = DataLoader(dataset_val,\n","                                       sampler=SequentialSampler(dataset_val),\n","                                       batch_size=batch_size)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(),\n","                      lr=2e-5,\n","                      eps=1e-8)\n","\n","    epochs = 4\n","\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps=0,\n","                                                num_training_steps=len(dataloader_train)*epochs)\n","\n","    for epoch in tqdm(range(1, epochs+1)):\n","\n","        model.train()\n","\n","        loss_train_total = 0\n","\n","        progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n","        for batch in progress_bar:\n","\n","            model.zero_grad()\n","\n","            batch = tuple(b.to(device) for b in batch)\n","\n","            inputs = {'input_ids':      batch[0],\n","                      'attention_mask': batch[1],\n","                      'labels':         batch[2],\n","                     }\n","\n","            outputs = model(**inputs)\n","\n","            loss = outputs[0]\n","            loss_train_total += loss.item()\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            optimizer.step()\n","            scheduler.step()\n","\n","            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n","\n","\n","        torch.save(model.state_dict(), f'/MyDrive/Paper_TA_ASAG/DATASET_TA/Data/Model_Save/finetuned_BERT_{df_final[\"path\"]}_epoch_{epoch}.model')\n","\n","        tqdm.write(f'\\nEpoch {epoch}')\n","\n","        loss_train_avg = loss_train_total/len(dataloader_train)\n","        tqdm.write(f'Training loss: {loss_train_avg}')\n","\n","        val_loss, predictions, true_vals = evaluate(dataloader_validation, device, model)\n","        val_f1 = f1_score_func(predictions, true_vals)\n","        val_qwk = qwk_score_func(predictions, true_vals)\n","        tqdm.write(f'Validation loss: {val_loss}')\n","        tqdm.write(f'F1 Score (Weighted): {val_f1}')\n","        tqdm.write(f'QWK Score: {val_qwk}')"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":922,"status":"ok","timestamp":1679828313413,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"Ju3GxLhL-4I_"},"outputs":[],"source":["def asag_systems(path_dir):\n","    list_pre_trained_model = ['indobenchmark/indobert-lite-base-p2']\n","    list_dir = os.listdir(path_dir)\n","    for m in list_pre_trained_model:\n","        print(m)\n","        for idx, ele in enumerate(list_dir):\n","            df_raw = pd.read_excel(open(path_dir+'/'+ele, 'rb'),\n","                                  sheet_name='Soal',\n","                                  header=1,\n","                                  index_col=0,\n","                                  usecols='B:D')\n","\n","            list_final = []\n","\n","            for i in df_raw.itertuples():\n","                list_final.append(\n","                    {\n","                        'jawaban': i[1],\n","                        'nilai': 100,\n","                        'tipe': 'train',\n","                        'path': f'{ele}, Negation',\n","                    }\n","                )\n","                df_tmp = pd.read_excel(open(path_dir+'/'+ele, 'rb'),\n","                                        sheet_name='No.'+str(i.Index),\n","                                        header=1,\n","                                        index_col=0,\n","                                        usecols='B:N')\n","                \n","                df_tmp = df_tmp.dropna()\n","                for j in df_tmp.itertuples():\n","                    list_final.append(\n","                        {\n","                            'jawaban': j[1],\n","                            'nilai': j[12],\n","                            'tipe': 'test',\n","                            'path': f'{ele}, Negation',\n","                        }\n","                    )\n","            if idx == 0:\n","                df_final = pd.DataFrame(list_final)\n","            else:\n","                df_final.append(pd.DataFrame(list_final), ignore_index=True)\n","\n","            print(' '.join(ele.rstrip('.xslx').split('_')))\n","    \n","            train_eval(df_final, m)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"ll93iyIwGrK-"},"outputs":[{"name":"stdout","output_type":"stream","text":["indobenchmark/indobert-lite-base-p2\n","Analisis Essay Grading Lifestyle\n"]},{"name":"stderr","output_type":"stream","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'AlbertTokenizerFast'. \n","The class this function is called from is 'BertTokenizer'.\n","You are using a model of type albert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n","Some weights of the model checkpoint at indobenchmark/indobert-lite-base-p2 were not used when initializing BertForSequenceClassification: ['encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight', 'encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight', 'encoder.albert_layer_groups.0.albert_layers.0.ffn.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight', 'encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias', 'encoder.embedding_hidden_mapping_in.weight', 'pooler.weight', 'encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight', 'encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias', 'pooler.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight', 'encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias', 'encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight', 'encoder.embedding_hidden_mapping_in.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight', 'encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias', 'encoder.albert_layer_groups.0.albert_layers.0.ffn.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p2 and are newly initialized: ['encoder.layer.0.attention.self.value.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'classifier.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'classifier.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.1.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.attention.self.value.weight', 'pooler.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.3.attention.self.query.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p2 and are newly initialized because the shapes did not match:\n","- embeddings.word_embeddings.weight: found shape torch.Size([30000, 128]) in the checkpoint and torch.Size([30000, 768]) in the model instantiated\n","- embeddings.position_embeddings.weight: found shape torch.Size([512, 128]) in the checkpoint and torch.Size([512, 768]) in the model instantiated\n","- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 128]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n","- embeddings.LayerNorm.weight: found shape torch.Size([128]) in the checkpoint and torch.Size([768]) in the model instantiated\n","- embeddings.LayerNorm.bias: found shape torch.Size([128]) in the checkpoint and torch.Size([768]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  0%|          | 0/4 [00:00\u003c?, ?it/s]\n","Epoch 1:   0%|          | 0/116 [00:00\u003c?, ?it/s]\u001b[A\n","Epoch 1:   0%|          | 0/116 [00:12\u003c?, ?it/s, training_loss=0.531]\u001b[A\n","Epoch 1:   1%|          | 1/116 [00:12\u003c24:48, 12.94s/it, training_loss=0.531]\u001b[A\n","Epoch 1:   1%|          | 1/116 [00:24\u003c24:48, 12.94s/it, training_loss=0.852]\u001b[A\n","Epoch 1:   2%|▏         | 2/116 [00:24\u003c22:49, 12.02s/it, training_loss=0.852]\u001b[A\n","Epoch 1:   2%|▏         | 2/116 [00:35\u003c22:49, 12.02s/it, training_loss=0.408]\u001b[A\n","Epoch 1:   3%|▎         | 3/116 [00:35\u003c22:00, 11.69s/it, training_loss=0.408]\u001b[A\n","Epoch 1:   3%|▎         | 3/116 [00:48\u003c22:00, 11.69s/it, training_loss=0.822]\u001b[A\n","Epoch 1:   3%|▎         | 4/116 [00:48\u003c22:38, 12.13s/it, training_loss=0.822]\u001b[A\n","Epoch 1:   3%|▎         | 4/116 [01:00\u003c22:38, 12.13s/it, training_loss=0.488]\u001b[A\n","Epoch 1:   4%|▍         | 5/116 [01:00\u003c22:41, 12.27s/it, training_loss=0.488]\u001b[A\n","Epoch 1:   4%|▍         | 5/116 [01:11\u003c22:41, 12.27s/it, training_loss=0.495]\u001b[A\n","Epoch 1:   5%|▌         | 6/116 [01:11\u003c21:15, 11.60s/it, training_loss=0.495]\u001b[A\n","Epoch 1:   5%|▌         | 6/116 [01:24\u003c21:15, 11.60s/it, training_loss=0.566]\u001b[A\n","Epoch 1:   6%|▌         | 7/116 [01:24\u003c21:46, 11.99s/it, training_loss=0.566]\u001b[A\n","Epoch 1:   6%|▌         | 7/116 [01:37\u003c21:46, 11.99s/it, training_loss=0.832]\u001b[A\n","Epoch 1:   7%|▋         | 8/116 [01:37\u003c22:38, 12.58s/it, training_loss=0.832]\u001b[A\n","Epoch 1:   7%|▋         | 8/116 [01:49\u003c22:38, 12.58s/it, training_loss=0.651]\u001b[A\n","Epoch 1:   8%|▊         | 9/116 [01:49\u003c21:47, 12.22s/it, training_loss=0.651]\u001b[A\n","Epoch 1:   8%|▊         | 9/116 [02:00\u003c21:47, 12.22s/it, training_loss=0.701]\u001b[A\n","Epoch 1:   9%|▊         | 10/116 [02:00\u003c21:07, 11.96s/it, training_loss=0.701]\u001b[A\n","Epoch 1:   9%|▊         | 10/116 [02:13\u003c21:07, 11.96s/it, training_loss=0.566]\u001b[A\n","Epoch 1:   9%|▉         | 11/116 [02:13\u003c21:26, 12.25s/it, training_loss=0.566]\u001b[A\n","Epoch 1:   9%|▉         | 11/116 [02:27\u003c21:26, 12.25s/it, training_loss=0.580]\u001b[A\n","Epoch 1:  10%|█         | 12/116 [02:27\u003c22:04, 12.74s/it, training_loss=0.580]\u001b[A\n","Epoch 1:  10%|█         | 12/116 [02:37\u003c22:04, 12.74s/it, training_loss=0.489]\u001b[A\n","Epoch 1:  11%|█         | 13/116 [02:37\u003c20:34, 11.98s/it, training_loss=0.489]\u001b[A\n","Epoch 1:  11%|█         | 13/116 [02:50\u003c20:34, 11.98s/it, training_loss=0.566]\u001b[A\n","Epoch 1:  12%|█▏        | 14/116 [02:50\u003c20:43, 12.20s/it, training_loss=0.566]\u001b[A\n","Epoch 1:  12%|█▏        | 14/116 [03:03\u003c20:43, 12.20s/it, training_loss=0.509]\u001b[A\n","Epoch 1:  13%|█▎        | 15/116 [03:03\u003c20:55, 12.43s/it, training_loss=0.509]\u001b[A\n","Epoch 1:  13%|█▎        | 15/116 [03:14\u003c20:55, 12.43s/it, training_loss=0.529]\u001b[A\n","Epoch 1:  14%|█▍        | 16/116 [03:14\u003c20:04, 12.05s/it, training_loss=0.529]\u001b[A\n","Epoch 1:  14%|█▍        | 16/116 [03:26\u003c20:04, 12.05s/it, training_loss=0.465]\u001b[A\n","Epoch 1:  15%|█▍        | 17/116 [03:26\u003c19:38, 11.91s/it, training_loss=0.465]\u001b[A\n","Epoch 1:  15%|█▍        | 17/116 [03:38\u003c19:38, 11.91s/it, training_loss=0.649]\u001b[A\n","Epoch 1:  16%|█▌        | 18/116 [03:38\u003c19:56, 12.20s/it, training_loss=0.649]\u001b[A\n","Epoch 1:  16%|█▌        | 18/116 [03:51\u003c19:56, 12.20s/it, training_loss=0.778]\u001b[A\n","Epoch 1:  16%|█▋        | 19/116 [03:51\u003c19:49, 12.26s/it, training_loss=0.778]\u001b[A\n","Epoch 1:  16%|█▋        | 19/116 [04:01\u003c19:49, 12.26s/it, training_loss=0.709]\u001b[A\n","Epoch 1:  17%|█▋        | 20/116 [04:01\u003c18:49, 11.76s/it, training_loss=0.709]\u001b[A\n","Epoch 1:  17%|█▋        | 20/116 [04:14\u003c18:49, 11.76s/it, training_loss=0.730]\u001b[A\n","Epoch 1:  18%|█▊        | 21/116 [04:14\u003c19:10, 12.11s/it, training_loss=0.730]\u001b[A\n","Epoch 1:  18%|█▊        | 21/116 [04:27\u003c19:10, 12.11s/it, training_loss=0.533]\u001b[A\n","Epoch 1:  19%|█▉        | 22/116 [04:27\u003c19:20, 12.35s/it, training_loss=0.533]\u001b[A\n","Epoch 1:  19%|█▉        | 22/116 [04:38\u003c19:20, 12.35s/it, training_loss=0.564]\u001b[A\n","Epoch 1:  20%|█▉        | 23/116 [04:38\u003c18:24, 11.88s/it, training_loss=0.564]\u001b[A\n","Epoch 1:  20%|█▉        | 23/116 [04:50\u003c18:24, 11.88s/it, training_loss=0.569]\u001b[A\n","Epoch 1:  21%|██        | 24/116 [04:50\u003c18:16, 11.92s/it, training_loss=0.569]\u001b[A\n","Epoch 1:  21%|██        | 24/116 [05:03\u003c18:16, 11.92s/it, training_loss=0.572]\u001b[A\n","Epoch 1:  22%|██▏       | 25/116 [05:03\u003c18:31, 12.22s/it, training_loss=0.572]\u001b[A\n","Epoch 1:  22%|██▏       | 25/116 [05:15\u003c18:31, 12.22s/it, training_loss=0.573]\u001b[A\n","Epoch 1:  22%|██▏       | 26/116 [05:15\u003c18:07, 12.08s/it, training_loss=0.573]\u001b[A\n","Epoch 1:  22%|██▏       | 26/116 [05:26\u003c18:07, 12.08s/it, training_loss=0.539]\u001b[A\n","Epoch 1:  23%|██▎       | 27/116 [05:26\u003c17:22, 11.72s/it, training_loss=0.539]\u001b[A\n","Epoch 1:  23%|██▎       | 27/116 [05:38\u003c17:22, 11.72s/it, training_loss=0.534]\u001b[A\n","Epoch 1:  24%|██▍       | 28/116 [05:38\u003c17:39, 12.04s/it, training_loss=0.534]\u001b[A\n","Epoch 1:  24%|██▍       | 28/116 [05:51\u003c17:39, 12.04s/it, training_loss=0.570]\u001b[A\n","Epoch 1:  25%|██▌       | 29/116 [05:51\u003c17:45, 12.25s/it, training_loss=0.570]\u001b[A\n","Epoch 1:  25%|██▌       | 29/116 [06:01\u003c17:45, 12.25s/it, training_loss=0.564]\u001b[A\n","Epoch 1:  26%|██▌       | 30/116 [06:01\u003c16:39, 11.62s/it, training_loss=0.564]\u001b[A\n","Epoch 1:  26%|██▌       | 30/116 [06:14\u003c16:39, 11.62s/it, training_loss=0.549]\u001b[A\n","Epoch 1:  27%|██▋       | 31/116 [06:14\u003c16:56, 11.96s/it, training_loss=0.549]\u001b[A\n","Epoch 1:  27%|██▋       | 31/116 [06:27\u003c16:56, 11.96s/it, training_loss=0.462]\u001b[A\n","Epoch 1:  28%|██▊       | 32/116 [06:27\u003c17:05, 12.21s/it, training_loss=0.462]\u001b[A\n","Epoch 1:  28%|██▊       | 32/116 [06:38\u003c17:05, 12.21s/it, training_loss=0.491]\u001b[A\n","Epoch 1:  28%|██▊       | 33/116 [06:38\u003c16:23, 11.85s/it, training_loss=0.491]\u001b[A\n","Epoch 1:  28%|██▊       | 33/116 [06:49\u003c16:23, 11.85s/it, training_loss=0.605]\u001b[A\n","Epoch 1:  29%|██▉       | 34/116 [06:49\u003c16:05, 11.77s/it, training_loss=0.605]\u001b[A\n","Epoch 1:  29%|██▉       | 34/116 [07:02\u003c16:05, 11.77s/it, training_loss=0.552]\u001b[A\n","Epoch 1:  30%|███       | 35/116 [07:02\u003c16:20, 12.10s/it, training_loss=0.552]\u001b[A\n","Epoch 1:  30%|███       | 35/116 [07:14\u003c16:20, 12.10s/it, training_loss=0.544]\u001b[A\n","Epoch 1:  31%|███       | 36/116 [07:14\u003c16:05, 12.07s/it, training_loss=0.544]\u001b[A\n","Epoch 1:  31%|███       | 36/116 [07:25\u003c16:05, 12.07s/it, training_loss=0.606]\u001b[A\n","Epoch 1:  32%|███▏      | 37/116 [07:25\u003c15:20, 11.65s/it, training_loss=0.606]\u001b[A\n","Epoch 1:  32%|███▏      | 37/116 [07:38\u003c15:20, 11.65s/it, training_loss=0.599]\u001b[A\n","Epoch 1:  33%|███▎      | 38/116 [07:38\u003c15:34, 11.99s/it, training_loss=0.599]\u001b[A\n","Epoch 1:  33%|███▎      | 38/116 [07:51\u003c15:34, 11.99s/it, training_loss=0.538]\u001b[A\n","Epoch 1:  34%|███▎      | 39/116 [07:51\u003c15:42, 12.24s/it, training_loss=0.538]\u001b[A\n","Epoch 1:  34%|███▎      | 39/116 [08:01\u003c15:42, 12.24s/it, training_loss=0.530]\u001b[A\n","Epoch 1:  34%|███▍      | 40/116 [08:01\u003c14:43, 11.62s/it, training_loss=0.530]\u001b[A\n","Epoch 1:  34%|███▍      | 40/116 [08:13\u003c14:43, 11.62s/it, training_loss=0.557]\u001b[A\n","Epoch 1:  35%|███▌      | 41/116 [08:13\u003c14:51, 11.88s/it, training_loss=0.557]\u001b[A\n","Epoch 1:  35%|███▌      | 41/116 [08:26\u003c14:51, 11.88s/it, training_loss=0.586]\u001b[A\n","Epoch 1:  36%|███▌      | 42/116 [08:26\u003c15:01, 12.18s/it, training_loss=0.586]\u001b[A\n","Epoch 1:  36%|███▌      | 42/116 [08:37\u003c15:01, 12.18s/it, training_loss=0.530]\u001b[A\n","Epoch 1:  37%|███▋      | 43/116 [08:37\u003c14:28, 11.90s/it, training_loss=0.530]\u001b[A\n","Epoch 1:  37%|███▋      | 43/116 [08:49\u003c14:28, 11.90s/it, training_loss=0.487]\u001b[A\n","Epoch 1:  38%|███▊      | 44/116 [08:49\u003c14:06, 11.76s/it, training_loss=0.487]\u001b[A\n","Epoch 1:  38%|███▊      | 44/116 [09:02\u003c14:06, 11.76s/it, training_loss=0.500]\u001b[A\n","Epoch 1:  39%|███▉      | 45/116 [09:02\u003c14:19, 12.11s/it, training_loss=0.500]\u001b[A\n","Epoch 1:  39%|███▉      | 45/116 [09:14\u003c14:19, 12.11s/it, training_loss=0.551]\u001b[A\n","Epoch 1:  40%|███▉      | 46/116 [09:14\u003c14:13, 12.19s/it, training_loss=0.551]\u001b[A\n","Epoch 1:  40%|███▉      | 46/116 [09:25\u003c14:13, 12.19s/it, training_loss=0.608]\u001b[A\n","Epoch 1:  41%|████      | 47/116 [09:25\u003c13:24, 11.66s/it, training_loss=0.608]\u001b[A\n","Epoch 1:  41%|████      | 47/116 [09:37\u003c13:24, 11.66s/it, training_loss=0.604]\u001b[A\n","Epoch 1:  41%|████▏     | 48/116 [09:37\u003c13:36, 12.00s/it, training_loss=0.604]\u001b[A\n","Epoch 1:  41%|████▏     | 48/116 [09:50\u003c13:36, 12.00s/it, training_loss=0.528]\u001b[A\n","Epoch 1:  42%|████▏     | 49/116 [09:50\u003c13:42, 12.27s/it, training_loss=0.528]\u001b[A\n","Epoch 1:  42%|████▏     | 49/116 [10:05\u003c13:42, 12.27s/it, training_loss=0.606]\u001b[A\n","Epoch 1:  43%|████▎     | 50/116 [10:05\u003c14:11, 12.91s/it, training_loss=0.606]\u001b[A\n","Epoch 1:  43%|████▎     | 50/116 [10:16\u003c14:11, 12.91s/it, training_loss=0.466]\u001b[A\n","Epoch 1:  44%|████▍     | 51/116 [10:16\u003c13:20, 12.31s/it, training_loss=0.466]\u001b[A\n","Epoch 1:  44%|████▍     | 51/116 [10:28\u003c13:20, 12.31s/it, training_loss=0.511]\u001b[A\n","Epoch 1:  45%|████▍     | 52/116 [10:28\u003c13:18, 12.47s/it, training_loss=0.511]\u001b[A\n","Epoch 1:  45%|████▍     | 52/116 [10:41\u003c13:18, 12.47s/it, training_loss=0.608]\u001b[A\n","Epoch 1:  46%|████▌     | 53/116 [10:41\u003c13:09, 12.53s/it, training_loss=0.608]\u001b[A\n","Epoch 1:  46%|████▌     | 53/116 [10:51\u003c13:09, 12.53s/it, training_loss=0.592]\u001b[A\n","Epoch 1:  47%|████▋     | 54/116 [10:51\u003c12:13, 11.83s/it, training_loss=0.592]\u001b[A\n","Epoch 1:  47%|████▋     | 54/116 [11:04\u003c12:13, 11.83s/it, training_loss=0.560]\u001b[A\n","Epoch 1:  47%|████▋     | 55/116 [11:04\u003c12:18, 12.10s/it, training_loss=0.560]\u001b[A\n","Epoch 1:  47%|████▋     | 55/116 [11:17\u003c12:18, 12.10s/it, training_loss=0.584]\u001b[A\n","Epoch 1:  48%|████▊     | 56/116 [11:17\u003c12:17, 12.29s/it, training_loss=0.584]\u001b[A\n","Epoch 1:  48%|████▊     | 56/116 [11:28\u003c12:17, 12.29s/it, training_loss=0.591]\u001b[A\n","Epoch 1:  49%|████▉     | 57/116 [11:28\u003c11:40, 11.87s/it, training_loss=0.591]\u001b[A\n","Epoch 1:  49%|████▉     | 57/116 [11:39\u003c11:40, 11.87s/it, training_loss=0.570]\u001b[A\n","Epoch 1:  50%|█████     | 58/116 [11:39\u003c11:23, 11.79s/it, training_loss=0.570]\u001b[A\n","Epoch 1:  50%|█████     | 58/116 [11:52\u003c11:23, 11.79s/it, training_loss=0.499]\u001b[A\n","Epoch 1:  51%|█████     | 59/116 [11:52\u003c11:29, 12.10s/it, training_loss=0.499]\u001b[A\n","Epoch 1:  51%|█████     | 59/116 [12:04\u003c11:29, 12.10s/it, training_loss=0.558]\u001b[A\n","Epoch 1:  52%|█████▏    | 60/116 [12:04\u003c11:15, 12.05s/it, training_loss=0.558]\u001b[A\n","Epoch 1:  52%|█████▏    | 60/116 [12:15\u003c11:15, 12.05s/it, training_loss=0.551]\u001b[A\n","Epoch 1:  53%|█████▎    | 61/116 [12:15\u003c10:42, 11.68s/it, training_loss=0.551]\u001b[A\n","Epoch 1:  53%|█████▎    | 61/116 [12:28\u003c10:42, 11.68s/it, training_loss=0.598]\u001b[A\n","Epoch 1:  53%|█████▎    | 62/116 [12:28\u003c10:48, 12.02s/it, training_loss=0.598]\u001b[A\n","Epoch 1:  53%|█████▎    | 62/116 [12:40\u003c10:48, 12.02s/it, training_loss=0.591]\u001b[A\n","Epoch 1:  54%|█████▍    | 63/116 [12:40\u003c10:47, 12.22s/it, training_loss=0.591]\u001b[A\n","Epoch 1:  54%|█████▍    | 63/116 [12:50\u003c10:47, 12.22s/it, training_loss=0.528]\u001b[A\n","Epoch 1:  55%|█████▌    | 64/116 [12:50\u003c10:02, 11.59s/it, training_loss=0.528]\u001b[A\n","Epoch 1:  55%|█████▌    | 64/116 [13:03\u003c10:02, 11.59s/it, training_loss=0.526]\u001b[A\n","Epoch 1:  56%|█████▌    | 65/116 [13:03\u003c10:07, 11.92s/it, training_loss=0.526]\u001b[A\n","Epoch 1:  56%|█████▌    | 65/116 [13:16\u003c10:07, 11.92s/it, training_loss=0.535]\u001b[A\n","Epoch 1:  57%|█████▋    | 66/116 [13:16\u003c10:09, 12.18s/it, training_loss=0.535]\u001b[A\n","Epoch 1:  57%|█████▋    | 66/116 [13:27\u003c10:09, 12.18s/it, training_loss=0.552]\u001b[A\n","Epoch 1:  58%|█████▊    | 67/116 [13:27\u003c09:39, 11.82s/it, training_loss=0.552]\u001b[A\n","Epoch 1:  58%|█████▊    | 67/116 [13:38\u003c09:39, 11.82s/it, training_loss=0.543]\u001b[A\n","Epoch 1:  59%|█████▊    | 68/116 [13:38\u003c09:23, 11.73s/it, training_loss=0.543]\u001b[A\n","Epoch 1:  59%|█████▊    | 68/116 [13:51\u003c09:23, 11.73s/it, training_loss=0.541]\u001b[A\n","Epoch 1:  59%|█████▉    | 69/116 [13:51\u003c09:26, 12.05s/it, training_loss=0.541]\u001b[A\n","Epoch 1:  59%|█████▉    | 69/116 [14:03\u003c09:26, 12.05s/it, training_loss=0.544]\u001b[A\n","Epoch 1:  60%|██████    | 70/116 [14:03\u003c09:15, 12.07s/it, training_loss=0.544]\u001b[A\n","Epoch 1:  60%|██████    | 70/116 [14:14\u003c09:15, 12.07s/it, training_loss=0.558]\u001b[A\n","Epoch 1:  61%|██████    | 71/116 [14:14\u003c08:43, 11.64s/it, training_loss=0.558]\u001b[A\n","Epoch 1:  61%|██████    | 71/116 [14:27\u003c08:43, 11.64s/it, training_loss=0.543]\u001b[A\n","Epoch 1:  62%|██████▏   | 72/116 [14:27\u003c08:47, 11.99s/it, training_loss=0.543]\u001b[A\n","Epoch 1:  62%|██████▏   | 72/116 [14:40\u003c08:47, 11.99s/it, training_loss=0.552]\u001b[A\n","Epoch 1:  63%|██████▎   | 73/116 [14:40\u003c08:45, 12.23s/it, training_loss=0.552]\u001b[A\n","Epoch 1:  63%|██████▎   | 73/116 [14:50\u003c08:45, 12.23s/it, training_loss=0.543]\u001b[A\n","Epoch 1:  64%|██████▍   | 74/116 [14:50\u003c08:08, 11.62s/it, training_loss=0.543]\u001b[A\n","Epoch 1:  64%|██████▍   | 74/116 [15:02\u003c08:08, 11.62s/it, training_loss=0.572]\u001b[A\n","Epoch 1:  65%|██████▍   | 75/116 [15:02\u003c08:06, 11.86s/it, training_loss=0.572]\u001b[A\n","Epoch 1:  65%|██████▍   | 75/116 [15:15\u003c08:06, 11.86s/it, training_loss=0.510]\u001b[A\n","Epoch 1:  66%|██████▌   | 76/116 [15:15\u003c08:05, 12.14s/it, training_loss=0.510]\u001b[A\n","Epoch 1:  66%|██████▌   | 76/116 [15:26\u003c08:05, 12.14s/it, training_loss=0.591]\u001b[A\n","Epoch 1:  66%|██████▋   | 77/116 [15:26\u003c07:42, 11.86s/it, training_loss=0.591]\u001b[A\n","Epoch 1:  66%|██████▋   | 77/116 [15:37\u003c07:42, 11.86s/it, training_loss=0.550]\u001b[A\n","Epoch 1:  67%|██████▋   | 78/116 [15:37\u003c07:24, 11.69s/it, training_loss=0.550]\u001b[A\n","Epoch 1:  67%|██████▋   | 78/116 [15:50\u003c07:24, 11.69s/it, training_loss=0.498]\u001b[A\n","Epoch 1:  68%|██████▊   | 79/116 [15:50\u003c07:24, 12.02s/it, training_loss=0.498]\u001b[A\n","Epoch 1:  68%|██████▊   | 79/116 [16:03\u003c07:24, 12.02s/it, training_loss=0.536]\u001b[A\n","Epoch 1:  69%|██████▉   | 80/116 [16:03\u003c07:15, 12.10s/it, training_loss=0.536]\u001b[A\n","Epoch 1:  69%|██████▉   | 80/116 [16:13\u003c07:15, 12.10s/it, training_loss=0.596]\u001b[A\n","Epoch 1:  70%|██████▉   | 81/116 [16:13\u003c06:46, 11.61s/it, training_loss=0.596]\u001b[A\n","Epoch 1:  70%|██████▉   | 81/116 [16:26\u003c06:46, 11.61s/it, training_loss=0.589]\u001b[A\n","Epoch 1:  71%|███████   | 82/116 [16:26\u003c06:46, 11.96s/it, training_loss=0.589]\u001b[A\n","Epoch 1:  71%|███████   | 82/116 [16:39\u003c06:46, 11.96s/it, training_loss=0.557]\u001b[A\n","Epoch 1:  72%|███████▏  | 83/116 [16:39\u003c06:42, 12.21s/it, training_loss=0.557]\u001b[A\n","Epoch 1:  72%|███████▏  | 83/116 [16:49\u003c06:42, 12.21s/it, training_loss=0.654]\u001b[A\n","Epoch 1:  72%|███████▏  | 84/116 [16:49\u003c06:11, 11.60s/it, training_loss=0.654]\u001b[A\n","Epoch 1:  72%|███████▏  | 84/116 [17:01\u003c06:11, 11.60s/it, training_loss=0.556]\u001b[A\n","Epoch 1:  73%|███████▎  | 85/116 [17:01\u003c06:06, 11.81s/it, training_loss=0.556]\u001b[A\n","Epoch 1:  73%|███████▎  | 85/116 [17:14\u003c06:06, 11.81s/it, training_loss=0.543]\u001b[A\n","Epoch 1:  74%|███████▍  | 86/116 [17:14\u003c06:02, 12.09s/it, training_loss=0.543]\u001b[A\n","Epoch 1:  74%|███████▍  | 86/116 [17:25\u003c06:02, 12.09s/it, training_loss=0.602]\u001b[A\n","Epoch 1:  75%|███████▌  | 87/116 [17:25\u003c05:43, 11.83s/it, training_loss=0.602]\u001b[A\n","Epoch 1:  75%|███████▌  | 87/116 [17:36\u003c05:43, 11.83s/it, training_loss=0.510]\u001b[A\n","Epoch 1:  76%|███████▌  | 88/116 [17:36\u003c05:26, 11.67s/it, training_loss=0.510]\u001b[A\n","Epoch 1:  76%|███████▌  | 88/116 [17:49\u003c05:26, 11.67s/it, training_loss=0.556]\u001b[A\n","Epoch 1:  77%|███████▋  | 89/116 [17:49\u003c05:23, 12.00s/it, training_loss=0.556]\u001b[A\n","Epoch 1:  77%|███████▋  | 89/116 [18:02\u003c05:23, 12.00s/it, training_loss=0.554]\u001b[A\n","Epoch 1:  78%|███████▊  | 90/116 [18:02\u003c05:15, 12.13s/it, training_loss=0.554]\u001b[A\n","Epoch 1:  78%|███████▊  | 90/116 [18:12\u003c05:15, 12.13s/it, training_loss=0.468]\u001b[A\n","Epoch 1:  78%|███████▊  | 91/116 [18:12\u003c04:50, 11.61s/it, training_loss=0.468]\u001b[A\n","Epoch 1:  78%|███████▊  | 91/116 [18:25\u003c04:50, 11.61s/it, training_loss=0.500]\u001b[A\n","Epoch 1:  79%|███████▉  | 92/116 [18:25\u003c04:47, 11.99s/it, training_loss=0.500]\u001b[A\n","Epoch 1:  79%|███████▉  | 92/116 [18:38\u003c04:47, 11.99s/it, training_loss=0.571]\u001b[A\n","Epoch 1:  80%|████████  | 93/116 [18:38\u003c04:41, 12.25s/it, training_loss=0.571]\u001b[A\n","Epoch 1:  80%|████████  | 93/116 [18:48\u003c04:41, 12.25s/it, training_loss=0.566]\u001b[A\n","Epoch 1:  81%|████████  | 94/116 [18:48\u003c04:19, 11.78s/it, training_loss=0.566]\u001b[A\n","Epoch 1:  81%|████████  | 94/116 [19:00\u003c04:19, 11.78s/it, training_loss=0.559]\u001b[A\n","Epoch 1:  82%|████████▏ | 95/116 [19:00\u003c04:09, 11.88s/it, training_loss=0.559]\u001b[A\n","Epoch 1:  82%|████████▏ | 95/116 [19:13\u003c04:09, 11.88s/it, training_loss=0.515]\u001b[A\n","Epoch 1:  83%|████████▎ | 96/116 [19:13\u003c04:03, 12.17s/it, training_loss=0.515]\u001b[A\n","Epoch 1:  83%|████████▎ | 96/116 [19:25\u003c04:03, 12.17s/it, training_loss=0.453]\u001b[A\n","Epoch 1:  84%|████████▎ | 97/116 [19:25\u003c03:49, 12.09s/it, training_loss=0.453]\u001b[A\n","Epoch 1:  84%|████████▎ | 97/116 [19:36\u003c03:49, 12.09s/it, training_loss=0.598]\u001b[A\n","Epoch 1:  84%|████████▍ | 98/116 [19:36\u003c03:31, 11.74s/it, training_loss=0.598]\u001b[A\n","Epoch 1:  84%|████████▍ | 98/116 [19:49\u003c03:31, 11.74s/it, training_loss=0.593]\u001b[A\n","Epoch 1:  85%|████████▌ | 99/116 [19:49\u003c03:25, 12.09s/it, training_loss=0.593]\u001b[A\n","Epoch 1:  85%|████████▌ | 99/116 [20:02\u003c03:25, 12.09s/it, training_loss=0.493]\u001b[A\n","Epoch 1:  86%|████████▌ | 100/116 [20:02\u003c03:17, 12.32s/it, training_loss=0.493]\u001b[A\n","Epoch 1:  86%|████████▌ | 100/116 [20:12\u003c03:17, 12.32s/it, training_loss=0.605]\u001b[A\n","Epoch 1:  87%|████████▋ | 101/116 [20:12\u003c02:55, 11.69s/it, training_loss=0.605]\u001b[A\n","Epoch 1:  87%|████████▋ | 101/116 [20:25\u003c02:55, 11.69s/it, training_loss=0.565]\u001b[A\n","Epoch 1:  88%|████████▊ | 102/116 [20:25\u003c02:47, 11.98s/it, training_loss=0.565]\u001b[A\n","Epoch 1:  88%|████████▊ | 102/116 [20:38\u003c02:47, 11.98s/it, training_loss=0.553]\u001b[A\n","Epoch 1:  89%|████████▉ | 103/116 [20:38\u003c02:39, 12.26s/it, training_loss=0.553]\u001b[A\n","Epoch 1:  89%|████████▉ | 103/116 [20:49\u003c02:39, 12.26s/it, training_loss=0.678]\u001b[A\n","Epoch 1:  90%|████████▉ | 104/116 [20:49\u003c02:23, 11.96s/it, training_loss=0.678]\u001b[A\n","Epoch 1:  90%|████████▉ | 104/116 [21:00\u003c02:23, 11.96s/it, training_loss=0.515]\u001b[A\n","Epoch 1:  91%|█████████ | 105/116 [21:00\u003c02:09, 11.80s/it, training_loss=0.515]\u001b[A\n","Epoch 1:  91%|█████████ | 105/116 [21:13\u003c02:09, 11.80s/it, training_loss=0.513]\u001b[A\n","Epoch 1:  91%|█████████▏| 106/116 [21:13\u003c02:01, 12.13s/it, training_loss=0.513]\u001b[A\n","Epoch 1:  91%|█████████▏| 106/116 [21:26\u003c02:01, 12.13s/it, training_loss=0.523]\u001b[A\n","Epoch 1:  92%|█████████▏| 107/116 [21:26\u003c01:50, 12.22s/it, training_loss=0.523]\u001b[A\n","Epoch 1:  92%|█████████▏| 107/116 [21:36\u003c01:50, 12.22s/it, training_loss=0.527]\u001b[A\n","Epoch 1:  93%|█████████▎| 108/116 [21:36\u003c01:33, 11.71s/it, training_loss=0.527]\u001b[A\n","Epoch 1:  93%|█████████▎| 108/116 [21:49\u003c01:33, 11.71s/it, training_loss=0.539]\u001b[A\n","Epoch 1:  94%|█████████▍| 109/116 [21:49\u003c01:24, 12.04s/it, training_loss=0.539]\u001b[A\n","Epoch 1:  94%|█████████▍| 109/116 [22:02\u003c01:24, 12.04s/it, training_loss=0.564]\u001b[A\n","Epoch 1:  95%|█████████▍| 110/116 [22:02\u003c01:13, 12.31s/it, training_loss=0.564]\u001b[A\n","Epoch 1:  95%|█████████▍| 110/116 [22:13\u003c01:13, 12.31s/it, training_loss=0.481]\u001b[A\n","Epoch 1:  96%|█████████▌| 111/116 [22:13\u003c00:58, 11.78s/it, training_loss=0.481]\u001b[A\n","Epoch 1:  96%|█████████▌| 111/116 [22:25\u003c00:58, 11.78s/it, training_loss=0.511]\u001b[A\n","Epoch 1:  97%|█████████▋| 112/116 [22:25\u003c00:47, 11.85s/it, training_loss=0.511]\u001b[A\n","Epoch 1:  97%|█████████▋| 112/116 [22:37\u003c00:47, 11.85s/it, training_loss=0.571]\u001b[A\n","Epoch 1:  97%|█████████▋| 113/116 [22:37\u003c00:36, 12.13s/it, training_loss=0.571]\u001b[A\n","Epoch 1:  97%|█████████▋| 113/116 [22:49\u003c00:36, 12.13s/it, training_loss=0.583]\u001b[A\n","Epoch 1:  98%|█████████▊| 114/116 [22:49\u003c00:23, 11.95s/it, training_loss=0.583]\u001b[A\n","Epoch 1:  98%|█████████▊| 114/116 [23:01\u003c00:23, 11.95s/it, training_loss=0.594]\u001b[A\n","Epoch 1:  99%|█████████▉| 115/116 [23:02\u003c00:12, 12.16s/it, training_loss=0.594]\u001b[A\n","Epoch 1:  99%|█████████▉| 115/116 [23:10\u003c00:12, 12.16s/it, training_loss=0.472]\u001b[A\n","Epoch 1: 100%|██████████| 116/116 [23:10\u003c00:00, 11.00s/it, training_loss=0.472]\u001b[A\n","  0%|          | 0/4 [23:10\u003c?, ?it/s]\n"]},{"ename":"RuntimeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-31-b277799f1436\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0masag_systems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Paper_TA_ASAG/DATASET_TA/Data/Data Lagi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m\u003cipython-input-30-e95f395a45eb\u003e\u001b[0m in \u001b[0;36masag_systems\u001b[0;34m(path_dir)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.xslx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 47\u001b[0;31m             \u001b[0mtrain_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m\u003cipython-input-27-a3dda0eac7c2\u003e\u001b[0m in \u001b[0;36mtrain_eval\u001b[0;34m(df_final, pretrainedmodel)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 176\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'/MyDrive/Paper_TA_ASAG/DATASET_TA/Data/Model_Save/finetuned_BERT_{df_final[\"path\"]}_epoch_{epoch}.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nEpoch {epoch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 422\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 309\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_writer_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 287\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_zipfile_writer_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Parent directory /MyDrive/Paper_TA_ASAG/DATASET_TA/Data/Model_Save does not exist."]}],"source":["asag_systems('/content/drive/MyDrive/Paper_TA_ASAG/DATASET_TA/Data/Data Lagi')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BxDuxX1LSI1V"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"766caac7eb69e8a0a4a596af183e8606e532f32ec205da93d6afbc58c03966c0"}}},"nbformat":4,"nbformat_minor":5}