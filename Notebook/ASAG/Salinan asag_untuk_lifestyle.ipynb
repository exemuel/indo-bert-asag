{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3120,"status":"ok","timestamp":1680597102743,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"50S27W_Vj_Bi","outputId":"ce87d32b-1a28-4505-92cf-787ff92a478b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8863,"status":"ok","timestamp":1680597113102,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"frHag4z7kGdZ","outputId":"f92edb28-c677-477f-e108-2eacfbfad4a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sastrawi in /usr/local/lib/python3.9/dist-packages (1.0.1)\n"]}],"source":["pip install sastrawi"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7875,"status":"ok","timestamp":1680597120945,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"kk8ilth6kH1d","outputId":"2b0ed199-0d55-4c11-faed-3aa6ae7d8a0c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.4.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from nltk) (1.16.0)\n"]}],"source":["pip install nltk"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1434,"status":"ok","timestamp":1680597122364,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"Ph17gJD0kKb4","outputId":"61bdaecf-a45f-4c43-da95-1c8d929385a6"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1680597122367,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"gs4oMdsrkL3E","outputId":"ff1aead3-5d6b-40a2-ae61-d45105b8ed98"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6797,"status":"ok","timestamp":1680597132602,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"OGmxzhivkNRJ","outputId":"a5e2a9c9-1bce-4d40-977e-1212d9a1f3f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nlp_id in /usr/local/lib/python3.9/dist-packages (0.1.13.0)\n","Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.9/dist-packages (from nlp_id) (3.4.5)\n","Requirement already satisfied: wget==3.2 in /usr/local/lib/python3.9/dist-packages (from nlp_id) (3.2)\n","Requirement already satisfied: scikit-learn==1.1.0 in /usr/local/lib/python3.9/dist-packages (from nlp_id) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from nltk==3.4.5-\u003enlp_id) (1.16.0)\n","Requirement already satisfied: scipy\u003e=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.1.0-\u003enlp_id) (1.10.1)\n","Requirement already satisfied: joblib\u003e=1.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.1.0-\u003enlp_id) (1.1.1)\n","Requirement already satisfied: numpy\u003e=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.1.0-\u003enlp_id) (1.22.4)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.1.0-\u003enlp_id) (3.1.0)\n"]}],"source":["pip install nlp_id"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27382,"status":"ok","timestamp":1680597165016,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"K9ZXuJrokX-x","outputId":"8fa38741-3628-432f-b095-d4676abd74fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Collecting huggingface-hub\u003c1.0,\u003e=0.11.0\n","  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n","Collecting tokenizers!=0.11.3,\u003c0.14,\u003e=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.11.0-\u003etransformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers) (2.0.12)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers) (3.4)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers) (1.26.15)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers) (2022.12.7)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.4\n"]}],"source":["pip install transformers"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":15043,"status":"ok","timestamp":1680597180024,"user":{"displayName":"Raja Muda Gading","userId":"11199760221932474938"},"user_tz":-420},"id":"LEM8axq7kjkZ"},"outputs":[],"source":["import re\n","import random\n","import pandas as pd\n","import torch\n","import tensorflow as tf\n","import numpy as np\n","\n","from nlp_id.lemmatizer import Lemmatizer\n","from nltk.corpus import stopwords\n","from tqdm import tqdm\n","from sklearn.preprocessing import KBinsDiscretizer\n","from sklearn.metrics import f1_score, cohen_kappa_score\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.initializers import TruncatedNormal\n","from tensorflow.keras.losses import CategoricalCrossentropy\n","from tensorflow.keras.metrics import CategoricalAccuracy\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Input, Dense\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from nltk.corpus import stopwords\n","\n","\n","seed_val = 17\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","def f1_score_func(preds, labels):\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return f1_score(labels_flat, preds_flat, average='weighted')\n","\n","def qwk_score_func(preds, labels):\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return cohen_kappa_score(labels_flat, preds_flat)\n","\n","def accuracy_per_class(preds, labels):\n","    label_dict_inverse = {v: k for k, v in label_dict.items()}\n","\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    for label in np.unique(labels_flat):\n","        y_preds = preds_flat[labels_flat==label]\n","        y_true = labels_flat[labels_flat==label]\n","        print(f'Class: {label_dict_inverse[label]}')\n","        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n","\n","def evaluate(dataloader_val, device, model):\n","\n","    model.eval()\n","\n","    loss_val_total = 0\n","    predictions, true_vals = [], []\n","\n","    for batch in dataloader_val:\n","\n","        batch = tuple(b.to(device) for b in batch)\n","\n","        inputs = {'input_ids':      batch[0],\n","                  'attention_mask': batch[1],\n","                  'labels':         batch[2],\n","                 }\n","\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","\n","        loss = outputs[0]\n","        logits = outputs[1]\n","        loss_val_total += loss.item()\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = inputs['labels'].cpu().numpy()\n","        predictions.append(logits)\n","        true_vals.append(label_ids)\n","\n","    loss_val_avg = loss_val_total/len(dataloader_val)\n","\n","    predictions = np.concatenate(predictions, axis=0)\n","    true_vals = np.concatenate(true_vals, axis=0)\n","\n","    return loss_val_avg, predictions, true_vals\n","\n","def train_eval(df_final, pretrainedmodel):\n","    # bin nilai (continuous variable) into intervals\n","    df_final['nilai'] = pd.qcut(df_final['nilai'], 5, labels=False, duplicates='drop')\n","\n","    # concatenate soal and jawaban\n","    df_final['soal-jawaban'] = df_final['soal']+df_final['jawaban']\n","\n","    # preprocessing\n","    # lowercasing\n","    df_final['soal-jawaban'] = df_final['soal-jawaban'].apply(lambda x: x.lower())\n","    # lemmatization\n","    lemmatizer = Lemmatizer()\n","    df_final['soal-jawaban'] = df_final['soal-jawaban'].apply(lambda x: lemmatizer.lemmatize(x))\n","    # stopword removal\n","    list_stopwords = set(stopwords.words('indonesian'))\n","    df_final['soal-jawaban'] = df_final['soal-jawaban'].apply(lambda x: ' '.join([item for item in x.split() if item not in list_stopwords]))\n","    # punctuation removal\n","    df_final['soal-jawaban'] = df_final['soal-jawaban'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n","\n","    # make sure that the training set and test set ratio is 80:20\n","    add = len(df_final[df_final['tipe'] == 'test']) - (round(0.2*(len(df_final[df_final['tipe'] == 'train'])+len(df_final[df_final['tipe'] == 'test']))))\n","    for i in df_final[df_final['tipe'] == 'test'].sample(n = add).itertuples():\n","        df_final.at[i.Index, 'tipe'] = 'train'\n","\n","    # load model and tokenizer\n","    tokenizer = BertTokenizer.from_pretrained(pretrainedmodel, ignore_mismatched_sizes=True)\n","\n","    encoded_data_train = tokenizer.batch_encode_plus(\n","        df_final[df_final.tipe=='train']['soal-jawaban'].values,\n","        add_special_tokens=True,\n","        return_attention_mask=True,\n","        pad_to_max_length=True,\n","        truncation=True,\n","        max_length=256,\n","        padding='max_length',\n","        return_tensors='pt'\n","    )\n","\n","    encoded_data_val = tokenizer.batch_encode_plus(\n","        df_final[df_final.tipe=='test']['soal-jawaban'].values,\n","        add_special_tokens=True,\n","        return_attention_mask=True,\n","        pad_to_max_length=True,\n","        truncation=True,\n","        max_length=256,\n","        padding='max_length',\n","        return_tensors='pt'\n","    )\n","\n","    input_ids_train = encoded_data_train['input_ids']\n","    attention_masks_train = encoded_data_train['attention_mask']\n","    labels_train = torch.tensor(df_final[df_final.tipe=='train'].nilai.values)\n","\n","    input_ids_val = encoded_data_val['input_ids']\n","    attention_masks_val = encoded_data_val['attention_mask']\n","    labels_val = torch.tensor(df_final[df_final.tipe=='test'].nilai.values)\n","\n","    dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n","    dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n","\n","    model = BertForSequenceClassification.from_pretrained(pretrainedmodel,\n","                                                          num_labels=5,\n","                                                          output_attentions=False,\n","                                                          output_hidden_states=False, ignore_mismatched_sizes=True)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","\n","    batch_size = 4\n","\n","    dataloader_train = DataLoader(dataset_train,\n","                                  sampler=RandomSampler(dataset_train),\n","                                  batch_size=batch_size)\n","\n","    dataloader_validation = DataLoader(dataset_val,\n","                                       sampler=SequentialSampler(dataset_val),\n","                                       batch_size=batch_size)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(),\n","                      lr=2e-5,\n","                      eps=1e-8)\n","\n","    epochs = 4\n","\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps=0,\n","                                                num_training_steps=len(dataloader_train)*epochs)\n","\n","    for epoch in tqdm(range(1, epochs+1)):\n","\n","        model.train()\n","\n","        loss_train_total = 0\n","\n","        progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n","        for batch in progress_bar:\n","\n","            model.zero_grad()\n","\n","            batch = tuple(b.to(device) for b in batch)\n","\n","            inputs = {'input_ids':      batch[0],\n","                      'attention_mask': batch[1],\n","                      'labels':         batch[2],\n","                     }\n","\n","            outputs = model(**inputs)\n","\n","            loss = outputs[0]\n","            loss_train_total += loss.item()\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            optimizer.step()\n","            scheduler.step()\n","\n","            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n","\n","        tqdm.write(f'\\nEpoch {epoch}')\n","\n","        loss_train_avg = loss_train_total/len(dataloader_train)\n","        tqdm.write(f'Training loss: {loss_train_avg}')\n","\n","        val_loss, predictions, true_vals = evaluate(dataloader_validation, device, model)\n","        val_f1 = f1_score_func(predictions, true_vals)\n","        val_qwk = qwk_score_func(predictions, true_vals)\n","        tqdm.write(f'Validation loss: {val_loss}')\n","        tqdm.write(f'F1 Score (Weighted): {val_f1}')\n","        tqdm.write(f'QWK Score: {val_qwk}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"AwIDxQ2UklHF"},"outputs":[{"name":"stdout","output_type":"stream","text":["indobenchmark/indobert-lite-base-p2\n","Analisis Essay Grading Lifestyle\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"24131f2d46de48638c810020f46c0d34","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/225k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca8d501d976e4188a3e22006c002daf7","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a96b2120576487d8d715bd31c7dff4d","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/2.00 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5613da5340d54a0c80702db42d1fcea5","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/1.54k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'AlbertTokenizerFast'. \n","The class this function is called from is 'BertTokenizer'.\n","You are using a model of type albert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fce438cfc2c14762855869dd78dce141","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/46.7M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at indobenchmark/indobert-lite-base-p2 were not used when initializing BertForSequenceClassification: ['encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias', 'encoder.embedding_hidden_mapping_in.weight', 'encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias', 'encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight', 'pooler.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias', 'encoder.albert_layer_groups.0.albert_layers.0.ffn.bias', 'encoder.embedding_hidden_mapping_in.bias', 'encoder.albert_layer_groups.0.albert_layers.0.ffn.weight', 'encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight', 'encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight', 'encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight', 'encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight', 'encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias', 'encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight', 'pooler.weight', 'encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias', 'encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight', 'encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p2 and are newly initialized: ['encoder.layer.7.intermediate.dense.weight', 'encoder.layer.10.output.dense.bias', 'classifier.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'classifier.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'pooler.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p2 and are newly initialized because the shapes did not match:\n","- embeddings.word_embeddings.weight: found shape torch.Size([30000, 128]) in the checkpoint and torch.Size([30000, 768]) in the model instantiated\n","- embeddings.position_embeddings.weight: found shape torch.Size([512, 128]) in the checkpoint and torch.Size([512, 768]) in the model instantiated\n","- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 128]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n","- embeddings.LayerNorm.weight: found shape torch.Size([128]) in the checkpoint and torch.Size([768]) in the model instantiated\n","- embeddings.LayerNorm.bias: found shape torch.Size([128]) in the checkpoint and torch.Size([768]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  0%|          | 0/4 [00:00\u003c?, ?it/s]\n","Epoch 1:   0%|          | 0/116 [00:00\u003c?, ?it/s]\u001b[A\n","Epoch 1:   0%|          | 0/116 [00:30\u003c?, ?it/s, training_loss=0.549]\u001b[A\n","Epoch 1:   1%|          | 1/116 [00:30\u003c58:43, 30.64s/it, training_loss=0.549]\u001b[A\n","Epoch 1:   1%|          | 1/116 [00:50\u003c58:43, 30.64s/it, training_loss=0.856]\u001b[A\n","Epoch 1:   2%|▏         | 2/116 [00:50\u003c46:14, 24.33s/it, training_loss=0.856]\u001b[A\n","Epoch 1:   2%|▏         | 2/116 [01:02\u003c46:14, 24.33s/it, training_loss=0.682]\u001b[A\n","Epoch 1:   3%|▎         | 3/116 [01:02\u003c35:24, 18.80s/it, training_loss=0.682]\u001b[A\n","Epoch 1:   3%|▎         | 3/116 [01:13\u003c35:24, 18.80s/it, training_loss=0.450]\u001b[A\n","Epoch 1:   3%|▎         | 4/116 [01:13\u003c29:16, 15.68s/it, training_loss=0.450]\u001b[A\n","Epoch 1:   3%|▎         | 4/116 [01:26\u003c29:16, 15.68s/it, training_loss=0.623]\u001b[A\n","Epoch 1:   4%|▍         | 5/116 [01:26\u003c27:12, 14.71s/it, training_loss=0.623]\u001b[A\n","Epoch 1:   4%|▍         | 5/116 [01:39\u003c27:12, 14.71s/it, training_loss=0.723]\u001b[A\n","Epoch 1:   5%|▌         | 6/116 [01:39\u003c25:38, 13.98s/it, training_loss=0.723]\u001b[A\n","Epoch 1:   5%|▌         | 6/116 [01:49\u003c25:38, 13.98s/it, training_loss=0.508]\u001b[A\n","Epoch 1:   6%|▌         | 7/116 [01:49\u003c23:10, 12.76s/it, training_loss=0.508]\u001b[A\n","Epoch 1:   6%|▌         | 7/116 [02:01\u003c23:10, 12.76s/it, training_loss=0.858]\u001b[A\n","Epoch 1:   7%|▋         | 8/116 [02:01\u003c22:30, 12.50s/it, training_loss=0.858]\u001b[A\n","Epoch 1:   7%|▋         | 8/116 [02:13\u003c22:30, 12.50s/it, training_loss=0.830]\u001b[A\n","Epoch 1:   8%|▊         | 9/116 [02:13\u003c22:17, 12.50s/it, training_loss=0.830]\u001b[A\n","Epoch 1:   8%|▊         | 9/116 [02:25\u003c22:17, 12.50s/it, training_loss=0.729]\u001b[A\n","Epoch 1:   9%|▊         | 10/116 [02:25\u003c21:43, 12.30s/it, training_loss=0.729]\u001b[A\n","Epoch 1:   9%|▊         | 10/116 [02:36\u003c21:43, 12.30s/it, training_loss=0.688]\u001b[A\n","Epoch 1:   9%|▉         | 11/116 [02:36\u003c20:29, 11.71s/it, training_loss=0.688]\u001b[A\n","Epoch 1:   9%|▉         | 11/116 [02:48\u003c20:29, 11.71s/it, training_loss=0.624]\u001b[A\n","Epoch 1:  10%|█         | 12/116 [02:48\u003c20:36, 11.89s/it, training_loss=0.624]\u001b[A\n","Epoch 1:  10%|█         | 12/116 [03:00\u003c20:36, 11.89s/it, training_loss=0.567]\u001b[A\n","Epoch 1:  11%|█         | 13/116 [03:00\u003c20:42, 12.07s/it, training_loss=0.567]\u001b[A\n","Epoch 1:  11%|█         | 13/116 [03:11\u003c20:42, 12.07s/it, training_loss=0.595]\u001b[A\n","Epoch 1:  12%|█▏        | 14/116 [03:11\u003c19:49, 11.66s/it, training_loss=0.595]\u001b[A\n","Epoch 1:  12%|█▏        | 14/116 [03:23\u003c19:49, 11.66s/it, training_loss=0.687]\u001b[A\n","Epoch 1:  13%|█▎        | 15/116 [03:23\u003c19:29, 11.58s/it, training_loss=0.687]\u001b[A\n","Epoch 1:  13%|█▎        | 15/116 [03:35\u003c19:29, 11.58s/it, training_loss=0.545]\u001b[A\n","Epoch 1:  14%|█▍        | 16/116 [03:35\u003c19:43, 11.84s/it, training_loss=0.545]\u001b[A\n","Epoch 1:  14%|█▍        | 16/116 [03:47\u003c19:43, 11.84s/it, training_loss=0.567]\u001b[A\n","Epoch 1:  15%|█▍        | 17/116 [03:47\u003c19:46, 11.99s/it, training_loss=0.567]\u001b[A\n","Epoch 1:  15%|█▍        | 17/116 [03:57\u003c19:46, 11.99s/it, training_loss=0.566]\u001b[A\n","Epoch 1:  16%|█▌        | 18/116 [03:57\u003c18:39, 11.42s/it, training_loss=0.566]\u001b[A\n","Epoch 1:  16%|█▌        | 18/116 [04:10\u003c18:39, 11.42s/it, training_loss=0.453]\u001b[A\n","Epoch 1:  16%|█▋        | 19/116 [04:10\u003c18:52, 11.67s/it, training_loss=0.453]\u001b[A\n","Epoch 1:  16%|█▋        | 19/116 [04:22\u003c18:52, 11.67s/it, training_loss=0.656]\u001b[A\n","Epoch 1:  17%|█▋        | 20/116 [04:22\u003c19:00, 11.88s/it, training_loss=0.656]\u001b[A\n","Epoch 1:  17%|█▋        | 20/116 [04:33\u003c19:00, 11.88s/it, training_loss=0.673]\u001b[A\n","Epoch 1:  18%|█▊        | 21/116 [04:33\u003c18:29, 11.68s/it, training_loss=0.673]\u001b[A\n","Epoch 1:  18%|█▊        | 21/116 [04:44\u003c18:29, 11.68s/it, training_loss=0.617]\u001b[A\n","Epoch 1:  19%|█▉        | 22/116 [04:44\u003c17:52, 11.41s/it, training_loss=0.617]\u001b[A\n","Epoch 1:  19%|█▉        | 22/116 [04:56\u003c17:52, 11.41s/it, training_loss=0.553]\u001b[A\n","Epoch 1:  20%|█▉        | 23/116 [04:56\u003c18:09, 11.71s/it, training_loss=0.553]\u001b[A\n","Epoch 1:  20%|█▉        | 23/116 [05:09\u003c18:09, 11.71s/it, training_loss=0.597]\u001b[A\n","Epoch 1:  21%|██        | 24/116 [05:09\u003c18:15, 11.91s/it, training_loss=0.597]\u001b[A\n","Epoch 1:  21%|██        | 24/116 [05:19\u003c18:15, 11.91s/it, training_loss=0.461]\u001b[A\n","Epoch 1:  22%|██▏       | 25/116 [05:19\u003c17:15, 11.38s/it, training_loss=0.461]\u001b[A\n","Epoch 1:  22%|██▏       | 25/116 [05:31\u003c17:15, 11.38s/it, training_loss=0.589]\u001b[A\n","Epoch 1:  22%|██▏       | 26/116 [05:31\u003c17:18, 11.54s/it, training_loss=0.589]\u001b[A\n","Epoch 1:  22%|██▏       | 26/116 [05:45\u003c17:18, 11.54s/it, training_loss=0.538]\u001b[A\n","Epoch 1:  23%|██▎       | 27/116 [05:45\u003c18:19, 12.36s/it, training_loss=0.538]\u001b[A\n","Epoch 1:  23%|██▎       | 27/116 [05:58\u003c18:19, 12.36s/it, training_loss=0.552]\u001b[A\n","Epoch 1:  24%|██▍       | 28/116 [05:58\u003c18:16, 12.46s/it, training_loss=0.552]\u001b[A\n","Epoch 1:  24%|██▍       | 28/116 [06:08\u003c18:16, 12.46s/it, training_loss=0.563]\u001b[A\n","Epoch 1:  25%|██▌       | 29/116 [06:08\u003c17:05, 11.79s/it, training_loss=0.563]\u001b[A\n","Epoch 1:  25%|██▌       | 29/116 [06:20\u003c17:05, 11.79s/it, training_loss=0.540]\u001b[A\n","Epoch 1:  26%|██▌       | 30/116 [06:20\u003c17:01, 11.87s/it, training_loss=0.540]\u001b[A\n","Epoch 1:  26%|██▌       | 30/116 [06:33\u003c17:01, 11.87s/it, training_loss=0.493]\u001b[A\n","Epoch 1:  27%|██▋       | 31/116 [06:33\u003c17:04, 12.05s/it, training_loss=0.493]\u001b[A\n","Epoch 1:  27%|██▋       | 31/116 [06:44\u003c17:04, 12.05s/it, training_loss=0.534]\u001b[A\n","Epoch 1:  28%|██▊       | 32/116 [06:44\u003c16:46, 11.98s/it, training_loss=0.534]\u001b[A\n","Epoch 1:  28%|██▊       | 32/116 [06:55\u003c16:46, 11.98s/it, training_loss=0.511]\u001b[A\n","Epoch 1:  28%|██▊       | 33/116 [06:55\u003c16:01, 11.58s/it, training_loss=0.511]\u001b[A\n","Epoch 1:  28%|██▊       | 33/116 [07:08\u003c16:01, 11.58s/it, training_loss=0.606]\u001b[A\n","Epoch 1:  29%|██▉       | 34/116 [07:08\u003c16:12, 11.86s/it, training_loss=0.606]\u001b[A\n","Epoch 1:  29%|██▉       | 34/116 [07:20\u003c16:12, 11.86s/it, training_loss=0.620]\u001b[A\n","Epoch 1:  30%|███       | 35/116 [07:20\u003c16:15, 12.05s/it, training_loss=0.620]\u001b[A\n","Epoch 1:  30%|███       | 35/116 [07:31\u003c16:15, 12.05s/it, training_loss=0.530]\u001b[A\n","Epoch 1:  31%|███       | 36/116 [07:31\u003c15:34, 11.68s/it, training_loss=0.530]\u001b[A\n","Epoch 1:  31%|███       | 36/116 [07:42\u003c15:34, 11.68s/it, training_loss=0.545]\u001b[A\n","Epoch 1:  32%|███▏      | 37/116 [07:42\u003c15:15, 11.58s/it, training_loss=0.545]\u001b[A\n","Epoch 1:  32%|███▏      | 37/116 [07:55\u003c15:15, 11.58s/it, training_loss=0.518]\u001b[A\n","Epoch 1:  33%|███▎      | 38/116 [07:55\u003c15:31, 11.95s/it, training_loss=0.518]\u001b[A\n","Epoch 1:  33%|███▎      | 38/116 [08:07\u003c15:31, 11.95s/it, training_loss=0.604]\u001b[A\n","Epoch 1:  34%|███▎      | 39/116 [08:07\u003c15:30, 12.08s/it, training_loss=0.604]\u001b[A\n","Epoch 1:  34%|███▎      | 39/116 [08:18\u003c15:30, 12.08s/it, training_loss=0.529]\u001b[A\n","Epoch 1:  34%|███▍      | 40/116 [08:18\u003c14:35, 11.51s/it, training_loss=0.529]\u001b[A\n","Epoch 1:  34%|███▍      | 40/116 [08:30\u003c14:35, 11.51s/it, training_loss=0.554]\u001b[A\n","Epoch 1:  35%|███▌      | 41/116 [08:30\u003c14:39, 11.72s/it, training_loss=0.554]\u001b[A\n","Epoch 1:  35%|███▌      | 41/116 [08:42\u003c14:39, 11.72s/it, training_loss=0.521]\u001b[A\n","Epoch 1:  36%|███▌      | 42/116 [08:42\u003c14:43, 11.94s/it, training_loss=0.521]\u001b[A\n","Epoch 1:  36%|███▌      | 42/116 [08:54\u003c14:43, 11.94s/it, training_loss=0.543]\u001b[A\n","Epoch 1:  37%|███▋      | 43/116 [08:54\u003c14:24, 11.84s/it, training_loss=0.543]\u001b[A\n","Epoch 1:  37%|███▋      | 43/116 [09:05\u003c14:24, 11.84s/it, training_loss=0.483]\u001b[A\n","Epoch 1:  38%|███▊      | 44/116 [09:05\u003c13:48, 11.50s/it, training_loss=0.483]\u001b[A\n","Epoch 1:  38%|███▊      | 44/116 [09:17\u003c13:48, 11.50s/it, training_loss=0.536]\u001b[A\n","Epoch 1:  39%|███▉      | 45/116 [09:17\u003c13:56, 11.78s/it, training_loss=0.536]\u001b[A\n","Epoch 1:  39%|███▉      | 45/116 [09:29\u003c13:56, 11.78s/it, training_loss=0.643]\u001b[A\n","Epoch 1:  40%|███▉      | 46/116 [09:29\u003c13:58, 11.97s/it, training_loss=0.643]\u001b[A\n","Epoch 1:  40%|███▉      | 46/116 [09:40\u003c13:58, 11.97s/it, training_loss=0.434]\u001b[A\n","Epoch 1:  41%|████      | 47/116 [09:40\u003c13:15, 11.53s/it, training_loss=0.434]\u001b[A\n","Epoch 1:  41%|████      | 47/116 [09:51\u003c13:15, 11.53s/it, training_loss=0.388]\u001b[A\n","Epoch 1:  41%|████▏     | 48/116 [09:51\u003c13:04, 11.53s/it, training_loss=0.388]\u001b[A\n","Epoch 1:  41%|████▏     | 48/116 [10:04\u003c13:04, 11.53s/it, training_loss=0.567]\u001b[A\n","Epoch 1:  42%|████▏     | 49/116 [10:04\u003c13:10, 11.80s/it, training_loss=0.567]\u001b[A\n","Epoch 1:  42%|████▏     | 49/116 [10:16\u003c13:10, 11.80s/it, training_loss=0.635]\u001b[A\n","Epoch 1:  43%|████▎     | 50/116 [10:16\u003c13:03, 11.87s/it, training_loss=0.635]\u001b[A\n","Epoch 1:  43%|████▎     | 50/116 [10:26\u003c13:03, 11.87s/it, training_loss=0.645]\u001b[A\n","Epoch 1:  44%|████▍     | 51/116 [10:26\u003c12:21, 11.41s/it, training_loss=0.645]\u001b[A\n","Epoch 1:  44%|████▍     | 51/116 [10:39\u003c12:21, 11.41s/it, training_loss=0.579]\u001b[A\n","Epoch 1:  45%|████▍     | 52/116 [10:39\u003c12:28, 11.70s/it, training_loss=0.579]\u001b[A\n","Epoch 1:  45%|████▍     | 52/116 [10:51\u003c12:28, 11.70s/it, training_loss=0.613]\u001b[A\n","Epoch 1:  46%|████▌     | 53/116 [10:51\u003c12:31, 11.93s/it, training_loss=0.613]\u001b[A\n","Epoch 1:  46%|████▌     | 53/116 [11:02\u003c12:31, 11.93s/it, training_loss=0.535]\u001b[A\n","Epoch 1:  47%|████▋     | 54/116 [11:02\u003c12:04, 11.69s/it, training_loss=0.535]\u001b[A\n","Epoch 1:  47%|████▋     | 54/116 [11:13\u003c12:04, 11.69s/it, training_loss=0.527]\u001b[A\n","Epoch 1:  47%|████▋     | 55/116 [11:13\u003c11:41, 11.50s/it, training_loss=0.527]\u001b[A\n","Epoch 1:  47%|████▋     | 55/116 [11:26\u003c11:41, 11.50s/it, training_loss=0.517]\u001b[A\n","Epoch 1:  48%|████▊     | 56/116 [11:26\u003c11:45, 11.77s/it, training_loss=0.517]\u001b[A\n","Epoch 1:  48%|████▊     | 56/116 [11:38\u003c11:45, 11.77s/it, training_loss=0.530]\u001b[A\n","Epoch 1:  49%|████▉     | 57/116 [11:38\u003c11:45, 11.96s/it, training_loss=0.530]\u001b[A\n","Epoch 1:  49%|████▉     | 57/116 [11:48\u003c11:45, 11.96s/it, training_loss=0.522]\u001b[A\n","Epoch 1:  50%|█████     | 58/116 [11:48\u003c11:02, 11.42s/it, training_loss=0.522]\u001b[A\n","Epoch 1:  50%|█████     | 58/116 [12:01\u003c11:02, 11.42s/it, training_loss=0.561]\u001b[A\n","Epoch 1:  51%|█████     | 59/116 [12:01\u003c11:06, 11.69s/it, training_loss=0.561]\u001b[A\n","Epoch 1:  51%|█████     | 59/116 [12:13\u003c11:06, 11.69s/it, training_loss=0.532]\u001b[A\n","Epoch 1:  52%|█████▏    | 60/116 [12:13\u003c11:11, 11.99s/it, training_loss=0.532]\u001b[A\n","Epoch 1:  52%|█████▏    | 60/116 [12:25\u003c11:11, 11.99s/it, training_loss=0.473]\u001b[A\n","Epoch 1:  53%|█████▎    | 61/116 [12:25\u003c10:55, 11.91s/it, training_loss=0.473]\u001b[A\n","Epoch 1:  53%|█████▎    | 61/116 [12:36\u003c10:55, 11.91s/it, training_loss=0.556]\u001b[A\n","Epoch 1:  53%|█████▎    | 62/116 [12:36\u003c10:22, 11.53s/it, training_loss=0.556]\u001b[A\n","Epoch 1:  53%|█████▎    | 62/116 [12:48\u003c10:22, 11.53s/it, training_loss=0.617]\u001b[A\n","Epoch 1:  54%|█████▍    | 63/116 [12:48\u003c10:25, 11.80s/it, training_loss=0.617]\u001b[A\n","Epoch 1:  54%|█████▍    | 63/116 [13:01\u003c10:25, 11.80s/it, training_loss=0.654]\u001b[A\n","Epoch 1:  55%|█████▌    | 64/116 [13:01\u003c10:24, 12.01s/it, training_loss=0.654]\u001b[A\n","Epoch 1:  55%|█████▌    | 64/116 [13:11\u003c10:24, 12.01s/it, training_loss=0.613]\u001b[A\n","Epoch 1:  56%|█████▌    | 65/116 [13:11\u003c09:51, 11.60s/it, training_loss=0.613]\u001b[A\n","Epoch 1:  56%|█████▌    | 65/116 [13:23\u003c09:51, 11.60s/it, training_loss=0.707]\u001b[A\n","Epoch 1:  57%|█████▋    | 66/116 [13:23\u003c09:38, 11.58s/it, training_loss=0.707]\u001b[A\n","Epoch 1:  57%|█████▋    | 66/116 [13:35\u003c09:38, 11.58s/it, training_loss=0.639]\u001b[A\n","Epoch 1:  58%|█████▊    | 67/116 [13:35\u003c09:40, 11.85s/it, training_loss=0.639]\u001b[A\n","Epoch 1:  58%|█████▊    | 67/116 [13:47\u003c09:40, 11.85s/it, training_loss=0.572]\u001b[A\n","Epoch 1:  59%|█████▊    | 68/116 [13:47\u003c09:32, 11.92s/it, training_loss=0.572]\u001b[A\n","Epoch 1:  59%|█████▊    | 68/116 [13:57\u003c09:32, 11.92s/it, training_loss=0.536]\u001b[A\n","Epoch 1:  59%|█████▉    | 69/116 [13:57\u003c08:56, 11.41s/it, training_loss=0.536]\u001b[A\n","Epoch 1:  59%|█████▉    | 69/116 [14:10\u003c08:56, 11.41s/it, training_loss=0.559]\u001b[A\n","Epoch 1:  60%|██████    | 70/116 [14:10\u003c08:59, 11.72s/it, training_loss=0.559]\u001b[A\n","Epoch 1:  60%|██████    | 70/116 [14:22\u003c08:59, 11.72s/it, training_loss=0.549]\u001b[A\n","Epoch 1:  61%|██████    | 71/116 [14:22\u003c08:58, 11.96s/it, training_loss=0.549]\u001b[A\n","Epoch 1:  61%|██████    | 71/116 [14:34\u003c08:58, 11.96s/it, training_loss=0.504]\u001b[A\n","Epoch 1:  62%|██████▏   | 72/116 [14:34\u003c08:34, 11.70s/it, training_loss=0.504]\u001b[A\n","Epoch 1:  62%|██████▏   | 72/116 [14:45\u003c08:34, 11.70s/it, training_loss=0.494]\u001b[A\n","Epoch 1:  63%|██████▎   | 73/116 [14:45\u003c08:14, 11.49s/it, training_loss=0.494]\u001b[A\n","Epoch 1:  63%|██████▎   | 73/116 [14:57\u003c08:14, 11.49s/it, training_loss=0.530]\u001b[A\n","Epoch 1:  64%|██████▍   | 74/116 [14:57\u003c08:18, 11.87s/it, training_loss=0.530]\u001b[A\n","Epoch 1:  64%|██████▍   | 74/116 [15:10\u003c08:18, 11.87s/it, training_loss=0.521]\u001b[A\n","Epoch 1:  65%|██████▍   | 75/116 [15:10\u003c08:14, 12.05s/it, training_loss=0.521]\u001b[A\n","Epoch 1:  65%|██████▍   | 75/116 [15:20\u003c08:14, 12.05s/it, training_loss=0.523]\u001b[A\n","Epoch 1:  66%|██████▌   | 76/116 [15:20\u003c07:39, 11.50s/it, training_loss=0.523]\u001b[A\n","Epoch 1:  66%|██████▌   | 76/116 [15:32\u003c07:39, 11.50s/it, training_loss=0.605]\u001b[A\n","Epoch 1:  66%|██████▋   | 77/116 [15:32\u003c07:33, 11.63s/it, training_loss=0.605]\u001b[A\n","Epoch 1:  66%|██████▋   | 77/116 [15:44\u003c07:33, 11.63s/it, training_loss=0.573]\u001b[A\n","Epoch 1:  67%|██████▋   | 78/116 [15:44\u003c07:32, 11.90s/it, training_loss=0.573]\u001b[A\n","Epoch 1:  67%|██████▋   | 78/116 [15:59\u003c07:32, 11.90s/it, training_loss=0.610]\u001b[A\n","Epoch 1:  68%|██████▊   | 79/116 [15:59\u003c07:52, 12.77s/it, training_loss=0.610]\u001b[A\n","Epoch 1:  68%|██████▊   | 79/116 [16:09\u003c07:52, 12.77s/it, training_loss=0.503]\u001b[A\n","Epoch 1:  69%|██████▉   | 80/116 [16:09\u003c07:11, 12.00s/it, training_loss=0.503]\u001b[A\n","Epoch 1:  69%|██████▉   | 80/116 [16:22\u003c07:11, 12.00s/it, training_loss=0.555]\u001b[A\n","Epoch 1:  70%|██████▉   | 81/116 [16:22\u003c07:03, 12.11s/it, training_loss=0.555]\u001b[A\n","Epoch 1:  70%|██████▉   | 81/116 [16:34\u003c07:03, 12.11s/it, training_loss=0.499]\u001b[A\n","Epoch 1:  71%|███████   | 82/116 [16:34\u003c06:56, 12.24s/it, training_loss=0.499]\u001b[A\n","Epoch 1:  71%|███████   | 82/116 [16:46\u003c06:56, 12.24s/it, training_loss=0.587]\u001b[A\n","Epoch 1:  72%|███████▏  | 83/116 [16:46\u003c06:37, 12.05s/it, training_loss=0.587]\u001b[A\n","Epoch 1:  72%|███████▏  | 83/116 [16:57\u003c06:37, 12.05s/it, training_loss=0.547]\u001b[A\n","Epoch 1:  72%|███████▏  | 84/116 [16:57\u003c06:14, 11.72s/it, training_loss=0.547]\u001b[A\n","Epoch 1:  72%|███████▏  | 84/116 [17:10\u003c06:14, 11.72s/it, training_loss=0.458]\u001b[A\n","Epoch 1:  73%|███████▎  | 85/116 [17:10\u003c06:11, 11.99s/it, training_loss=0.458]\u001b[A\n","Epoch 1:  73%|███████▎  | 85/116 [17:22\u003c06:11, 11.99s/it, training_loss=0.401]\u001b[A\n","Epoch 1:  74%|███████▍  | 86/116 [17:22\u003c06:04, 12.16s/it, training_loss=0.401]\u001b[A\n","Epoch 1:  74%|███████▍  | 86/116 [17:33\u003c06:04, 12.16s/it, training_loss=0.603]\u001b[A\n","Epoch 1:  75%|███████▌  | 87/116 [17:33\u003c05:44, 11.89s/it, training_loss=0.603]\u001b[A\n","Epoch 1:  75%|███████▌  | 87/116 [17:45\u003c05:44, 11.89s/it, training_loss=0.451]\u001b[A\n","Epoch 1:  76%|███████▌  | 88/116 [17:45\u003c05:29, 11.77s/it, training_loss=0.451]\u001b[A\n","Epoch 1:  76%|███████▌  | 88/116 [17:58\u003c05:29, 11.77s/it, training_loss=0.452]\u001b[A\n","Epoch 1:  77%|███████▋  | 89/116 [17:58\u003c05:24, 12.03s/it, training_loss=0.452]\u001b[A\n","Epoch 1:  77%|███████▋  | 89/116 [18:10\u003c05:24, 12.03s/it, training_loss=0.555]\u001b[A\n","Epoch 1:  78%|███████▊  | 90/116 [18:10\u003c05:16, 12.15s/it, training_loss=0.555]\u001b[A\n","Epoch 1:  78%|███████▊  | 90/116 [18:20\u003c05:16, 12.15s/it, training_loss=0.635]\u001b[A\n","Epoch 1:  78%|███████▊  | 91/116 [18:20\u003c04:49, 11.57s/it, training_loss=0.635]\u001b[A\n","Epoch 1:  78%|███████▊  | 91/116 [18:32\u003c04:49, 11.57s/it, training_loss=0.390]\u001b[A\n","Epoch 1:  79%|███████▉  | 92/116 [18:32\u003c04:42, 11.77s/it, training_loss=0.390]\u001b[A\n","Epoch 1:  79%|███████▉  | 92/116 [18:45\u003c04:42, 11.77s/it, training_loss=0.562]\u001b[A\n","Epoch 1:  80%|████████  | 93/116 [18:45\u003c04:35, 12.00s/it, training_loss=0.562]\u001b[A\n","Epoch 1:  80%|████████  | 93/116 [18:57\u003c04:35, 12.00s/it, training_loss=0.565]\u001b[A\n","Epoch 1:  81%|████████  | 94/116 [18:57\u003c04:23, 11.98s/it, training_loss=0.565]\u001b[A\n","Epoch 1:  81%|████████  | 94/116 [19:08\u003c04:23, 11.98s/it, training_loss=0.473]\u001b[A\n","Epoch 1:  82%|████████▏ | 95/116 [19:08\u003c04:03, 11.58s/it, training_loss=0.473]\u001b[A\n","Epoch 1:  82%|████████▏ | 95/116 [19:20\u003c04:03, 11.58s/it, training_loss=0.482]\u001b[A\n","Epoch 1:  83%|████████▎ | 96/116 [19:20\u003c03:56, 11.84s/it, training_loss=0.482]\u001b[A\n","Epoch 1:  83%|████████▎ | 96/116 [19:33\u003c03:56, 11.84s/it, training_loss=0.375]\u001b[A\n","Epoch 1:  84%|████████▎ | 97/116 [19:33\u003c03:48, 12.05s/it, training_loss=0.375]\u001b[A\n","Epoch 1:  84%|████████▎ | 97/116 [19:44\u003c03:48, 12.05s/it, training_loss=0.595]\u001b[A\n","Epoch 1:  84%|████████▍ | 98/116 [19:44\u003c03:31, 11.74s/it, training_loss=0.595]\u001b[A\n","Epoch 1:  84%|████████▍ | 98/116 [19:55\u003c03:31, 11.74s/it, training_loss=0.529]\u001b[A\n","Epoch 1:  85%|████████▌ | 99/116 [19:55\u003c03:17, 11.60s/it, training_loss=0.529]\u001b[A\n","Epoch 1:  85%|████████▌ | 99/116 [20:07\u003c03:17, 11.60s/it, training_loss=0.345]\u001b[A\n","Epoch 1:  86%|████████▌ | 100/116 [20:07\u003c03:10, 11.88s/it, training_loss=0.345]\u001b[A\n","Epoch 1:  86%|████████▌ | 100/116 [20:20\u003c03:10, 11.88s/it, training_loss=0.668]\u001b[A\n","Epoch 1:  87%|████████▋ | 101/116 [20:20\u003c03:01, 12.08s/it, training_loss=0.668]\u001b[A\n","Epoch 1:  87%|████████▋ | 101/116 [20:30\u003c03:01, 12.08s/it, training_loss=0.485]\u001b[A\n","Epoch 1:  88%|████████▊ | 102/116 [20:30\u003c02:41, 11.54s/it, training_loss=0.485]\u001b[A\n","Epoch 1:  88%|████████▊ | 102/116 [20:42\u003c02:41, 11.54s/it, training_loss=0.775]\u001b[A\n","Epoch 1:  89%|████████▉ | 103/116 [20:42\u003c02:31, 11.68s/it, training_loss=0.775]\u001b[A\n","Epoch 1:  89%|████████▉ | 103/116 [20:55\u003c02:31, 11.68s/it, training_loss=0.374]\u001b[A\n","Epoch 1:  90%|████████▉ | 104/116 [20:55\u003c02:23, 11.95s/it, training_loss=0.374]\u001b[A\n","Epoch 1:  90%|████████▉ | 104/116 [21:07\u003c02:23, 11.95s/it, training_loss=0.412]\u001b[A\n","Epoch 1:  91%|█████████ | 105/116 [21:07\u003c02:11, 11.97s/it, training_loss=0.412]\u001b[A\n","Epoch 1:  91%|█████████ | 105/116 [21:17\u003c02:11, 11.97s/it, training_loss=0.635]\u001b[A\n","Epoch 1:  91%|█████████▏| 106/116 [21:17\u003c01:55, 11.51s/it, training_loss=0.635]\u001b[A\n","Epoch 1:  91%|█████████▏| 106/116 [21:30\u003c01:55, 11.51s/it, training_loss=0.534]\u001b[A\n","Epoch 1:  92%|█████████▏| 107/116 [21:30\u003c01:46, 11.78s/it, training_loss=0.534]\u001b[A\n","Epoch 1:  92%|█████████▏| 107/116 [21:42\u003c01:46, 11.78s/it, training_loss=0.426]\u001b[A\n","Epoch 1:  93%|█████████▎| 108/116 [21:42\u003c01:35, 11.99s/it, training_loss=0.426]\u001b[A\n","Epoch 1:  93%|█████████▎| 108/116 [21:53\u003c01:35, 11.99s/it, training_loss=0.445]\u001b[A\n","Epoch 1:  94%|█████████▍| 109/116 [21:53\u003c01:21, 11.66s/it, training_loss=0.445]\u001b[A\n","Epoch 1:  94%|█████████▍| 109/116 [22:04\u003c01:21, 11.66s/it, training_loss=0.720]\u001b[A\n","Epoch 1:  95%|█████████▍| 110/116 [22:04\u003c01:09, 11.58s/it, training_loss=0.720]\u001b[A\n","Epoch 1:  95%|█████████▍| 110/116 [22:17\u003c01:09, 11.58s/it, training_loss=0.408]\u001b[A\n","Epoch 1:  96%|█████████▌| 111/116 [22:17\u003c00:59, 11.90s/it, training_loss=0.408]\u001b[A\n","Epoch 1:  96%|█████████▌| 111/116 [22:29\u003c00:59, 11.90s/it, training_loss=0.493]\u001b[A\n","Epoch 1:  97%|█████████▋| 112/116 [22:29\u003c00:48, 12.06s/it, training_loss=0.493]\u001b[A\n","Epoch 1:  97%|█████████▋| 112/116 [22:40\u003c00:48, 12.06s/it, training_loss=0.355]\u001b[A\n","Epoch 1:  97%|█████████▋| 113/116 [22:40\u003c00:34, 11.49s/it, training_loss=0.355]\u001b[A\n","Epoch 1:  97%|█████████▋| 113/116 [22:52\u003c00:34, 11.49s/it, training_loss=0.661]\u001b[A\n","Epoch 1:  98%|█████████▊| 114/116 [22:52\u003c00:23, 11.68s/it, training_loss=0.661]\u001b[A\n","Epoch 1:  98%|█████████▊| 114/116 [23:04\u003c00:23, 11.68s/it, training_loss=0.474]\u001b[A\n","Epoch 1:  99%|█████████▉| 115/116 [23:04\u003c00:11, 11.95s/it, training_loss=0.474]\u001b[A\n","Epoch 1:  99%|█████████▉| 115/116 [23:10\u003c00:11, 11.95s/it, training_loss=0.400]\u001b[A\n","Epoch 1: 100%|██████████| 116/116 [23:10\u003c00:00, 10.05s/it, training_loss=0.400]\u001b[A\n","  0%|          | 0/4 [23:10\u003c?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 1\n","Training loss: 1.6669005227499996\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▌       | 1/4 [24:58\u003c1:14:54, 1498.02s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss: 1.4375267172681874\n","F1 Score (Weighted): 0.2141778139607347\n","QWK Score: 0.17446615174920488\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 2:   0%|          | 0/116 [00:00\u003c?, ?it/s]\u001b[A\n","Epoch 2:   0%|          | 0/116 [00:11\u003c?, ?it/s, training_loss=0.363]\u001b[A\n","Epoch 2:   1%|          | 1/116 [00:11\u003c22:25, 11.70s/it, training_loss=0.363]\u001b[A\n","Epoch 2:   1%|          | 1/116 [00:22\u003c22:25, 11.70s/it, training_loss=0.415]\u001b[A\n","Epoch 2:   2%|▏         | 2/116 [00:22\u003c21:07, 11.11s/it, training_loss=0.415]\u001b[A\n","Epoch 2:   2%|▏         | 2/116 [00:34\u003c21:07, 11.11s/it, training_loss=0.455]\u001b[A\n","Epoch 2:   3%|▎         | 3/116 [00:34\u003c22:07, 11.75s/it, training_loss=0.455]\u001b[A\n","Epoch 2:   3%|▎         | 3/116 [00:47\u003c22:07, 11.75s/it, training_loss=0.477]\u001b[A\n","Epoch 2:   3%|▎         | 4/116 [00:47\u003c22:30, 12.06s/it, training_loss=0.477]\u001b[A\n","Epoch 2:   3%|▎         | 4/116 [00:58\u003c22:30, 12.06s/it, training_loss=0.456]\u001b[A\n","Epoch 2:   4%|▍         | 5/116 [00:58\u003c21:22, 11.56s/it, training_loss=0.456]\u001b[A\n","Epoch 2:   4%|▍         | 5/116 [01:09\u003c21:22, 11.56s/it, training_loss=0.598]\u001b[A\n","Epoch 2:   5%|▌         | 6/116 [01:09\u003c21:15, 11.59s/it, training_loss=0.598]\u001b[A\n","Epoch 2:   5%|▌         | 6/116 [01:22\u003c21:15, 11.59s/it, training_loss=0.412]\u001b[A\n","Epoch 2:   6%|▌         | 7/116 [01:22\u003c21:33, 11.87s/it, training_loss=0.412]\u001b[A\n","Epoch 2:   6%|▌         | 7/116 [01:34\u003c21:33, 11.87s/it, training_loss=0.394]\u001b[A\n","Epoch 2:   7%|▋         | 8/116 [01:34\u003c21:37, 12.01s/it, training_loss=0.394]\u001b[A\n","Epoch 2:   7%|▋         | 8/116 [01:44\u003c21:37, 12.01s/it, training_loss=0.425]\u001b[A\n","Epoch 2:   8%|▊         | 9/116 [01:44\u003c20:20, 11.40s/it, training_loss=0.425]\u001b[A\n","Epoch 2:   8%|▊         | 9/116 [01:57\u003c20:20, 11.40s/it, training_loss=0.479]\u001b[A\n","Epoch 2:   9%|▊         | 10/116 [01:57\u003c20:46, 11.76s/it, training_loss=0.479]\u001b[A\n","Epoch 2:   9%|▊         | 10/116 [02:09\u003c20:46, 11.76s/it, training_loss=0.375]\u001b[A\n","Epoch 2:   9%|▉         | 11/116 [02:09\u003c20:59, 12.00s/it, training_loss=0.375]\u001b[A\n","Epoch 2:   9%|▉         | 11/116 [02:21\u003c20:59, 12.00s/it, training_loss=0.415]\u001b[A\n","Epoch 2:  10%|█         | 12/116 [02:21\u003c20:39, 11.92s/it, training_loss=0.415]\u001b[A\n","Epoch 2:  10%|█         | 12/116 [02:32\u003c20:39, 11.92s/it, training_loss=0.347]\u001b[A\n","Epoch 2:  11%|█         | 13/116 [02:32\u003c19:54, 11.59s/it, training_loss=0.347]\u001b[A\n","Epoch 2:  11%|█         | 13/116 [02:44\u003c19:54, 11.59s/it, training_loss=0.487]\u001b[A\n","Epoch 2:  12%|█▏        | 14/116 [02:44\u003c20:12, 11.88s/it, training_loss=0.487]\u001b[A\n","Epoch 2:  12%|█▏        | 14/116 [02:57\u003c20:12, 11.88s/it, training_loss=0.420]\u001b[A\n","Epoch 2:  13%|█▎        | 15/116 [02:57\u003c20:20, 12.09s/it, training_loss=0.420]\u001b[A\n","Epoch 2:  13%|█▎        | 15/116 [03:08\u003c20:20, 12.09s/it, training_loss=0.286]\u001b[A\n","Epoch 2:  14%|█▍        | 16/116 [03:08\u003c19:35, 11.76s/it, training_loss=0.286]\u001b[A\n","Epoch 2:  14%|█▍        | 16/116 [03:19\u003c19:35, 11.76s/it, training_loss=0.632]\u001b[A\n","Epoch 2:  15%|█▍        | 17/116 [03:19\u003c19:13, 11.65s/it, training_loss=0.632]\u001b[A\n","Epoch 2:  15%|█▍        | 17/116 [03:32\u003c19:13, 11.65s/it, training_loss=0.517]\u001b[A\n","Epoch 2:  16%|█▌        | 18/116 [03:32\u003c19:27, 11.91s/it, training_loss=0.517]\u001b[A\n","Epoch 2:  16%|█▌        | 18/116 [03:44\u003c19:27, 11.91s/it, training_loss=0.231]\u001b[A\n","Epoch 2:  16%|█▋        | 19/116 [03:44\u003c19:29, 12.05s/it, training_loss=0.231]\u001b[A\n","Epoch 2:  16%|█▋        | 19/116 [03:54\u003c19:29, 12.05s/it, training_loss=0.329]\u001b[A\n","Epoch 2:  17%|█▋        | 20/116 [03:54\u003c18:21, 11.47s/it, training_loss=0.329]\u001b[A\n","Epoch 2:  17%|█▋        | 20/116 [04:07\u003c18:21, 11.47s/it, training_loss=0.588]\u001b[A\n","Epoch 2:  18%|█▊        | 21/116 [04:07\u003c18:35, 11.74s/it, training_loss=0.588]\u001b[A\n","Epoch 2:  18%|█▊        | 21/116 [04:19\u003c18:35, 11.74s/it, training_loss=0.290]\u001b[A\n","Epoch 2:  19%|█▉        | 22/116 [04:19\u003c18:45, 11.97s/it, training_loss=0.290]\u001b[A\n","Epoch 2:  19%|█▉        | 22/116 [04:31\u003c18:45, 11.97s/it, training_loss=0.328]\u001b[A\n","Epoch 2:  20%|█▉        | 23/116 [04:31\u003c18:42, 12.07s/it, training_loss=0.328]\u001b[A\n","Epoch 2:  20%|█▉        | 23/116 [04:44\u003c18:42, 12.07s/it, training_loss=0.377]\u001b[A\n","Epoch 2:  21%|██        | 24/116 [04:44\u003c18:45, 12.23s/it, training_loss=0.377]\u001b[A\n","Epoch 2:  21%|██        | 24/116 [04:57\u003c18:45, 12.23s/it, training_loss=0.346]\u001b[A\n","Epoch 2:  22%|██▏       | 25/116 [04:57\u003c18:43, 12.35s/it, training_loss=0.346]\u001b[A\n","Epoch 2:  22%|██▏       | 25/116 [05:09\u003c18:43, 12.35s/it, training_loss=0.365]\u001b[A\n","Epoch 2:  22%|██▏       | 26/116 [05:09\u003c18:42, 12.48s/it, training_loss=0.365]\u001b[A\n","Epoch 2:  22%|██▏       | 26/116 [05:22\u003c18:42, 12.48s/it, training_loss=0.383]\u001b[A\n","Epoch 2:  23%|██▎       | 27/116 [05:22\u003c18:19, 12.35s/it, training_loss=0.383]\u001b[A\n","Epoch 2:  23%|██▎       | 27/116 [05:32\u003c18:19, 12.35s/it, training_loss=0.316]\u001b[A\n","Epoch 2:  24%|██▍       | 28/116 [05:32\u003c17:23, 11.86s/it, training_loss=0.316]\u001b[A\n","Epoch 2:  24%|██▍       | 28/116 [05:45\u003c17:23, 11.86s/it, training_loss=0.408]\u001b[A\n","Epoch 2:  25%|██▌       | 29/116 [05:45\u003c17:27, 12.04s/it, training_loss=0.408]\u001b[A\n","Epoch 2:  25%|██▌       | 29/116 [05:57\u003c17:27, 12.04s/it, training_loss=0.609]\u001b[A\n","Epoch 2:  26%|██▌       | 30/116 [05:57\u003c17:27, 12.18s/it, training_loss=0.609]\u001b[A\n","Epoch 2:  26%|██▌       | 30/116 [06:08\u003c17:27, 12.18s/it, training_loss=1.237]\u001b[A\n","Epoch 2:  27%|██▋       | 31/116 [06:08\u003c16:48, 11.87s/it, training_loss=1.237]\u001b[A\n","Epoch 2:  27%|██▋       | 31/116 [06:19\u003c16:48, 11.87s/it, training_loss=0.853]\u001b[A\n","Epoch 2:  28%|██▊       | 32/116 [06:20\u003c16:18, 11.65s/it, training_loss=0.853]\u001b[A\n","Epoch 2:  28%|██▊       | 32/116 [06:32\u003c16:18, 11.65s/it, training_loss=0.436]\u001b[A\n","Epoch 2:  28%|██▊       | 33/116 [06:32\u003c16:28, 11.91s/it, training_loss=0.436]\u001b[A\n","Epoch 2:  28%|██▊       | 33/116 [06:45\u003c16:28, 11.91s/it, training_loss=0.643]\u001b[A\n","Epoch 2:  29%|██▉       | 34/116 [06:45\u003c16:32, 12.11s/it, training_loss=0.643]\u001b[A\n","Epoch 2:  29%|██▉       | 34/116 [06:55\u003c16:32, 12.11s/it, training_loss=0.395]\u001b[A\n","Epoch 2:  30%|███       | 35/116 [06:55\u003c15:37, 11.58s/it, training_loss=0.395]\u001b[A\n","Epoch 2:  30%|███       | 35/116 [07:07\u003c15:37, 11.58s/it, training_loss=0.576]\u001b[A\n","Epoch 2:  31%|███       | 36/116 [07:07\u003c15:38, 11.73s/it, training_loss=0.576]\u001b[A\n","Epoch 2:  31%|███       | 36/116 [07:20\u003c15:38, 11.73s/it, training_loss=0.377]\u001b[A\n","Epoch 2:  32%|███▏      | 37/116 [07:20\u003c15:46, 11.98s/it, training_loss=0.377]\u001b[A\n","Epoch 2:  32%|███▏      | 37/116 [07:32\u003c15:46, 11.98s/it, training_loss=0.235]\u001b[A\n","Epoch 2:  33%|███▎      | 38/116 [07:32\u003c15:33, 11.97s/it, training_loss=0.235]\u001b[A\n","Epoch 2:  33%|███▎      | 38/116 [07:42\u003c15:33, 11.97s/it, training_loss=0.303]\u001b[A\n","Epoch 2:  34%|███▎      | 39/116 [07:42\u003c14:49, 11.56s/it, training_loss=0.303]\u001b[A\n","Epoch 2:  34%|███▎      | 39/116 [07:55\u003c14:49, 11.56s/it, training_loss=0.381]\u001b[A\n","Epoch 2:  34%|███▍      | 40/116 [07:55\u003c14:58, 11.83s/it, training_loss=0.381]\u001b[A\n","Epoch 2:  34%|███▍      | 40/116 [08:07\u003c14:58, 11.83s/it, training_loss=0.344]\u001b[A\n","Epoch 2:  35%|███▌      | 41/116 [08:07\u003c15:03, 12.05s/it, training_loss=0.344]\u001b[A\n","Epoch 2:  35%|███▌      | 41/116 [08:18\u003c15:03, 12.05s/it, training_loss=0.658]\u001b[A\n","Epoch 2:  36%|███▌      | 42/116 [08:18\u003c14:34, 11.81s/it, training_loss=0.658]\u001b[A\n","Epoch 2:  36%|███▌      | 42/116 [08:30\u003c14:34, 11.81s/it, training_loss=0.460]\u001b[A\n","Epoch 2:  37%|███▋      | 43/116 [08:30\u003c14:07, 11.61s/it, training_loss=0.460]\u001b[A\n","Epoch 2:  37%|███▋      | 43/116 [08:42\u003c14:07, 11.61s/it, training_loss=0.498]\u001b[A\n","Epoch 2:  38%|███▊      | 44/116 [08:42\u003c14:13, 11.86s/it, training_loss=0.498]\u001b[A\n","Epoch 2:  38%|███▊      | 44/116 [08:54\u003c14:13, 11.86s/it, training_loss=0.509]\u001b[A\n","Epoch 2:  39%|███▉      | 45/116 [08:54\u003c14:13, 12.02s/it, training_loss=0.509]\u001b[A\n","Epoch 2:  39%|███▉      | 45/116 [09:04\u003c14:13, 12.02s/it, training_loss=0.294]\u001b[A\n","Epoch 2:  40%|███▉      | 46/116 [09:04\u003c13:20, 11.44s/it, training_loss=0.294]\u001b[A\n","Epoch 2:  40%|███▉      | 46/116 [09:17\u003c13:20, 11.44s/it, training_loss=0.397]\u001b[A\n","Epoch 2:  41%|████      | 47/116 [09:17\u003c13:23, 11.65s/it, training_loss=0.397]\u001b[A\n","Epoch 2:  41%|████      | 47/116 [09:29\u003c13:23, 11.65s/it, training_loss=0.273]\u001b[A\n","Epoch 2:  41%|████▏     | 48/116 [09:29\u003c13:29, 11.90s/it, training_loss=0.273]\u001b[A\n","Epoch 2:  41%|████▏     | 48/116 [09:41\u003c13:29, 11.90s/it, training_loss=0.406]\u001b[A\n","Epoch 2:  42%|████▏     | 49/116 [09:41\u003c13:12, 11.83s/it, training_loss=0.406]\u001b[A\n","Epoch 2:  42%|████▏     | 49/116 [09:51\u003c13:12, 11.83s/it, training_loss=0.443]\u001b[A\n","Epoch 2:  43%|████▎     | 50/116 [09:51\u003c12:36, 11.46s/it, training_loss=0.443]\u001b[A\n","Epoch 2:  43%|████▎     | 50/116 [10:04\u003c12:36, 11.46s/it, training_loss=0.453]\u001b[A\n","Epoch 2:  44%|████▍     | 51/116 [10:04\u003c12:48, 11.82s/it, training_loss=0.453]\u001b[A\n","Epoch 2:  44%|████▍     | 51/116 [10:17\u003c12:48, 11.82s/it, training_loss=0.655]\u001b[A\n","Epoch 2:  45%|████▍     | 52/116 [10:17\u003c12:50, 12.04s/it, training_loss=0.655]\u001b[A\n","Epoch 2:  45%|████▍     | 52/116 [10:27\u003c12:50, 12.04s/it, training_loss=0.329]\u001b[A\n","Epoch 2:  46%|████▌     | 53/116 [10:27\u003c12:12, 11.63s/it, training_loss=0.329]\u001b[A\n","Epoch 2:  46%|████▌     | 53/116 [10:39\u003c12:12, 11.63s/it, training_loss=0.509]\u001b[A\n","Epoch 2:  47%|████▋     | 54/116 [10:39\u003c12:00, 11.62s/it, training_loss=0.509]\u001b[A\n","Epoch 2:  47%|████▋     | 54/116 [10:51\u003c12:00, 11.62s/it, training_loss=0.354]\u001b[A\n","Epoch 2:  47%|████▋     | 55/116 [10:51\u003c12:03, 11.86s/it, training_loss=0.354]\u001b[A\n","Epoch 2:  47%|████▋     | 55/116 [11:03\u003c12:03, 11.86s/it, training_loss=0.490]\u001b[A\n","Epoch 2:  48%|████▊     | 56/116 [11:03\u003c11:57, 11.95s/it, training_loss=0.490]\u001b[A\n","Epoch 2:  48%|████▊     | 56/116 [11:14\u003c11:57, 11.95s/it, training_loss=0.404]\u001b[A\n","Epoch 2:  49%|████▉     | 57/116 [11:14\u003c11:13, 11.42s/it, training_loss=0.404]\u001b[A\n","Epoch 2:  49%|████▉     | 57/116 [11:26\u003c11:13, 11.42s/it, training_loss=0.347]\u001b[A\n","Epoch 2:  50%|█████     | 58/116 [11:26\u003c11:21, 11.74s/it, training_loss=0.347]\u001b[A\n","Epoch 2:  50%|█████     | 58/116 [11:39\u003c11:21, 11.74s/it, training_loss=0.371]\u001b[A\n","Epoch 2:  51%|█████     | 59/116 [11:39\u003c11:22, 11.97s/it, training_loss=0.371]\u001b[A\n","Epoch 2:  51%|█████     | 59/116 [11:50\u003c11:22, 11.97s/it, training_loss=0.532]\u001b[A\n","Epoch 2:  52%|█████▏    | 60/116 [11:50\u003c10:55, 11.71s/it, training_loss=0.532]\u001b[A\n","Epoch 2:  52%|█████▏    | 60/116 [12:01\u003c10:55, 11.71s/it, training_loss=0.436]\u001b[A\n","Epoch 2:  53%|█████▎    | 61/116 [12:01\u003c10:36, 11.57s/it, training_loss=0.436]\u001b[A\n","Epoch 2:  53%|█████▎    | 61/116 [12:13\u003c10:36, 11.57s/it, training_loss=0.396]\u001b[A\n","Epoch 2:  53%|█████▎    | 62/116 [12:13\u003c10:40, 11.85s/it, training_loss=0.396]\u001b[A\n","Epoch 2:  53%|█████▎    | 62/116 [12:26\u003c10:40, 11.85s/it, training_loss=0.449]\u001b[A\n","Epoch 2:  54%|█████▍    | 63/116 [12:26\u003c10:38, 12.04s/it, training_loss=0.449]\u001b[A\n","Epoch 2:  54%|█████▍    | 63/116 [12:36\u003c10:38, 12.04s/it, training_loss=0.515]\u001b[A\n","Epoch 2:  55%|█████▌    | 64/116 [12:36\u003c09:58, 11.51s/it, training_loss=0.515]\u001b[A\n","Epoch 2:  55%|█████▌    | 64/116 [12:48\u003c09:58, 11.51s/it, training_loss=0.395]\u001b[A\n","Epoch 2:  56%|█████▌    | 65/116 [12:48\u003c09:53, 11.64s/it, training_loss=0.395]\u001b[A\n","Epoch 2:  56%|█████▌    | 65/116 [13:01\u003c09:53, 11.64s/it, training_loss=0.421]\u001b[A\n","Epoch 2:  57%|█████▋    | 66/116 [13:01\u003c09:55, 11.92s/it, training_loss=0.421]\u001b[A\n","Epoch 2:  57%|█████▋    | 66/116 [13:13\u003c09:55, 11.92s/it, training_loss=0.343]\u001b[A\n","Epoch 2:  58%|█████▊    | 67/116 [13:13\u003c09:43, 11.91s/it, training_loss=0.343]\u001b[A\n","Epoch 2:  58%|█████▊    | 67/116 [13:23\u003c09:43, 11.91s/it, training_loss=0.498]\u001b[A\n","Epoch 2:  59%|█████▊    | 68/116 [13:23\u003c09:12, 11.51s/it, training_loss=0.498]\u001b[A\n","Epoch 2:  59%|█████▊    | 68/116 [13:36\u003c09:12, 11.51s/it, training_loss=0.327]\u001b[A\n","Epoch 2:  59%|█████▉    | 69/116 [13:36\u003c09:14, 11.81s/it, training_loss=0.327]\u001b[A\n","Epoch 2:  59%|█████▉    | 69/116 [13:48\u003c09:14, 11.81s/it, training_loss=0.437]\u001b[A\n","Epoch 2:  60%|██████    | 70/116 [13:48\u003c09:13, 12.03s/it, training_loss=0.437]\u001b[A\n","Epoch 2:  60%|██████    | 70/116 [13:59\u003c09:13, 12.03s/it, training_loss=0.550]\u001b[A\n","Epoch 2:  61%|██████    | 71/116 [13:59\u003c08:50, 11.79s/it, training_loss=0.550]\u001b[A\n","Epoch 2:  61%|██████    | 71/116 [14:11\u003c08:50, 11.79s/it, training_loss=0.407]\u001b[A\n","Epoch 2:  62%|██████▏   | 72/116 [14:11\u003c08:32, 11.65s/it, training_loss=0.407]\u001b[A\n","Epoch 2:  62%|██████▏   | 72/116 [14:23\u003c08:32, 11.65s/it, training_loss=0.333]\u001b[A\n","Epoch 2:  63%|██████▎   | 73/116 [14:23\u003c08:31, 11.89s/it, training_loss=0.333]\u001b[A\n","Epoch 2:  63%|██████▎   | 73/116 [14:36\u003c08:31, 11.89s/it, training_loss=0.268]\u001b[A\n","Epoch 2:  64%|██████▍   | 74/116 [14:36\u003c08:26, 12.07s/it, training_loss=0.268]\u001b[A\n","Epoch 2:  64%|██████▍   | 74/116 [14:46\u003c08:26, 12.07s/it, training_loss=0.592]\u001b[A\n","Epoch 2:  65%|██████▍   | 75/116 [14:46\u003c07:54, 11.56s/it, training_loss=0.592]\u001b[A\n","Epoch 2:  65%|██████▍   | 75/116 [14:58\u003c07:54, 11.56s/it, training_loss=0.331]\u001b[A\n","Epoch 2:  66%|██████▌   | 76/116 [14:58\u003c07:46, 11.65s/it, training_loss=0.331]\u001b[A\n","Epoch 2:  66%|██████▌   | 76/116 [15:10\u003c07:46, 11.65s/it, training_loss=0.545]\u001b[A\n","Epoch 2:  66%|██████▋   | 77/116 [15:10\u003c07:44, 11.90s/it, training_loss=0.545]\u001b[A\n","Epoch 2:  66%|██████▋   | 77/116 [15:22\u003c07:44, 11.90s/it, training_loss=0.442]\u001b[A\n","Epoch 2:  67%|██████▋   | 78/116 [15:22\u003c07:32, 11.91s/it, training_loss=0.442]\u001b[A\n","Epoch 2:  67%|██████▋   | 78/116 [15:33\u003c07:32, 11.91s/it, training_loss=0.479]\u001b[A\n","Epoch 2:  68%|██████▊   | 79/116 [15:33\u003c07:05, 11.50s/it, training_loss=0.479]\u001b[A\n","Epoch 2:  68%|██████▊   | 79/116 [15:45\u003c07:05, 11.50s/it, training_loss=0.511]\u001b[A\n","Epoch 2:  69%|██████▉   | 80/116 [15:45\u003c07:04, 11.80s/it, training_loss=0.511]\u001b[A\n","Epoch 2:  69%|██████▉   | 80/116 [15:58\u003c07:04, 11.80s/it, training_loss=0.471]\u001b[A\n","Epoch 2:  70%|██████▉   | 81/116 [15:58\u003c07:01, 12.04s/it, training_loss=0.471]\u001b[A\n","Epoch 2:  70%|██████▉   | 81/116 [16:09\u003c07:01, 12.04s/it, training_loss=0.510]\u001b[A\n","Epoch 2:  71%|███████   | 82/116 [16:09\u003c06:39, 11.76s/it, training_loss=0.510]\u001b[A\n","Epoch 2:  71%|███████   | 82/116 [16:20\u003c06:39, 11.76s/it, training_loss=0.371]\u001b[A\n","Epoch 2:  72%|███████▏  | 83/116 [16:20\u003c06:21, 11.56s/it, training_loss=0.371]\u001b[A\n","Epoch 2:  72%|███████▏  | 83/116 [16:33\u003c06:21, 11.56s/it, training_loss=0.426]\u001b[A\n","Epoch 2:  72%|███████▏  | 84/116 [16:33\u003c06:18, 11.83s/it, training_loss=0.426]\u001b[A\n","Epoch 2:  72%|███████▏  | 84/116 [16:45\u003c06:18, 11.83s/it, training_loss=0.388]\u001b[A\n","Epoch 2:  73%|███████▎  | 85/116 [16:45\u003c06:12, 12.02s/it, training_loss=0.388]\u001b[A\n","Epoch 2:  73%|███████▎  | 85/116 [16:55\u003c06:12, 12.02s/it, training_loss=0.507]\u001b[A\n","Epoch 2:  74%|███████▍  | 86/116 [16:55\u003c05:44, 11.47s/it, training_loss=0.507]\u001b[A\n","Epoch 2:  74%|███████▍  | 86/116 [17:08\u003c05:44, 11.47s/it, training_loss=0.649]\u001b[A\n","Epoch 2:  75%|███████▌  | 87/116 [17:08\u003c05:39, 11.70s/it, training_loss=0.649]\u001b[A\n","Epoch 2:  75%|███████▌  | 87/116 [17:20\u003c05:39, 11.70s/it, training_loss=0.630]\u001b[A\n","Epoch 2:  76%|███████▌  | 88/116 [17:20\u003c05:33, 11.91s/it, training_loss=0.630]\u001b[A\n","Epoch 2:  76%|███████▌  | 88/116 [17:32\u003c05:33, 11.91s/it, training_loss=0.351]\u001b[A\n","Epoch 2:  77%|███████▋  | 89/116 [17:32\u003c05:20, 11.88s/it, training_loss=0.351]\u001b[A\n","Epoch 2:  77%|███████▋  | 89/116 [17:42\u003c05:20, 11.88s/it, training_loss=0.320]\u001b[A\n","Epoch 2:  78%|███████▊  | 90/116 [17:42\u003c04:59, 11.51s/it, training_loss=0.320]\u001b[A\n","Epoch 2:  78%|███████▊  | 90/116 [17:55\u003c04:59, 11.51s/it, training_loss=0.341]\u001b[A\n","Epoch 2:  78%|███████▊  | 91/116 [17:55\u003c04:55, 11.82s/it, training_loss=0.341]\u001b[A\n","Epoch 2:  78%|███████▊  | 91/116 [18:08\u003c04:55, 11.82s/it, training_loss=0.442]\u001b[A\n","Epoch 2:  79%|███████▉  | 92/116 [18:08\u003c04:49, 12.06s/it, training_loss=0.442]\u001b[A\n","Epoch 2:  79%|███████▉  | 92/116 [18:18\u003c04:49, 12.06s/it, training_loss=0.245]\u001b[A\n","Epoch 2:  80%|████████  | 93/116 [18:18\u003c04:28, 11.69s/it, training_loss=0.245]\u001b[A\n","Epoch 2:  80%|████████  | 93/116 [18:30\u003c04:28, 11.69s/it, training_loss=0.293]\u001b[A\n","Epoch 2:  81%|████████  | 94/116 [18:30\u003c04:14, 11.59s/it, training_loss=0.293]\u001b[A\n","Epoch 2:  81%|████████  | 94/116 [18:42\u003c04:14, 11.59s/it, training_loss=0.580]\u001b[A\n","Epoch 2:  82%|████████▏ | 95/116 [18:42\u003c04:09, 11.86s/it, training_loss=0.580]\u001b[A\n","Epoch 2:  82%|████████▏ | 95/116 [18:55\u003c04:09, 11.86s/it, training_loss=0.524]\u001b[A\n","Epoch 2:  83%|████████▎ | 96/116 [18:55\u003c03:59, 11.99s/it, training_loss=0.524]\u001b[A\n","Epoch 2:  83%|████████▎ | 96/116 [19:05\u003c03:59, 11.99s/it, training_loss=0.511]\u001b[A\n","Epoch 2:  84%|████████▎ | 97/116 [19:05\u003c03:37, 11.42s/it, training_loss=0.511]\u001b[A\n","Epoch 2:  84%|████████▎ | 97/116 [19:17\u003c03:37, 11.42s/it, training_loss=0.460]\u001b[A\n","Epoch 2:  84%|████████▍ | 98/116 [19:17\u003c03:30, 11.70s/it, training_loss=0.460]\u001b[A\n","Epoch 2:  84%|████████▍ | 98/116 [19:30\u003c03:30, 11.70s/it, training_loss=0.327]\u001b[A\n","Epoch 2:  85%|████████▌ | 99/116 [19:30\u003c03:23, 11.96s/it, training_loss=0.327]\u001b[A\n","Epoch 2:  85%|████████▌ | 99/116 [19:41\u003c03:23, 11.96s/it, training_loss=0.236]\u001b[A\n","Epoch 2:  86%|████████▌ | 100/116 [19:41\u003c03:09, 11.83s/it, training_loss=0.236]\u001b[A\n","Epoch 2:  86%|████████▌ | 100/116 [19:52\u003c03:09, 11.83s/it, training_loss=0.446]\u001b[A\n","Epoch 2:  87%|████████▋ | 101/116 [19:52\u003c02:53, 11.54s/it, training_loss=0.446]\u001b[A\n","Epoch 2:  87%|████████▋ | 101/116 [20:04\u003c02:53, 11.54s/it, training_loss=0.500]\u001b[A\n","Epoch 2:  88%|████████▊ | 102/116 [20:04\u003c02:45, 11.84s/it, training_loss=0.500]\u001b[A\n","Epoch 2:  88%|████████▊ | 102/116 [20:17\u003c02:45, 11.84s/it, training_loss=0.397]\u001b[A\n","Epoch 2:  89%|████████▉ | 103/116 [20:17\u003c02:36, 12.03s/it, training_loss=0.397]\u001b[A\n","Epoch 2:  89%|████████▉ | 103/116 [20:27\u003c02:36, 12.03s/it, training_loss=0.482]\u001b[A\n","Epoch 2:  90%|████████▉ | 104/116 [20:27\u003c02:18, 11.55s/it, training_loss=0.482]\u001b[A\n","Epoch 2:  90%|████████▉ | 104/116 [20:39\u003c02:18, 11.55s/it, training_loss=0.131]\u001b[A\n","Epoch 2:  91%|█████████ | 105/116 [20:39\u003c02:07, 11.63s/it, training_loss=0.131]\u001b[A\n","Epoch 2:  91%|█████████ | 105/116 [20:52\u003c02:07, 11.63s/it, training_loss=0.534]\u001b[A\n","Epoch 2:  91%|█████████▏| 106/116 [20:52\u003c01:58, 11.88s/it, training_loss=0.534]\u001b[A\n","Epoch 2:  91%|█████████▏| 106/116 [21:04\u003c01:58, 11.88s/it, training_loss=0.551]\u001b[A\n","Epoch 2:  92%|█████████▏| 107/116 [21:04\u003c01:47, 11.96s/it, training_loss=0.551]\u001b[A\n","Epoch 2:  92%|█████████▏| 107/116 [21:14\u003c01:47, 11.96s/it, training_loss=0.539]\u001b[A\n","Epoch 2:  93%|█████████▎| 108/116 [21:14\u003c01:31, 11.46s/it, training_loss=0.539]\u001b[A\n","Epoch 2:  93%|█████████▎| 108/116 [21:26\u003c01:31, 11.46s/it, training_loss=0.428]\u001b[A\n","Epoch 2:  94%|█████████▍| 109/116 [21:26\u003c01:22, 11.74s/it, training_loss=0.428]\u001b[A\n","Epoch 2:  94%|█████████▍| 109/116 [21:39\u003c01:22, 11.74s/it, training_loss=0.665]\u001b[A\n","Epoch 2:  95%|█████████▍| 110/116 [21:39\u003c01:11, 11.98s/it, training_loss=0.665]\u001b[A\n","Epoch 2:  95%|█████████▍| 110/116 [21:50\u003c01:11, 11.98s/it, training_loss=0.251]\u001b[A\n","Epoch 2:  96%|█████████▌| 111/116 [21:50\u003c00:58, 11.73s/it, training_loss=0.251]\u001b[A\n","Epoch 2:  96%|█████████▌| 111/116 [22:01\u003c00:58, 11.73s/it, training_loss=0.597]\u001b[A\n","Epoch 2:  97%|█████████▋| 112/116 [22:01\u003c00:46, 11.55s/it, training_loss=0.597]\u001b[A\n","Epoch 2:  97%|█████████▋| 112/116 [22:14\u003c00:46, 11.55s/it, training_loss=0.599]\u001b[A\n","Epoch 2:  97%|█████████▋| 113/116 [22:14\u003c00:35, 11.82s/it, training_loss=0.599]\u001b[A\n","Epoch 2:  97%|█████████▋| 113/116 [22:26\u003c00:35, 11.82s/it, training_loss=0.515]\u001b[A\n","Epoch 2:  98%|█████████▊| 114/116 [22:26\u003c00:24, 12.02s/it, training_loss=0.515]\u001b[A\n","Epoch 2:  98%|█████████▊| 114/116 [22:37\u003c00:24, 12.02s/it, training_loss=0.352]\u001b[A\n","Epoch 2:  99%|█████████▉| 115/116 [22:37\u003c00:11, 11.54s/it, training_loss=0.352]\u001b[A\n","Epoch 2:  99%|█████████▉| 115/116 [22:44\u003c00:11, 11.54s/it, training_loss=0.394]\u001b[A\n","Epoch 2: 100%|██████████| 116/116 [22:44\u003c00:00, 10.30s/it, training_loss=0.394]\u001b[A\n"," 25%|██▌       | 1/4 [47:42\u003c1:14:54, 1498.02s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 2\n","Training loss: 1.3272958621382713\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 2/4 [49:27\u003c49:22, 1481.32s/it]  "]},{"name":"stdout","output_type":"stream","text":["Validation loss: 1.3234741996074546\n","F1 Score (Weighted): 0.36309041839733425\n","QWK Score: 0.26876720129068987\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 3:   0%|          | 0/116 [00:00\u003c?, ?it/s]\u001b[A\n","Epoch 3:   0%|          | 0/116 [00:12\u003c?, ?it/s, training_loss=0.530]\u001b[A\n","Epoch 3:   1%|          | 1/116 [00:12\u003c23:55, 12.48s/it, training_loss=0.530]\u001b[A\n","Epoch 3:   1%|          | 1/116 [00:25\u003c23:55, 12.48s/it, training_loss=0.429]\u001b[A\n","Epoch 3:   2%|▏         | 2/116 [00:25\u003c23:49, 12.54s/it, training_loss=0.429]\u001b[A\n","Epoch 3:   2%|▏         | 2/116 [00:36\u003c23:49, 12.54s/it, training_loss=0.486]\u001b[A\n","Epoch 3:   3%|▎         | 3/116 [00:36\u003c22:48, 12.11s/it, training_loss=0.486]\u001b[A\n","Epoch 3:   3%|▎         | 3/116 [00:47\u003c22:48, 12.11s/it, training_loss=0.561]\u001b[A\n","Epoch 3:   3%|▎         | 4/116 [00:47\u003c21:38, 11.60s/it, training_loss=0.561]\u001b[A\n","Epoch 3:   3%|▎         | 4/116 [00:59\u003c21:38, 11.60s/it, training_loss=0.291]\u001b[A\n","Epoch 3:   4%|▍         | 5/116 [00:59\u003c22:01, 11.91s/it, training_loss=0.291]\u001b[A\n","Epoch 3:   4%|▍         | 5/116 [01:12\u003c22:01, 11.91s/it, training_loss=0.570]\u001b[A\n","Epoch 3:   5%|▌         | 6/116 [01:12\u003c22:10, 12.09s/it, training_loss=0.570]\u001b[A\n","Epoch 3:   5%|▌         | 6/116 [01:22\u003c22:10, 12.09s/it, training_loss=0.359]\u001b[A\n","Epoch 3:   6%|▌         | 7/116 [01:22\u003c21:02, 11.59s/it, training_loss=0.359]\u001b[A\n","Epoch 3:   6%|▌         | 7/116 [01:34\u003c21:02, 11.59s/it, training_loss=0.447]\u001b[A\n","Epoch 3:   7%|▋         | 8/116 [01:34\u003c20:56, 11.63s/it, training_loss=0.447]\u001b[A\n","Epoch 3:   7%|▋         | 8/116 [01:47\u003c20:56, 11.63s/it, training_loss=0.376]\u001b[A\n","Epoch 3:   8%|▊         | 9/116 [01:47\u003c21:12, 11.89s/it, training_loss=0.376]\u001b[A\n","Epoch 3:   8%|▊         | 9/116 [01:59\u003c21:12, 11.89s/it, training_loss=0.409]\u001b[A\n","Epoch 3:   9%|▊         | 10/116 [01:59\u003c21:09, 11.98s/it, training_loss=0.409]\u001b[A\n","Epoch 3:   9%|▊         | 10/116 [02:09\u003c21:09, 11.98s/it, training_loss=0.311]\u001b[A\n","Epoch 3:   9%|▉         | 11/116 [02:09\u003c20:02, 11.45s/it, training_loss=0.311]\u001b[A\n","Epoch 3:   9%|▉         | 11/116 [02:22\u003c20:02, 11.45s/it, training_loss=0.430]\u001b[A\n","Epoch 3:  10%|█         | 12/116 [02:22\u003c20:24, 11.78s/it, training_loss=0.430]\u001b[A\n","Epoch 3:  10%|█         | 12/116 [02:34\u003c20:24, 11.78s/it, training_loss=0.408]\u001b[A\n","Epoch 3:  11%|█         | 13/116 [02:34\u003c20:39, 12.03s/it, training_loss=0.408]\u001b[A\n","Epoch 3:  11%|█         | 13/116 [02:46\u003c20:39, 12.03s/it, training_loss=0.331]\u001b[A\n","Epoch 3:  12%|█▏        | 14/116 [02:46\u003c20:13, 11.89s/it, training_loss=0.331]\u001b[A\n","Epoch 3:  12%|█▏        | 14/116 [02:57\u003c20:13, 11.89s/it, training_loss=0.379]\u001b[A\n","Epoch 3:  13%|█▎        | 15/116 [02:57\u003c19:29, 11.58s/it, training_loss=0.379]\u001b[A\n","Epoch 3:  13%|█▎        | 15/116 [03:09\u003c19:29, 11.58s/it, training_loss=0.443]\u001b[A\n","Epoch 3:  14%|█▍        | 16/116 [03:09\u003c19:46, 11.86s/it, training_loss=0.443]\u001b[A\n","Epoch 3:  14%|█▍        | 16/116 [03:22\u003c19:46, 11.86s/it, training_loss=0.343]\u001b[A\n","Epoch 3:  15%|█▍        | 17/116 [03:22\u003c19:54, 12.07s/it, training_loss=0.343]\u001b[A\n","Epoch 3:  15%|█▍        | 17/116 [03:32\u003c19:54, 12.07s/it, training_loss=0.567]\u001b[A\n","Epoch 3:  16%|█▌        | 18/116 [03:32\u003c18:57, 11.61s/it, training_loss=0.567]\u001b[A\n","Epoch 3:  16%|█▌        | 18/116 [03:44\u003c18:57, 11.61s/it, training_loss=0.413]\u001b[A\n","Epoch 3:  16%|█▋        | 19/116 [03:44\u003c18:47, 11.63s/it, training_loss=0.413]\u001b[A\n","Epoch 3:  16%|█▋        | 19/116 [03:56\u003c18:47, 11.63s/it, training_loss=0.400]\u001b[A\n","Epoch 3:  17%|█▋        | 20/116 [03:56\u003c19:01, 11.89s/it, training_loss=0.400]\u001b[A\n","Epoch 3:  17%|█▋        | 20/116 [04:09\u003c19:01, 11.89s/it, training_loss=0.335]\u001b[A\n","Epoch 3:  18%|█▊        | 21/116 [04:09\u003c18:58, 11.99s/it, training_loss=0.335]\u001b[A\n","Epoch 3:  18%|█▊        | 21/116 [04:19\u003c18:58, 11.99s/it, training_loss=0.504]\u001b[A\n","Epoch 3:  19%|█▉        | 22/116 [04:19\u003c17:56, 11.45s/it, training_loss=0.504]\u001b[A\n","Epoch 3:  19%|█▉        | 22/116 [04:31\u003c17:56, 11.45s/it, training_loss=0.279]\u001b[A\n","Epoch 3:  20%|█▉        | 23/116 [04:31\u003c18:13, 11.76s/it, training_loss=0.279]\u001b[A\n","Epoch 3:  20%|█▉        | 23/116 [04:44\u003c18:13, 11.76s/it, training_loss=0.447]\u001b[A\n","Epoch 3:  21%|██        | 24/116 [04:44\u003c18:23, 11.99s/it, training_loss=0.447]\u001b[A\n","Epoch 3:  21%|██        | 24/116 [04:55\u003c18:23, 11.99s/it, training_loss=0.483]\u001b[A\n","Epoch 3:  22%|██▏       | 25/116 [04:55\u003c17:54, 11.80s/it, training_loss=0.483]\u001b[A\n","Epoch 3:  22%|██▏       | 25/116 [05:06\u003c17:54, 11.80s/it, training_loss=0.700]\u001b[A\n","Epoch 3:  22%|██▏       | 26/116 [05:06\u003c17:20, 11.56s/it, training_loss=0.700]\u001b[A\n","Epoch 3:  22%|██▏       | 26/116 [05:19\u003c17:20, 11.56s/it, training_loss=0.465]\u001b[A\n","Epoch 3:  23%|██▎       | 27/116 [05:19\u003c17:32, 11.82s/it, training_loss=0.465]\u001b[A\n","Epoch 3:  23%|██▎       | 27/116 [05:31\u003c17:32, 11.82s/it, training_loss=0.568]\u001b[A\n","Epoch 3:  24%|██▍       | 28/116 [05:31\u003c17:40, 12.05s/it, training_loss=0.568]\u001b[A\n","Epoch 3:  24%|██▍       | 28/116 [05:42\u003c17:40, 12.05s/it, training_loss=0.633]\u001b[A\n","Epoch 3:  25%|██▌       | 29/116 [05:42\u003c16:53, 11.65s/it, training_loss=0.633]\u001b[A\n","Epoch 3:  25%|██▌       | 29/116 [05:54\u003c16:53, 11.65s/it, training_loss=0.344]\u001b[A\n","Epoch 3:  26%|██▌       | 30/116 [05:54\u003c16:40, 11.64s/it, training_loss=0.344]\u001b[A\n","Epoch 3:  26%|██▌       | 30/116 [06:06\u003c16:40, 11.64s/it, training_loss=0.264]\u001b[A\n","Epoch 3:  27%|██▋       | 31/116 [06:06\u003c16:49, 11.87s/it, training_loss=0.264]\u001b[A\n","Epoch 3:  27%|██▋       | 31/116 [06:18\u003c16:49, 11.87s/it, training_loss=0.335]\u001b[A\n","Epoch 3:  28%|██▊       | 32/116 [06:18\u003c16:43, 11.95s/it, training_loss=0.335]\u001b[A\n","Epoch 3:  28%|██▊       | 32/116 [06:28\u003c16:43, 11.95s/it, training_loss=0.504]\u001b[A\n","Epoch 3:  28%|██▊       | 33/116 [06:28\u003c15:50, 11.45s/it, training_loss=0.504]\u001b[A\n","Epoch 3:  28%|██▊       | 33/116 [06:41\u003c15:50, 11.45s/it, training_loss=0.201]\u001b[A\n","Epoch 3:  29%|██▉       | 34/116 [06:41\u003c16:05, 11.77s/it, training_loss=0.201]\u001b[A\n","Epoch 3:  29%|██▉       | 34/116 [06:53\u003c16:05, 11.77s/it, training_loss=0.267]\u001b[A\n","Epoch 3:  30%|███       | 35/116 [06:53\u003c16:11, 11.99s/it, training_loss=0.267]\u001b[A\n","Epoch 3:  30%|███       | 35/116 [07:05\u003c16:11, 11.99s/it, training_loss=0.341]\u001b[A\n","Epoch 3:  31%|███       | 36/116 [07:05\u003c15:42, 11.79s/it, training_loss=0.341]\u001b[A\n","Epoch 3:  31%|███       | 36/116 [07:16\u003c15:42, 11.79s/it, training_loss=0.325]\u001b[A\n","Epoch 3:  32%|███▏      | 37/116 [07:16\u003c15:12, 11.56s/it, training_loss=0.325]\u001b[A\n","Epoch 3:  32%|███▏      | 37/116 [07:28\u003c15:12, 11.56s/it, training_loss=0.491]\u001b[A\n","Epoch 3:  33%|███▎      | 38/116 [07:28\u003c15:24, 11.85s/it, training_loss=0.491]\u001b[A\n","Epoch 3:  33%|███▎      | 38/116 [07:41\u003c15:24, 11.85s/it, training_loss=0.284]\u001b[A\n","Epoch 3:  34%|███▎      | 39/116 [07:41\u003c15:28, 12.06s/it, training_loss=0.284]\u001b[A\n","Epoch 3:  34%|███▎      | 39/116 [07:51\u003c15:28, 12.06s/it, training_loss=0.416]\u001b[A\n","Epoch 3:  34%|███▍      | 40/116 [07:51\u003c14:36, 11.53s/it, training_loss=0.416]\u001b[A\n","Epoch 3:  34%|███▍      | 40/116 [08:03\u003c14:36, 11.53s/it, training_loss=0.556]\u001b[A\n","Epoch 3:  35%|███▌      | 41/116 [08:03\u003c14:31, 11.62s/it, training_loss=0.556]\u001b[A\n","Epoch 3:  35%|███▌      | 41/116 [08:15\u003c14:31, 11.62s/it, training_loss=0.531]\u001b[A\n","Epoch 3:  36%|███▌      | 42/116 [08:15\u003c14:40, 11.90s/it, training_loss=0.531]\u001b[A\n","Epoch 3:  36%|███▌      | 42/116 [08:27\u003c14:40, 11.90s/it, training_loss=0.138]\u001b[A\n","Epoch 3:  37%|███▋      | 43/116 [08:28\u003c14:31, 11.94s/it, training_loss=0.138]\u001b[A\n","Epoch 3:  37%|███▋      | 43/116 [08:38\u003c14:31, 11.94s/it, training_loss=0.205]\u001b[A\n","Epoch 3:  38%|███▊      | 44/116 [08:38\u003c13:47, 11.50s/it, training_loss=0.205]\u001b[A\n","Epoch 3:  38%|███▊      | 44/116 [08:50\u003c13:47, 11.50s/it, training_loss=0.475]\u001b[A\n","Epoch 3:  39%|███▉      | 45/116 [08:50\u003c13:56, 11.78s/it, training_loss=0.475]\u001b[A\n","Epoch 3:  39%|███▉      | 45/116 [09:03\u003c13:56, 11.78s/it, training_loss=0.229]\u001b[A\n","Epoch 3:  40%|███▉      | 46/116 [09:03\u003c13:59, 11.99s/it, training_loss=0.229]\u001b[A\n","Epoch 3:  40%|███▉      | 46/116 [09:14\u003c13:59, 11.99s/it, training_loss=0.332]\u001b[A\n","Epoch 3:  41%|████      | 47/116 [09:14\u003c13:27, 11.70s/it, training_loss=0.332]\u001b[A\n","Epoch 3:  41%|████      | 47/116 [09:25\u003c13:27, 11.70s/it, training_loss=0.260]\u001b[A\n","Epoch 3:  41%|████▏     | 48/116 [09:25\u003c13:04, 11.54s/it, training_loss=0.260]\u001b[A\n","Epoch 3:  41%|████▏     | 48/116 [09:38\u003c13:04, 11.54s/it, training_loss=0.290]\u001b[A\n","Epoch 3:  42%|████▏     | 49/116 [09:38\u003c13:12, 11.83s/it, training_loss=0.290]\u001b[A\n","Epoch 3:  42%|████▏     | 49/116 [09:50\u003c13:12, 11.83s/it, training_loss=0.433]\u001b[A\n","Epoch 3:  43%|████▎     | 50/116 [09:50\u003c13:12, 12.00s/it, training_loss=0.433]\u001b[A\n","Epoch 3:  43%|████▎     | 50/116 [10:00\u003c13:12, 12.00s/it, training_loss=0.224]\u001b[A\n","Epoch 3:  44%|████▍     | 51/116 [10:00\u003c12:25, 11.47s/it, training_loss=0.224]\u001b[A\n","Epoch 3:  44%|████▍     | 51/116 [10:12\u003c12:25, 11.47s/it, training_loss=0.432]\u001b[A\n","Epoch 3:  45%|████▍     | 52/116 [10:12\u003c12:24, 11.63s/it, training_loss=0.432]\u001b[A"]}],"source":["import os\n","import pandas as pd\n","\n","path_dir = '/content/drive/MyDrive/Paper_TA_ASAG/DATASET_TA/Data/Data_Lagi/Lifestyle'\n","list_dir = os.listdir(path_dir)\n","\n","list_pre_trained_model = ['indobenchmark/indobert-lite-base-p2']\n","\n","for m in list_pre_trained_model:\n","    print(m)\n","    for idx, ele in enumerate(list_dir):\n","        df_raw = pd.read_excel(open(path_dir+'/'+ele, 'rb'),\n","                               sheet_name='Soal',\n","                               header=1,\n","                               index_col=0,\n","                               usecols='B:D')\n","\n","        list_final = []\n","\n","        for i in df_raw.itertuples():\n","            list_final.append(\n","                {\n","                    'soal': i[1],\n","                    'jawaban': i[2],\n","                    'nilai': 100,\n","                    'tipe': 'train'\n","                }\n","            )\n","            df_tmp = pd.read_excel(open(path_dir+'/'+ele, 'rb'),\n","                                        sheet_name='No.'+str(i.Index),\n","                                        header=1,\n","                                        index_col=0,\n","                                        usecols='B:N')\n","            df_tmp = df_tmp.dropna()\n","            for j in df_tmp.itertuples():\n","                list_final.append(\n","                    {\n","                        'soal': i[1],\n","                        'jawaban': j[2],\n","                        'nilai': j[12],\n","                        'tipe': 'test'\n","                    }\n","                )\n","        if idx == 0:\n","            df_final = pd.DataFrame(list_final)\n","        else:\n","            df_final.append(pd.DataFrame(list_final), ignore_index=True)\n","\n","        print(' '.join(ele.rstrip('.xslx').split('_')))\n","        train_eval(df_final, m)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_L2EdYIAAYkI"},"outputs":[],"source":["def train_eval_raw(df_final, pretrainedmodel):\n","    # bin nilai (continuous variable) into intervals\n","    df_final['nilai'] = pd.qcut(df_final['nilai'], 5, labels=False, duplicates='drop')\n","\n","    # concatenate soal and jawaban\n","    df_final['soal-jawaban'] = df_final['soal']+df_final['jawaban']\n","\n","    # make sure that the training set and test set ratio is 80:20\n","    add = len(df_final[df_final['tipe'] == 'test']) - (round(0.2*(len(df_final[df_final['tipe'] == 'train'])+len(df_final[df_final['tipe'] == 'test']))))\n","    for i in df_final[df_final['tipe'] == 'test'].sample(n = add).itertuples():\n","        df_final.at[i.Index, 'tipe'] = 'train'\n","\n","    # load model and tokenizer\n","    tokenizer = BertTokenizer.from_pretrained(pretrainedmodel, ignore_mismatched_sizes=True)\n","\n","    encoded_data_train = tokenizer.batch_encode_plus(\n","        df_final[df_final.tipe=='train']['soal-jawaban'].values,\n","        add_special_tokens=True,\n","        return_attention_mask=True,\n","        pad_to_max_length=True,\n","        truncation=True,\n","        max_length=256,\n","        padding='max_length',\n","        return_tensors='pt'\n","    )\n","\n","    encoded_data_val = tokenizer.batch_encode_plus(\n","        df_final[df_final.tipe=='test']['soal-jawaban'].values,\n","        add_special_tokens=True,\n","        return_attention_mask=True,\n","        pad_to_max_length=True,\n","        truncation=True,\n","        max_length=256,\n","        padding='max_length',\n","        return_tensors='pt'\n","    )\n","\n","    input_ids_train = encoded_data_train['input_ids']\n","    attention_masks_train = encoded_data_train['attention_mask']\n","    labels_train = torch.tensor(df_final[df_final.tipe=='train'].nilai.values)\n","\n","    input_ids_val = encoded_data_val['input_ids']\n","    attention_masks_val = encoded_data_val['attention_mask']\n","    labels_val = torch.tensor(df_final[df_final.tipe=='test'].nilai.values)\n","\n","    dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n","    dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n","\n","    model = BertForSequenceClassification.from_pretrained(pretrainedmodel,\n","                                                          num_labels=5,\n","                                                          output_attentions=False,\n","                                                          output_hidden_states=False, ignore_mismatched_sizes=True)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","\n","    batch_size = 4\n","\n","    dataloader_train = DataLoader(dataset_train,\n","                                  sampler=RandomSampler(dataset_train),\n","                                  batch_size=batch_size)\n","\n","    dataloader_validation = DataLoader(dataset_val,\n","                                       sampler=SequentialSampler(dataset_val),\n","                                       batch_size=batch_size)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(),\n","                      lr=2e-5,\n","                      eps=1e-8)\n","\n","    epochs = 4\n","\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps=0,\n","                                                num_training_steps=len(dataloader_train)*epochs)\n","\n","    for epoch in tqdm(range(1, epochs+1)):\n","\n","        model.train()\n","\n","        loss_train_total = 0\n","\n","        progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n","        for batch in progress_bar:\n","\n","            model.zero_grad()\n","\n","            batch = tuple(b.to(device) for b in batch)\n","\n","            inputs = {'input_ids':      batch[0],\n","                      'attention_mask': batch[1],\n","                      'labels':         batch[2],\n","                     }\n","\n","            outputs = model(**inputs)\n","\n","            loss = outputs[0]\n","            loss_train_total += loss.item()\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            optimizer.step()\n","            scheduler.step()\n","\n","            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n","\n","        torch.save(model.state_dict(), f'/content/drive/MyDrive/Paper_TA_ASAG/DATASET_TA/Data/Data_Lagi/Lifestyle_Save/finetuned_BERT_raw_epoch_{epoch}.model')\n","        tqdm.write(f'\\nEpoch {epoch}')\n","\n","        loss_train_avg = loss_train_total/len(dataloader_train)\n","        tqdm.write(f'Training loss: {loss_train_avg}')\n","\n","        val_loss, predictions, true_vals = evaluate(dataloader_validation, device, model)\n","        val_f1 = f1_score_func(predictions, true_vals)\n","        val_qwk = qwk_score_func(predictions, true_vals)\n","        tqdm.write(f'Validation loss: {val_loss}')\n","        tqdm.write(f'F1 Score (Weighted): {val_f1}')\n","        tqdm.write(f'QWK Score: {val_qwk}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"42kqY-CvVx1m","outputId":"d12172c6-9883-4af0-d62b-e8f8968fef51"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mOutput streaming akan dipotong hingga 5000 baris terakhir.\u001b[0m\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n"]}],"source":["while 1:\n","  print(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wkywYff0Fnsl"},"outputs":[],"source":["import os\n","import pandas as pd\n","\n","path_dir = '/content/drive/MyDrive/Paper_TA_ASAG/DATASET_TA/Data/Data_Lagi/Lifestyle'\n","list_dir = os.listdir(path_dir)\n","\n","list_pre_trained_model = ['indobenchmark/indobert-lite-base-p2']\n","\n","for m in list_pre_trained_model:\n","    print(m)\n","    for idx, ele in enumerate(list_dir):\n","        df_raw = pd.read_excel(open(path_dir+'/'+ele, 'rb'),\n","                               sheet_name='Soal',\n","                               header=1,\n","                               index_col=0,\n","                               usecols='B:D')\n","\n","        list_final = []\n","\n","        for i in df_raw.itertuples():\n","            list_final.append(\n","                {\n","                    'soal': i[1],\n","                    'jawaban': i[2],\n","                    'nilai': 100,\n","                    'tipe': 'train'\n","                }\n","            )\n","            df_tmp = pd.read_excel(open(path_dir+'/'+ele, 'rb'),\n","                                        sheet_name='No.'+str(i.Index),\n","                                        header=1,\n","                                        index_col=0,\n","                                        usecols='B:N')\n","            df_tmp = df_tmp.dropna()\n","            for j in df_tmp.itertuples():\n","                list_final.append(\n","                    {\n","                        'soal': i[1],\n","                        'jawaban': j[2],\n","                        'nilai': j[12],\n","                        'tipe': 'test'\n","                    }\n","                )\n","        if idx == 0:\n","            df_final = pd.DataFrame(list_final)\n","        else:\n","            df_final.append(pd.DataFrame(list_final), ignore_index=True)\n","\n","        print(' '.join(ele.rstrip('.xslx').split('_')))\n","        train_eval_raw(df_final, m)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v5Zk9bJnX0ti"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOs5ZUl/imwrDj0L04Cqf7N","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}